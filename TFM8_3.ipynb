{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import keras\n",
    "import six\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,AveragePooling1D,Flatten,concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/roberto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/roberto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/roberto/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/roberto/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abro el archivo en el que se encuentra el dataset de los problemas\n",
    "with open('singleop.json', 'r') as f:\n",
    "    datastore = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archivo que contiene un listado de nombres\n",
    "nombres = pd.read_csv('nombres-2015.csv')\n",
    "names = pd.read_csv('yob2019.txt', header=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = pd.read_csv('problemas_adicionales2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preguntas = []\n",
    "respuestas = []\n",
    "ecuaciones = []\n",
    "alineacion = []\n",
    "\n",
    "for item in datastore:\n",
    "    preguntas.append(item['sQuestion'])\n",
    "    respuestas.append(item['lSolutions'])\n",
    "    ecuaciones.append(item['lEquations'])\n",
    "    alineacion.append(item['lAlignments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tengo  159  sumas  162  restas,  117  multiplicaciones,  124  divisiones y otras operaciones  0\n"
     ]
    }
   ],
   "source": [
    "# Necesito convertir el dataset en un problema de clasificacion para que la red neuronal pueda identificar\n",
    "# si estoy tratando de resolver un problema de sumas, restas, multiplicaciones o divisiones.\n",
    "# Esto va a crear una lista con el tipo de operacion y que va a ser el resultado a inferir.\n",
    "operaciones = []\n",
    "sumas =0\n",
    "restas =0\n",
    "multiplicaciones =0\n",
    "divisiones = 0\n",
    "otras = 0\n",
    "#Clasifico las operaciones en 0 para sumas, 1 para restas, 2 para multiplicaciones, 3 para divisiones y 4 sino lo encuentro.\n",
    "for operacion in ecuaciones:\n",
    "    if (operacion[0].find('+')>=0):\n",
    "        operaciones.append(0)\n",
    "        sumas = sumas + 1\n",
    "    elif (operacion[0].find('-') >= 0 ):\n",
    "        operaciones.append(1)\n",
    "        restas = restas + 1\n",
    "    elif(operacion[0].find('*') >=0):\n",
    "        operaciones.append(2)\n",
    "        multiplicaciones = multiplicaciones + 1\n",
    "    elif(operacion[0].find('/')):\n",
    "        operaciones.append(3)\n",
    "        divisiones = divisiones + 1\n",
    "    else:\n",
    "        operaciones.append(4)\n",
    "        otras = otras + 1\n",
    "\n",
    "print('Tengo ', sumas, ' sumas ', restas, ' restas, ', multiplicaciones, ' multiplicaciones, ', divisiones, ' divisiones y otras operaciones ', otras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preguntas2 = dataset2['Preguntas'].tolist()\n",
    "respuestas2 = dataset2['respuestas'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preguntas3 = preguntas + preguntas2\n",
    "respuestas3 = operaciones + respuestas2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#El listado de nombres lo voy a truncar a los 15K primeros, dado que el resto son nombres muy residuales.\n",
    "nombres_ = nombres['nombre'][:15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_= names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombres_ = nombres_.append(names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_words = list(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nom = nombres['nombre'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomb =  nom + st_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tenemos'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nomb[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El vector preguntas_sin, consiste en las preguntas a las que voy a eliminar todos los nombres propios que no\n",
    "# anaden ningun valor al conjunto de preguntas. No quiero que esos nombres se procesen y por tanto los elimino.\n",
    "def eliminar_palabras(dataset, stopw):\n",
    "    preguntas_sin = []\n",
    "    for palabras in dataset:\n",
    "        frases = [word for word in palabras.split(' ') if word not in stopw]\n",
    "        frases = \" \".join(frases)\n",
    "        preguntas_sin.append(frases)\n",
    "    return preguntas_sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preguntas_sin = eliminar_palabras(preguntas3, nomb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def mezclarPalabras(frase):\n",
    "    separar = frase.split()\n",
    "    shuffle(separar)\n",
    "    return ' '.join(separar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "974"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(respuestas3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "12\n",
      "2\n",
      "23\n",
      "3\n",
      "34\n",
      "4\n",
      "45\n",
      "5\n",
      "56\n",
      "6\n",
      "67\n",
      "7\n",
      "78\n",
      "8\n",
      "89\n",
      "9\n",
      "100\n",
      "10\n",
      "111\n",
      "11\n",
      "122\n",
      "12\n",
      "133\n",
      "13\n",
      "144\n",
      "14\n",
      "155\n",
      "15\n",
      "166\n",
      "16\n",
      "177\n",
      "17\n",
      "188\n",
      "18\n",
      "199\n",
      "19\n",
      "210\n",
      "20\n",
      "221\n",
      "21\n",
      "232\n",
      "22\n",
      "243\n",
      "23\n",
      "254\n",
      "24\n",
      "265\n",
      "25\n",
      "276\n",
      "26\n",
      "287\n",
      "27\n",
      "298\n",
      "28\n",
      "309\n",
      "29\n",
      "320\n",
      "30\n",
      "331\n",
      "31\n",
      "342\n",
      "32\n",
      "353\n",
      "33\n",
      "364\n",
      "34\n",
      "375\n",
      "35\n",
      "386\n",
      "36\n",
      "397\n",
      "37\n",
      "408\n",
      "38\n",
      "419\n",
      "39\n",
      "430\n",
      "40\n",
      "441\n",
      "41\n",
      "452\n",
      "42\n",
      "463\n",
      "43\n",
      "474\n",
      "44\n",
      "485\n",
      "45\n",
      "496\n",
      "46\n",
      "507\n",
      "47\n",
      "518\n",
      "48\n",
      "529\n",
      "49\n",
      "540\n",
      "50\n",
      "551\n",
      "51\n",
      "562\n",
      "52\n",
      "573\n",
      "53\n",
      "584\n",
      "54\n",
      "595\n",
      "55\n",
      "606\n",
      "56\n",
      "617\n",
      "57\n",
      "628\n",
      "58\n",
      "639\n",
      "59\n",
      "650\n",
      "60\n",
      "661\n",
      "61\n",
      "672\n",
      "62\n",
      "683\n",
      "63\n",
      "694\n",
      "64\n",
      "705\n",
      "65\n",
      "716\n",
      "66\n",
      "727\n",
      "67\n",
      "738\n",
      "68\n",
      "749\n",
      "69\n",
      "760\n",
      "70\n",
      "771\n",
      "71\n",
      "782\n",
      "72\n",
      "793\n",
      "73\n",
      "804\n",
      "74\n",
      "815\n",
      "75\n",
      "826\n",
      "76\n",
      "837\n",
      "77\n",
      "848\n",
      "78\n",
      "859\n",
      "79\n",
      "870\n",
      "80\n",
      "881\n",
      "81\n",
      "892\n",
      "82\n",
      "903\n",
      "83\n",
      "914\n",
      "84\n",
      "925\n",
      "85\n",
      "936\n",
      "86\n",
      "947\n",
      "87\n",
      "958\n",
      "88\n",
      "969\n",
      "89\n",
      "980\n",
      "90\n",
      "991\n",
      "91\n",
      "1002\n",
      "92\n",
      "1013\n",
      "93\n",
      "1024\n",
      "94\n",
      "1035\n",
      "95\n",
      "1046\n",
      "96\n",
      "1057\n",
      "97\n",
      "1068\n",
      "98\n",
      "1079\n",
      "99\n",
      "1090\n",
      "100\n",
      "1101\n",
      "101\n",
      "1112\n",
      "102\n",
      "1123\n",
      "103\n",
      "1134\n",
      "104\n",
      "1145\n",
      "105\n",
      "1156\n",
      "106\n",
      "1167\n",
      "107\n",
      "1178\n",
      "108\n",
      "1189\n",
      "109\n",
      "1200\n",
      "110\n",
      "1211\n",
      "111\n",
      "1222\n",
      "112\n",
      "1233\n",
      "113\n",
      "1244\n",
      "114\n",
      "1255\n",
      "115\n",
      "1266\n",
      "116\n",
      "1277\n",
      "117\n",
      "1288\n",
      "118\n",
      "1299\n",
      "119\n",
      "1310\n",
      "120\n",
      "1321\n",
      "121\n",
      "1332\n",
      "122\n",
      "1343\n",
      "123\n",
      "1354\n",
      "124\n",
      "1365\n",
      "125\n",
      "1376\n",
      "126\n",
      "1387\n",
      "127\n",
      "1398\n",
      "128\n",
      "1409\n",
      "129\n",
      "1420\n",
      "130\n",
      "1431\n",
      "131\n",
      "1442\n",
      "132\n",
      "1453\n",
      "133\n",
      "1464\n",
      "134\n",
      "1475\n",
      "135\n",
      "1486\n",
      "136\n",
      "1497\n",
      "137\n",
      "1508\n",
      "138\n",
      "1519\n",
      "139\n",
      "1530\n",
      "140\n",
      "1541\n",
      "141\n",
      "1552\n",
      "142\n",
      "1563\n",
      "143\n",
      "1574\n",
      "144\n",
      "1585\n",
      "145\n",
      "1596\n",
      "146\n",
      "1607\n",
      "147\n",
      "1618\n",
      "148\n",
      "1629\n",
      "149\n",
      "1640\n",
      "150\n",
      "1651\n",
      "151\n",
      "1662\n",
      "152\n",
      "1673\n",
      "153\n",
      "1684\n",
      "154\n",
      "1695\n",
      "155\n",
      "1706\n",
      "156\n",
      "1717\n",
      "157\n",
      "1728\n",
      "158\n",
      "1739\n",
      "159\n",
      "1750\n",
      "160\n",
      "1761\n",
      "161\n",
      "1772\n",
      "162\n",
      "1783\n",
      "163\n",
      "1794\n",
      "164\n",
      "1805\n",
      "165\n",
      "1816\n",
      "166\n",
      "1827\n",
      "167\n",
      "1838\n",
      "168\n",
      "1849\n",
      "169\n",
      "1860\n",
      "170\n",
      "1871\n",
      "171\n",
      "1882\n",
      "172\n",
      "1893\n",
      "173\n",
      "1904\n",
      "174\n",
      "1915\n",
      "175\n",
      "1926\n",
      "176\n",
      "1937\n",
      "177\n",
      "1948\n",
      "178\n",
      "1959\n",
      "179\n",
      "1970\n",
      "180\n",
      "1981\n",
      "181\n",
      "1992\n",
      "182\n",
      "2003\n",
      "183\n",
      "2014\n",
      "184\n",
      "2025\n",
      "185\n",
      "2036\n",
      "186\n",
      "2047\n",
      "187\n",
      "2058\n",
      "188\n",
      "2069\n",
      "189\n",
      "2080\n",
      "190\n",
      "2091\n",
      "191\n",
      "2102\n",
      "192\n",
      "2113\n",
      "193\n",
      "2124\n",
      "194\n",
      "2135\n",
      "195\n",
      "2146\n",
      "196\n",
      "2157\n",
      "197\n",
      "2168\n",
      "198\n",
      "2179\n",
      "199\n",
      "2190\n",
      "200\n",
      "2201\n",
      "201\n",
      "2212\n",
      "202\n",
      "2223\n",
      "203\n",
      "2234\n",
      "204\n",
      "2245\n",
      "205\n",
      "2256\n",
      "206\n",
      "2267\n",
      "207\n",
      "2278\n",
      "208\n",
      "2289\n",
      "209\n",
      "2300\n",
      "210\n",
      "2311\n",
      "211\n",
      "2322\n",
      "212\n",
      "2333\n",
      "213\n",
      "2344\n",
      "214\n",
      "2355\n",
      "215\n",
      "2366\n",
      "216\n",
      "2377\n",
      "217\n",
      "2388\n",
      "218\n",
      "2399\n",
      "219\n",
      "2410\n",
      "220\n",
      "2421\n",
      "221\n",
      "2432\n",
      "222\n",
      "2443\n",
      "223\n",
      "2454\n",
      "224\n",
      "2465\n",
      "225\n",
      "2476\n",
      "226\n",
      "2487\n",
      "227\n",
      "2498\n",
      "228\n",
      "2509\n",
      "229\n",
      "2520\n",
      "230\n",
      "2531\n",
      "231\n",
      "2542\n",
      "232\n",
      "2553\n",
      "233\n",
      "2564\n",
      "234\n",
      "2575\n",
      "235\n",
      "2586\n",
      "236\n",
      "2597\n",
      "237\n",
      "2608\n",
      "238\n",
      "2619\n",
      "239\n",
      "2630\n",
      "240\n",
      "2641\n",
      "241\n",
      "2652\n",
      "242\n",
      "2663\n",
      "243\n",
      "2674\n",
      "244\n",
      "2685\n",
      "245\n",
      "2696\n",
      "246\n",
      "2707\n",
      "247\n",
      "2718\n",
      "248\n",
      "2729\n",
      "249\n",
      "2740\n",
      "250\n",
      "2751\n",
      "251\n",
      "2762\n",
      "252\n",
      "2773\n",
      "253\n",
      "2784\n",
      "254\n",
      "2795\n",
      "255\n",
      "2806\n",
      "256\n",
      "2817\n",
      "257\n",
      "2828\n",
      "258\n",
      "2839\n",
      "259\n",
      "2850\n",
      "260\n",
      "2861\n",
      "261\n",
      "2872\n",
      "262\n",
      "2883\n",
      "263\n",
      "2894\n",
      "264\n",
      "2905\n",
      "265\n",
      "2916\n",
      "266\n",
      "2927\n",
      "267\n",
      "2938\n",
      "268\n",
      "2949\n",
      "269\n",
      "2960\n",
      "270\n",
      "2971\n",
      "271\n",
      "2982\n",
      "272\n",
      "2993\n",
      "273\n",
      "3004\n",
      "274\n",
      "3015\n",
      "275\n",
      "3026\n",
      "276\n",
      "3037\n",
      "277\n",
      "3048\n",
      "278\n",
      "3059\n",
      "279\n",
      "3070\n",
      "280\n",
      "3081\n",
      "281\n",
      "3092\n",
      "282\n",
      "3103\n",
      "283\n",
      "3114\n",
      "284\n",
      "3125\n",
      "285\n",
      "3136\n",
      "286\n",
      "3147\n",
      "287\n",
      "3158\n",
      "288\n",
      "3169\n",
      "289\n",
      "3180\n",
      "290\n",
      "3191\n",
      "291\n",
      "3202\n",
      "292\n",
      "3213\n",
      "293\n",
      "3224\n",
      "294\n",
      "3235\n",
      "295\n",
      "3246\n",
      "296\n",
      "3257\n",
      "297\n",
      "3268\n",
      "298\n",
      "3279\n",
      "299\n",
      "3290\n",
      "300\n",
      "3301\n",
      "301\n",
      "3312\n",
      "302\n",
      "3323\n",
      "303\n",
      "3334\n",
      "304\n",
      "3345\n",
      "305\n",
      "3356\n",
      "306\n",
      "3367\n",
      "307\n",
      "3378\n",
      "308\n",
      "3389\n",
      "309\n",
      "3400\n",
      "310\n",
      "3411\n",
      "311\n",
      "3422\n",
      "312\n",
      "3433\n",
      "313\n",
      "3444\n",
      "314\n",
      "3455\n",
      "315\n",
      "3466\n",
      "316\n",
      "3477\n",
      "317\n",
      "3488\n",
      "318\n",
      "3499\n",
      "319\n",
      "3510\n",
      "320\n",
      "3521\n",
      "321\n",
      "3532\n",
      "322\n",
      "3543\n",
      "323\n",
      "3554\n",
      "324\n",
      "3565\n",
      "325\n",
      "3576\n",
      "326\n",
      "3587\n",
      "327\n",
      "3598\n",
      "328\n",
      "3609\n",
      "329\n",
      "3620\n",
      "330\n",
      "3631\n",
      "331\n",
      "3642\n",
      "332\n",
      "3653\n",
      "333\n",
      "3664\n",
      "334\n",
      "3675\n",
      "335\n",
      "3686\n",
      "336\n",
      "3697\n",
      "337\n",
      "3708\n",
      "338\n",
      "3719\n",
      "339\n",
      "3730\n",
      "340\n",
      "3741\n",
      "341\n",
      "3752\n",
      "342\n",
      "3763\n",
      "343\n",
      "3774\n",
      "344\n",
      "3785\n",
      "345\n",
      "3796\n",
      "346\n",
      "3807\n",
      "347\n",
      "3818\n",
      "348\n",
      "3829\n",
      "349\n",
      "3840\n",
      "350\n",
      "3851\n",
      "351\n",
      "3862\n",
      "352\n",
      "3873\n",
      "353\n",
      "3884\n",
      "354\n",
      "3895\n",
      "355\n",
      "3906\n",
      "356\n",
      "3917\n",
      "357\n",
      "3928\n",
      "358\n",
      "3939\n",
      "359\n",
      "3950\n",
      "360\n",
      "3961\n",
      "361\n",
      "3972\n",
      "362\n",
      "3983\n",
      "363\n",
      "3994\n",
      "364\n",
      "4005\n",
      "365\n",
      "4016\n",
      "366\n",
      "4027\n",
      "367\n",
      "4038\n",
      "368\n",
      "4049\n",
      "369\n",
      "4060\n",
      "370\n",
      "4071\n",
      "371\n",
      "4082\n",
      "372\n",
      "4093\n",
      "373\n",
      "4104\n",
      "374\n",
      "4115\n",
      "375\n",
      "4126\n",
      "376\n",
      "4137\n",
      "377\n",
      "4148\n",
      "378\n",
      "4159\n",
      "379\n",
      "4170\n",
      "380\n",
      "4181\n",
      "381\n",
      "4192\n",
      "382\n",
      "4203\n",
      "383\n",
      "4214\n",
      "384\n",
      "4225\n",
      "385\n",
      "4236\n",
      "386\n",
      "4247\n",
      "387\n",
      "4258\n",
      "388\n",
      "4269\n",
      "389\n",
      "4280\n",
      "390\n",
      "4291\n",
      "391\n",
      "4302\n",
      "392\n",
      "4313\n",
      "393\n",
      "4324\n",
      "394\n",
      "4335\n",
      "395\n",
      "4346\n",
      "396\n",
      "4357\n",
      "397\n",
      "4368\n",
      "398\n",
      "4379\n",
      "399\n",
      "4390\n",
      "400\n",
      "4401\n",
      "401\n",
      "4412\n",
      "402\n",
      "4423\n",
      "403\n",
      "4434\n",
      "404\n",
      "4445\n",
      "405\n",
      "4456\n",
      "406\n",
      "4467\n",
      "407\n",
      "4478\n",
      "408\n",
      "4489\n",
      "409\n",
      "4500\n",
      "410\n",
      "4511\n",
      "411\n",
      "4522\n",
      "412\n",
      "4533\n",
      "413\n",
      "4544\n",
      "414\n",
      "4555\n",
      "415\n",
      "4566\n",
      "416\n",
      "4577\n",
      "417\n",
      "4588\n",
      "418\n",
      "4599\n",
      "419\n",
      "4610\n",
      "420\n",
      "4621\n",
      "421\n",
      "4632\n",
      "422\n",
      "4643\n",
      "423\n",
      "4654\n",
      "424\n",
      "4665\n",
      "425\n",
      "4676\n",
      "426\n",
      "4687\n",
      "427\n",
      "4698\n",
      "428\n",
      "4709\n",
      "429\n",
      "4720\n",
      "430\n",
      "4731\n",
      "431\n",
      "4742\n",
      "432\n",
      "4753\n",
      "433\n",
      "4764\n",
      "434\n",
      "4775\n",
      "435\n",
      "4786\n",
      "436\n",
      "4797\n",
      "437\n",
      "4808\n",
      "438\n",
      "4819\n",
      "439\n",
      "4830\n",
      "440\n",
      "4841\n",
      "441\n",
      "4852\n",
      "442\n",
      "4863\n",
      "443\n",
      "4874\n",
      "444\n",
      "4885\n",
      "445\n",
      "4896\n",
      "446\n",
      "4907\n",
      "447\n",
      "4918\n",
      "448\n",
      "4929\n",
      "449\n",
      "4940\n",
      "450\n",
      "4951\n",
      "451\n",
      "4962\n",
      "452\n",
      "4973\n",
      "453\n",
      "4984\n",
      "454\n",
      "4995\n",
      "455\n",
      "5006\n",
      "456\n",
      "5017\n",
      "457\n",
      "5028\n",
      "458\n",
      "5039\n",
      "459\n",
      "5050\n",
      "460\n",
      "5061\n",
      "461\n",
      "5072\n",
      "462\n",
      "5083\n",
      "463\n",
      "5094\n",
      "464\n",
      "5105\n",
      "465\n",
      "5116\n",
      "466\n",
      "5127\n",
      "467\n",
      "5138\n",
      "468\n",
      "5149\n",
      "469\n",
      "5160\n",
      "470\n",
      "5171\n",
      "471\n",
      "5182\n",
      "472\n",
      "5193\n",
      "473\n",
      "5204\n",
      "474\n",
      "5215\n",
      "475\n",
      "5226\n",
      "476\n",
      "5237\n",
      "477\n",
      "5248\n",
      "478\n",
      "5259\n",
      "479\n",
      "5270\n",
      "480\n",
      "5281\n",
      "481\n",
      "5292\n",
      "482\n",
      "5303\n",
      "483\n",
      "5314\n",
      "484\n",
      "5325\n",
      "485\n",
      "5336\n",
      "486\n",
      "5347\n",
      "487\n",
      "5358\n",
      "488\n",
      "5369\n",
      "489\n",
      "5380\n",
      "490\n",
      "5391\n",
      "491\n",
      "5402\n",
      "492\n",
      "5413\n",
      "493\n",
      "5424\n",
      "494\n",
      "5435\n",
      "495\n",
      "5446\n",
      "496\n",
      "5457\n",
      "497\n",
      "5468\n",
      "498\n",
      "5479\n",
      "499\n",
      "5490\n",
      "500\n",
      "5501\n",
      "501\n",
      "5512\n",
      "502\n",
      "5523\n",
      "503\n",
      "5534\n",
      "504\n",
      "5545\n",
      "505\n",
      "5556\n",
      "506\n",
      "5567\n",
      "507\n",
      "5578\n",
      "508\n",
      "5589\n",
      "509\n",
      "5600\n",
      "510\n",
      "5611\n",
      "511\n",
      "5622\n",
      "512\n",
      "5633\n",
      "513\n",
      "5644\n",
      "514\n",
      "5655\n",
      "515\n",
      "5666\n",
      "516\n",
      "5677\n",
      "517\n",
      "5688\n",
      "518\n",
      "5699\n",
      "519\n",
      "5710\n",
      "520\n",
      "5721\n",
      "521\n",
      "5732\n",
      "522\n",
      "5743\n",
      "523\n",
      "5754\n",
      "524\n",
      "5765\n",
      "525\n",
      "5776\n",
      "526\n",
      "5787\n",
      "527\n",
      "5798\n",
      "528\n",
      "5809\n",
      "529\n",
      "5820\n",
      "530\n",
      "5831\n",
      "531\n",
      "5842\n",
      "532\n",
      "5853\n",
      "533\n",
      "5864\n",
      "534\n",
      "5875\n",
      "535\n",
      "5886\n",
      "536\n",
      "5897\n",
      "537\n",
      "5908\n",
      "538\n",
      "5919\n",
      "539\n",
      "5930\n",
      "540\n",
      "5941\n",
      "541\n",
      "5952\n",
      "542\n",
      "5963\n",
      "543\n",
      "5974\n",
      "544\n",
      "5985\n",
      "545\n",
      "5996\n",
      "546\n",
      "6007\n",
      "547\n",
      "6018\n",
      "548\n",
      "6029\n",
      "549\n",
      "6040\n",
      "550\n",
      "6051\n",
      "551\n",
      "6062\n",
      "552\n",
      "6073\n",
      "553\n",
      "6084\n",
      "554\n",
      "6095\n",
      "555\n",
      "6106\n",
      "556\n",
      "6117\n",
      "557\n",
      "6128\n",
      "558\n",
      "6139\n",
      "559\n",
      "6150\n",
      "560\n",
      "6161\n",
      "561\n",
      "6172\n",
      "562\n",
      "6183\n",
      "563\n",
      "6194\n",
      "564\n",
      "6205\n",
      "565\n",
      "6216\n",
      "566\n",
      "6227\n",
      "567\n",
      "6238\n",
      "568\n",
      "6249\n",
      "569\n",
      "6260\n",
      "570\n",
      "6271\n",
      "571\n",
      "6282\n",
      "572\n",
      "6293\n",
      "573\n",
      "6304\n",
      "574\n",
      "6315\n",
      "575\n",
      "6326\n",
      "576\n",
      "6337\n",
      "577\n",
      "6348\n",
      "578\n",
      "6359\n",
      "579\n",
      "6370\n",
      "580\n",
      "6381\n",
      "581\n",
      "6392\n",
      "582\n",
      "6403\n",
      "583\n",
      "6414\n",
      "584\n",
      "6425\n",
      "585\n",
      "6436\n",
      "586\n",
      "6447\n",
      "587\n",
      "6458\n",
      "588\n",
      "6469\n",
      "589\n",
      "6480\n",
      "590\n",
      "6491\n",
      "591\n",
      "6502\n",
      "592\n",
      "6513\n",
      "593\n",
      "6524\n",
      "594\n",
      "6535\n",
      "595\n",
      "6546\n",
      "596\n",
      "6557\n",
      "597\n",
      "6568\n",
      "598\n",
      "6579\n",
      "599\n",
      "6590\n",
      "600\n",
      "6601\n",
      "601\n",
      "6612\n",
      "602\n",
      "6623\n",
      "603\n",
      "6634\n",
      "604\n",
      "6645\n",
      "605\n",
      "6656\n",
      "606\n",
      "6667\n",
      "607\n",
      "6678\n",
      "608\n",
      "6689\n",
      "609\n",
      "6700\n",
      "610\n",
      "6711\n",
      "611\n",
      "6722\n",
      "612\n",
      "6733\n",
      "613\n",
      "6744\n",
      "614\n",
      "6755\n",
      "615\n",
      "6766\n",
      "616\n",
      "6777\n",
      "617\n",
      "6788\n",
      "618\n",
      "6799\n",
      "619\n",
      "6810\n",
      "620\n",
      "6821\n",
      "621\n",
      "6832\n",
      "622\n",
      "6843\n",
      "623\n",
      "6854\n",
      "624\n",
      "6865\n",
      "625\n",
      "6876\n",
      "626\n",
      "6887\n",
      "627\n",
      "6898\n",
      "628\n",
      "6909\n",
      "629\n",
      "6920\n",
      "630\n",
      "6931\n",
      "631\n",
      "6942\n",
      "632\n",
      "6953\n",
      "633\n",
      "6964\n",
      "634\n",
      "6975\n",
      "635\n",
      "6986\n",
      "636\n",
      "6997\n",
      "637\n",
      "7008\n",
      "638\n",
      "7019\n",
      "639\n",
      "7030\n",
      "640\n",
      "7041\n",
      "641\n",
      "7052\n",
      "642\n",
      "7063\n",
      "643\n",
      "7074\n",
      "644\n",
      "7085\n",
      "645\n",
      "7096\n",
      "646\n",
      "7107\n",
      "647\n",
      "7118\n",
      "648\n",
      "7129\n",
      "649\n",
      "7140\n",
      "650\n",
      "7151\n",
      "651\n",
      "7162\n",
      "652\n",
      "7173\n",
      "653\n",
      "7184\n",
      "654\n",
      "7195\n",
      "655\n",
      "7206\n",
      "656\n",
      "7217\n",
      "657\n",
      "7228\n",
      "658\n",
      "7239\n",
      "659\n",
      "7250\n",
      "660\n",
      "7261\n",
      "661\n",
      "7272\n",
      "662\n",
      "7283\n",
      "663\n",
      "7294\n",
      "664\n",
      "7305\n",
      "665\n",
      "7316\n",
      "666\n",
      "7327\n",
      "667\n",
      "7338\n",
      "668\n",
      "7349\n",
      "669\n",
      "7360\n",
      "670\n",
      "7371\n",
      "671\n",
      "7382\n",
      "672\n",
      "7393\n",
      "673\n",
      "7404\n",
      "674\n",
      "7415\n",
      "675\n",
      "7426\n",
      "676\n",
      "7437\n",
      "677\n",
      "7448\n",
      "678\n",
      "7459\n",
      "679\n",
      "7470\n",
      "680\n",
      "7481\n",
      "681\n",
      "7492\n",
      "682\n",
      "7503\n",
      "683\n",
      "7514\n",
      "684\n",
      "7525\n",
      "685\n",
      "7536\n",
      "686\n",
      "7547\n",
      "687\n",
      "7558\n",
      "688\n",
      "7569\n",
      "689\n",
      "7580\n",
      "690\n",
      "7591\n",
      "691\n",
      "7602\n",
      "692\n",
      "7613\n",
      "693\n",
      "7624\n",
      "694\n",
      "7635\n",
      "695\n",
      "7646\n",
      "696\n",
      "7657\n",
      "697\n",
      "7668\n",
      "698\n",
      "7679\n",
      "699\n",
      "7690\n",
      "700\n",
      "7701\n",
      "701\n",
      "7712\n",
      "702\n",
      "7723\n",
      "703\n",
      "7734\n",
      "704\n",
      "7745\n",
      "705\n",
      "7756\n",
      "706\n",
      "7767\n",
      "707\n",
      "7778\n",
      "708\n",
      "7789\n",
      "709\n",
      "7800\n",
      "710\n",
      "7811\n",
      "711\n",
      "7822\n",
      "712\n",
      "7833\n",
      "713\n",
      "7844\n",
      "714\n",
      "7855\n",
      "715\n",
      "7866\n",
      "716\n",
      "7877\n",
      "717\n",
      "7888\n",
      "718\n",
      "7899\n",
      "719\n",
      "7910\n",
      "720\n",
      "7921\n",
      "721\n",
      "7932\n",
      "722\n",
      "7943\n",
      "723\n",
      "7954\n",
      "724\n",
      "7965\n",
      "725\n",
      "7976\n",
      "726\n",
      "7987\n",
      "727\n",
      "7998\n",
      "728\n",
      "8009\n",
      "729\n",
      "8020\n",
      "730\n",
      "8031\n",
      "731\n",
      "8042\n",
      "732\n",
      "8053\n",
      "733\n",
      "8064\n",
      "734\n",
      "8075\n",
      "735\n",
      "8086\n",
      "736\n",
      "8097\n",
      "737\n",
      "8108\n",
      "738\n",
      "8119\n",
      "739\n",
      "8130\n",
      "740\n",
      "8141\n",
      "741\n",
      "8152\n",
      "742\n",
      "8163\n",
      "743\n",
      "8174\n",
      "744\n",
      "8185\n",
      "745\n",
      "8196\n",
      "746\n",
      "8207\n",
      "747\n",
      "8218\n",
      "748\n",
      "8229\n",
      "749\n",
      "8240\n",
      "750\n",
      "8251\n",
      "751\n",
      "8262\n",
      "752\n",
      "8273\n",
      "753\n",
      "8284\n",
      "754\n",
      "8295\n",
      "755\n",
      "8306\n",
      "756\n",
      "8317\n",
      "757\n",
      "8328\n",
      "758\n",
      "8339\n",
      "759\n",
      "8350\n",
      "760\n",
      "8361\n",
      "761\n",
      "8372\n",
      "762\n",
      "8383\n",
      "763\n",
      "8394\n",
      "764\n",
      "8405\n",
      "765\n",
      "8416\n",
      "766\n",
      "8427\n",
      "767\n",
      "8438\n",
      "768\n",
      "8449\n",
      "769\n",
      "8460\n",
      "770\n",
      "8471\n",
      "771\n",
      "8482\n",
      "772\n",
      "8493\n",
      "773\n",
      "8504\n",
      "774\n",
      "8515\n",
      "775\n",
      "8526\n",
      "776\n",
      "8537\n",
      "777\n",
      "8548\n",
      "778\n",
      "8559\n",
      "779\n",
      "8570\n",
      "780\n",
      "8581\n",
      "781\n",
      "8592\n",
      "782\n",
      "8603\n",
      "783\n",
      "8614\n",
      "784\n",
      "8625\n",
      "785\n",
      "8636\n",
      "786\n",
      "8647\n",
      "787\n",
      "8658\n",
      "788\n",
      "8669\n",
      "789\n",
      "8680\n",
      "790\n",
      "8691\n",
      "791\n",
      "8702\n",
      "792\n",
      "8713\n",
      "793\n",
      "8724\n",
      "794\n",
      "8735\n",
      "795\n",
      "8746\n",
      "796\n",
      "8757\n",
      "797\n",
      "8768\n",
      "798\n",
      "8779\n",
      "799\n",
      "8790\n",
      "800\n",
      "8801\n",
      "801\n",
      "8812\n",
      "802\n",
      "8823\n",
      "803\n",
      "8834\n",
      "804\n",
      "8845\n",
      "805\n",
      "8856\n",
      "806\n",
      "8867\n",
      "807\n",
      "8878\n",
      "808\n",
      "8889\n",
      "809\n",
      "8900\n",
      "810\n",
      "8911\n",
      "811\n",
      "8922\n",
      "812\n",
      "8933\n",
      "813\n",
      "8944\n",
      "814\n",
      "8955\n",
      "815\n",
      "8966\n",
      "816\n",
      "8977\n",
      "817\n",
      "8988\n",
      "818\n",
      "8999\n",
      "819\n",
      "9010\n",
      "820\n",
      "9021\n",
      "821\n",
      "9032\n",
      "822\n",
      "9043\n",
      "823\n",
      "9054\n",
      "824\n",
      "9065\n",
      "825\n",
      "9076\n",
      "826\n",
      "9087\n",
      "827\n",
      "9098\n",
      "828\n",
      "9109\n",
      "829\n",
      "9120\n",
      "830\n",
      "9131\n",
      "831\n",
      "9142\n",
      "832\n",
      "9153\n",
      "833\n",
      "9164\n",
      "834\n",
      "9175\n",
      "835\n",
      "9186\n",
      "836\n",
      "9197\n",
      "837\n",
      "9208\n",
      "838\n",
      "9219\n",
      "839\n",
      "9230\n",
      "840\n",
      "9241\n",
      "841\n",
      "9252\n",
      "842\n",
      "9263\n",
      "843\n",
      "9274\n",
      "844\n",
      "9285\n",
      "845\n",
      "9296\n",
      "846\n",
      "9307\n",
      "847\n",
      "9318\n",
      "848\n",
      "9329\n",
      "849\n",
      "9340\n",
      "850\n",
      "9351\n",
      "851\n",
      "9362\n",
      "852\n",
      "9373\n",
      "853\n",
      "9384\n",
      "854\n",
      "9395\n",
      "855\n",
      "9406\n",
      "856\n",
      "9417\n",
      "857\n",
      "9428\n",
      "858\n",
      "9439\n",
      "859\n",
      "9450\n",
      "860\n",
      "9461\n",
      "861\n",
      "9472\n",
      "862\n",
      "9483\n",
      "863\n",
      "9494\n",
      "864\n",
      "9505\n",
      "865\n",
      "9516\n",
      "866\n",
      "9527\n",
      "867\n",
      "9538\n",
      "868\n",
      "9549\n",
      "869\n",
      "9560\n",
      "870\n",
      "9571\n",
      "871\n",
      "9582\n",
      "872\n",
      "9593\n",
      "873\n",
      "9604\n",
      "874\n",
      "9615\n",
      "875\n",
      "9626\n",
      "876\n",
      "9637\n",
      "877\n",
      "9648\n",
      "878\n",
      "9659\n",
      "879\n",
      "9670\n",
      "880\n",
      "9681\n",
      "881\n",
      "9692\n",
      "882\n",
      "9703\n",
      "883\n",
      "9714\n",
      "884\n",
      "9725\n",
      "885\n",
      "9736\n",
      "886\n",
      "9747\n",
      "887\n",
      "9758\n",
      "888\n",
      "9769\n",
      "889\n",
      "9780\n",
      "890\n",
      "9791\n",
      "891\n",
      "9802\n",
      "892\n",
      "9813\n",
      "893\n",
      "9824\n",
      "894\n",
      "9835\n",
      "895\n",
      "9846\n",
      "896\n",
      "9857\n",
      "897\n",
      "9868\n",
      "898\n",
      "9879\n",
      "899\n",
      "9890\n",
      "900\n",
      "9901\n",
      "901\n",
      "9912\n",
      "902\n",
      "9923\n",
      "903\n",
      "9934\n",
      "904\n",
      "9945\n",
      "905\n",
      "9956\n",
      "906\n",
      "9967\n",
      "907\n",
      "9978\n",
      "908\n",
      "9989\n",
      "909\n",
      "10000\n",
      "910\n",
      "10011\n",
      "911\n",
      "10022\n",
      "912\n",
      "10033\n",
      "913\n",
      "10044\n",
      "914\n",
      "10055\n",
      "915\n",
      "10066\n",
      "916\n",
      "10077\n",
      "917\n",
      "10088\n",
      "918\n",
      "10099\n",
      "919\n",
      "10110\n",
      "920\n",
      "10121\n",
      "921\n",
      "10132\n",
      "922\n",
      "10143\n",
      "923\n",
      "10154\n",
      "924\n",
      "10165\n",
      "925\n",
      "10176\n",
      "926\n",
      "10187\n",
      "927\n",
      "10198\n",
      "928\n",
      "10209\n",
      "929\n",
      "10220\n",
      "930\n",
      "10231\n",
      "931\n",
      "10242\n",
      "932\n",
      "10253\n",
      "933\n",
      "10264\n",
      "934\n",
      "10275\n",
      "935\n",
      "10286\n",
      "936\n",
      "10297\n",
      "937\n",
      "10308\n",
      "938\n",
      "10319\n",
      "939\n",
      "10330\n",
      "940\n",
      "10341\n",
      "941\n",
      "10352\n",
      "942\n",
      "10363\n",
      "943\n",
      "10374\n",
      "944\n",
      "10385\n",
      "945\n",
      "10396\n",
      "946\n",
      "10407\n",
      "947\n",
      "10418\n",
      "948\n",
      "10429\n",
      "949\n",
      "10440\n",
      "950\n",
      "10451\n",
      "951\n",
      "10462\n",
      "952\n",
      "10473\n",
      "953\n",
      "10484\n",
      "954\n",
      "10495\n",
      "955\n",
      "10506\n",
      "956\n",
      "10517\n",
      "957\n",
      "10528\n",
      "958\n",
      "10539\n",
      "959\n",
      "10550\n",
      "960\n",
      "10561\n",
      "961\n",
      "10572\n",
      "962\n",
      "10583\n",
      "963\n",
      "10594\n",
      "964\n",
      "10605\n",
      "965\n",
      "10616\n",
      "966\n",
      "10627\n",
      "967\n",
      "10638\n",
      "968\n",
      "10649\n",
      "969\n",
      "10660\n",
      "970\n",
      "10671\n",
      "971\n",
      "10682\n",
      "972\n",
      "10693\n",
      "973\n",
      "10704\n"
     ]
    }
   ],
   "source": [
    "preguntas_mezcla = []\n",
    "resp_mezcla = []\n",
    "\n",
    "for i, pregunta in enumerate(preguntas_sin):\n",
    "    preguntas_mezcla.append(pregunta)\n",
    "    resp_mezcla.append(respuestas3[i])\n",
    "    print(i)\n",
    "    print(len(resp_mezcla))\n",
    "    for j in range(10):\n",
    "        preguntas_mezcla.append(mezclarPalabras(pregunta))\n",
    "        resp_mezcla.append(respuestas3[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10714"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preguntas_mezcla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mar tiene 85 céntimos y quiere comprar postales. Cada postal cuesta 9 céntimos ¿Cuántas postales puede comprar?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preguntas3[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¿cuántas botella 7 caja? Si pone dentro, tapas 7 tapas caja tapas'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preguntas_mezcla[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "preguntas3, respuestas3 = shuffle(preguntas_mezcla,resp_mezcla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminarStopWords(preg):\n",
    "    frases = []\n",
    "    palabra_sola = []\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    for i,pregunta in enumerate(preg):\n",
    "        palabras = tokenizer.tokenize(pregunta)\n",
    "        preguntas_w = []\n",
    "        for j, palabra in enumerate(reversed(palabras)):\n",
    "            preguntas_w.append(palabra)\n",
    "            palabra_sola.append(palabra)\n",
    "            if(j == 10):\n",
    "                continue\n",
    "        frases.append(preguntas_w)\n",
    "    return frases, palabra_sola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Es necesario extraer las palabras del conjunto de test.\n",
    "\n",
    "frases, palabra_sola = eliminarStopWords(preguntas3[:8500])\n",
    "frases_test, _ = eliminarStopWords(preguntas3[8500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_unicas = set(palabra_sola)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2305"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(palabras_unicas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_unicas = list(palabras_unicas)\n",
    "\n",
    "#Voy a convertir los indices a escala logaritmica para evitar que puedan reventar los pesos en la red neuronal\n",
    "vocabulario = {p:i for i, p in enumerate(p_unicas)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulario['<OOP>'] = len(vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamano max es  31  y la media de la longitud de las frases es de 12.821294117647058\n"
     ]
    }
   ],
   "source": [
    "tamanoMedio = 0\n",
    "tamanoTotal = 0\n",
    "\n",
    "for pregunta in frases:\n",
    "    if(len(pregunta) > tamanoTotal):\n",
    "        tamanoTotal = len(pregunta)\n",
    "    tamanoMedio = tamanoMedio + len(pregunta)\n",
    "\n",
    "print(\"El tamano max es \", tamanoTotal, \" y la media de la longitud de las frases es de\", tamanoMedio/len(frases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voy a meter en este vector todas mis preguntas y todas las palabras.\n",
    "# Para esta prueba, vamos a poner en la posición de la frase, el número de la palabra que estamos procesando.\n",
    "# El objetivo es procesar las palabras teniendo en cuenta el orden secuencial de la frase.\n",
    "training_X = np.zeros([len(frases), tamanoTotal, len(palabras_unicas)+1])\n",
    "test_X = np.zeros([len(frases_test), tamanoTotal, len(palabras_unicas)+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8500, 31, 2306)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_X.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pregunta in enumerate(frases):\n",
    "    for j, palabras in enumerate(pregunta):\n",
    "        training_X[i, tamanoTotal - len(pregunta) + j, vocabulario[palabras]] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cereales', 'vagaba', 'semanas', 'blancas', 'midió', 'líneas', 'premios', 'anillos', 'vez', '3', 'preparando', 'anotaría', 'borde', 'jugando', 'tobogán', 'tickets', 'asientos', 'promedio', '101', '121', 'suministros', 'larga', 'pegar', 'madre', '57', 'fuente', 'Barcelona', 'anotaron', 'Bobby', '2315', 'cartero', 'zoológico', 'crecido', 'recoger', 'Cuan', 'catedral', '14', 'afila', 'barra', 'euros', 'Roger', 'contiene', 'son', 'dan', 'velitas', 'arvejas', 'tomaron', 'nidos', 'Michelle', 'autobús', 'completas', 'tocado', 'fotogramas', 'depósitos', 'muñecos', 'patas', 'fruteros', 'have', 'fresas', 'llegaron', 'alcanzara', 'camión', 'recibido', 'problemas', 'estudiante', 'sentados', 'manzana', '324', 'Susan', 'quedan', 'competición', 'hermano', 'bajaron', 'directo', 'blusa', 'tiempo', 'turistas', 'unas', 'paga', 'colmena', 'gomas', 'pegó', 'B', 'Cuántos', 'planos', 'En', 'uno', '63', 'Juanjo', 'agregan', 'contenedor', 'Arriba', 'Lansing', 'canasta', 'preparó', 'igual', 'gradas', 'separar', 'Iván', 'primeros', 'menta', 'quedaron', 'pegan', 'gran', 'Theresa', 'donuts', '94', 'abetos', 'repartir', 'podrían', 'abeja', 'afuera', 'compañero', 'racimo', '30', 'hotel', 'Lleva', '219', 'distintas', 'tener', 'Kenia', 'Susana', 'melocotones', 'Se', 'paseó', 'Donald', 'She', 'nuevas', 'pecera', 'restaurant', 'kilos', 'Están', 'camiones', 'azulejos', 'tocan', 'flores', 'arriba', 'garaje', 'Le', 'acto', 'miembro', '25', 'marcado', 'Natalia', 'España', 'negros', 'organizados', 'rápido', 'libros', '5256', 'ayudar', '44', 'primera', 'Ambos', 'Perdió', 'sacado', 'avena', 'concierto', 'Thomas', 'peso', 'inflados', 'Marilyn', 'ladrillo', 'Ceasar', 'azules', 'grado', 'Ted', 'set', 'hojas', 'payaso', 'necesita', 'comía', 'tebeos', '9', 'vacíos', 'rompen', 'ardillas', 'materiales', 'Madrid', 'compartirlas', 'Cual', 'Ganó', 'Tienen', 'jugaron', 'factura', 'dividir', 'plazas', 'goma', 'adultos', 'comprar', 'barco', 'pequeños', 'escribiendo', '10', 'Mark', 'Karen', 'bandejas', 'sardinas', 'casilleros', 'tan', 'deben', 'habian', 'onzas', 'bebé', '2436', '18', 'Recibe', 'Gloria', 'tenis', 'trabajan', 'orejas', 'y', '38', 'comió', 'Nicolás', 'comerse', 'mañanas', 'Estás', 'casillero', '61', 'hicieron', '280', 'fallado', 'resuelto', 'Jesse', 'cuadrados', 'caballos', 'caracoles', 'Así', '287', 'tizas', '120', 'Pierde', 'volver', 'ayer', 'compuesta', 'De', 'terminó', 'ruta', 'monedero', '320', '372', 'uvas', 'padres', 'momento', '96', 'recetas', 'cabía', 'dos', 'pesarán', '873', 'autitos', 'verdes', 'Jill', 'regalar', 'Paradise', 'guardas', 'No', 'Eliana', '2', 'Lino', 'pisos', 'velocidad', 'verduleria', 'calendarios', 'reparado', 'Jack', 'almacenista', 'Kathy', 'misma', 'cabezas', 'kiosco', 'llegan', 'sólo', 'costurero', 'empezar', 'amarillas', 'jaulas', 'tapas', 'formas', 'sobraron', '252', 'bicicleta', 'huesos', 'organiza', 'carrera', 'condujeron', 'Zach', 'Ramón', 'mariposa', 'frambuesas', 'sola', 'estantería', 'junta', 'Agustín', 'oído', 'orilla', 'Sofía', 'comemos', 'compro', 'colocar', 'libreta', '157', '21', 'comieron', 'volado', 'relojes', 'noria', 'perfectos', 'Marie', 'construyendo', 'leyó', 'Compró', 'siguiente', 'sera', 'monos', '143', 'atunes', 'puestos', '1750', 'almorzar', 'nuez', 'blanca', '147', 'viajaba', 'coleccion', 'velas', 'Sara', 'cuadra', 'tía', 'saca', 'granero', 'acelgas', 'canicas', '261', 'Rupert', 'obtendra', 'armario', 'calientes', 'Luego', 'jardín', 'hoja', 'colocará', 'Doris', '200', 'sientas', 'con', 'pagó', 'Quién', 'polleras', 'ir', 'preparar', 'pegado', 'Quiere', 'extra', 'mil', 'fotografías', 'omnibus', 'Andre', 'zorra', 'semana', 'oficina', 'usan', '247', 'Primero', 'pesada', 'condujo', 'hecho', 'plantó', 'tres', 'Toda', 'queden', 'será', 'terminar', 'saco', 'segunda', 'comer', 'países', 'maduras', 'vendió', 'explotado', 'autobus', 'equipo', 'ocupadas', 'collares', 'necesitará', 'Ashley', 'Joaquín', 'puedo', 'participan', 'contenido', 'izquierdo', 'bebe', 'habita', 'ser', 'planta', 'inflado', 'papel', 'rubia', 'bolas', 'naranjas', 'perrito', 'mal', 'gustaría', 'fabricarlos', 'DVD', 'fresa', 'bocina', 'Calcular', 'viernes', 'cuentra', 'Sr', '232', 'pasado', 'caen', 'años', 'nació', 'Fede', 'marcos', 'utiliza', 'dados', 'agua', 'Yanina', 'impresionó', 'sentaron', 'corredores', 'piso', 'limón', '134', 'utilizó', 'James', 'té', '210', 'aula', 'pueden', 'amigos', 'llegó', 'llenan', 'Nicholas', 'costarían', 'frijoles', 'amarillos', 'dárselos', 'grupos', '11', 'uva', 'millas', 'cantidad', 'rotos', 'llena', 'ventanas', 'contener', 'trajo', 'colocaron', 'conejos', 'señora', 'sándwiches', 'Nintendo', 'Antes', 'Heine', 'quedo', '12', 'mandado', 'negocio', 'había', 'trabajó', 'cumpleaños', 'lotes', '860', 'página', '8', 'pila', 'comienza', 'familia', 'Robin', 'desayunar', 'ganó', 'caminó', 'tienda', 'comerciante', 'margaritas', 'mismo', '127', 'Wong', 'Una', 'otra', 'suma', 'ponerlas', 'Harry', 'hacia', '91', 'trae', 'moderna', 'Iesha', 'panecillos', 'La', 'dispuestos', 'huerto', 'billete', 'hipopotamo', 'Nicolas', 'ganar', '126', 'Kimberly', 'Rose', 'dátiles', 'granjero', 'dólares', 'Christine', '397', 'verdulería', '240', 'tendero', 'vendida', 'tardó', '792', 'gastar', 'pinos', 'tendrá', '34', 'recorre', 'Pusiste', 'traje', 'pino', 'centímetros', 'capítulos', 'crayones', 'tierra', '183', 'cabello', 'juguetes', 'Hound', 'Herrera', 'cuenta', 'paginas', 'CUantas', 'llevará', 'tamaño', 'Janice', 'reciba', 'caliente', 'queso', 'arroz', 'corresponden', '222', 'Ahora', 'formar', 'boletos', 'lápices', 'doble', 'He', 'Warren', 'garrafas', 'merluzas', 'blusas', 'Gastó', 'Pega', 'recorrerá', 'huevo', 'metros', '256', 'balones', 'aterrizan', 'hamburguesas', 'Legos', 'cien', 'alguna', 'pieza', 'llevaron', 'manzanas', 'maestra', 'hipopótamos', '269', 'zapateros', 'juntarán', 'meta', 'Louis', '380', 'botellas', 'Tenemos', 'robó', 'piezas', 'cuestan', 'pantalón', 'pastillas', 'viaje', 'viajar', 'presentes', 'maceta', 'pollitos', 'M', '1209', 'alumnos', '22', 'biblioteca', 'teníamos', 'costado', 'usar', 'Larry', 'columnas', 'aluminio', 'Craig', 'vendio', 'Vida', 'peregrinación', 'libras', 'claveles', 'chispas', 'mantengo', 'pan', 'comprando', 'ballenero', 'obrero', '54', 'rosas', 'tomar', '56', 'Tim', 'madera', 'primarias', 'tornillos', 'decidieron', 'dentro', 'Las', 'tortas', 'Arma', 'rollos', 'peral', 'pizza', '16', '2186', 'empiezan', 'llevar', 'necesito', 'hambrientas', 'Misha', 'voló', 'ambas', 'comidos', 'fabricado', 'reserva', 'examen', '270', 'salud', 'Katy', 'joven', 'nevera', 'mayorista', 'frutas', '150', 'abandonaron', 'detuvieron', 'secan', 'chicles', 'Guzmán', 'tapa', 'pagar', '117', 'sales', 'playa', 'pista', 'Recogió', 'tirados', 'tendrían', 'Roy', 'vacías', '00', 'cacahuetes', 'siempre', 'frutales', 'vuelan', 'deterioradas', 'segundo', '476', 'cuesta', 'Alrededor', '168', 'lleva', 'pulgadas', 'Germán', 'ahorró', 'continúan', 'encontré', 'siguen', 'anteojos', 'pastelería', 'tomando', '68', 'reparan', 'Lori', 'tela', 'Calcula', '3409', 'clase', 'niños', 'reparten', 'ballena', '4', 'recreo', 'búhos', 'gato', 'pasajeros', 'cuarta', 'corral', 'Sobraron', 'medias', 'fans', 'ahora', '109', 'avanzó', 'hospital', 'transporta', 'preparado', 'Beka', '160', '568', 'recogido', 'gris', 'juguetería', 'confeccionar', 'insectos', '698', '276', 'unieron', 'compañeros', 'puerta', 'etiquetas', '59', 'cosecha', 'rojo', 'estallar', 'comedor', 'baraja', 'magdalenas', 'ciudad', 'Deborah', 'coco', '578', 'Kristi', 'ramos', 'Matemáticas', 'Cade', 'puntos', 'Clarence', 'agrega', 'compras', 'medialunas', 'cabían', 'Necesita', 'pulgas', 'cerezas', 'mesas', 'bolsillo', 'centavos', 'Gaby', 'helado', 'pasteles', 'Peter', 'matemática', 'hace', 'Dorothy', '33', 'Pedimos', 'combinaciones', 'Mercedes', 'escultura', 'Diego', 'Lucas', 'cortó', 'Hay', 'baldosas', 'Teníamos', 'chocolates', 'cabe', 'abejas', 'nada', 'perro', 'hacer', 'fideos', '112', 'leer', 'contó', 'Shirley', '756', 'perros', 'pierde', 'palitos', '679', 'DVDs', 'empaqueta', 'desenterró', 'prepara', 'pinta', 'zanahoria', 'baño', 'Fred', 'violeta', 'paradas', 'logramos', 'media', '95', 'juntaran', 'Va', 'cogido', 'Jennifer', 'lenguaje', 'abuela', 'regalado', 'cucurucho', 'campo', 'pájaros', 'acuario', 'duraznos', 'peregrino', 'Hoy', 'horneó', 'cachorros', 'estrellas', 'Repartió', '544', 'Jackson', 'siete', 'podrán', 'Brenda', '58', 'quinto', 'enteras', 'camisetas', 'Añadió', 'Relevo', '119', '237', 'siembra', 'Julían', 'pescadero', 'escuala', 'viendo', 'naranjos', 'prepararon', 'Tenía', 'pelirroja', 'fábrica', 'morocha', 'auto', 'pegatinas', 'otro', 'kilómetros', 'termino', 'llenaré', 'Dave', 'gatos', 'navideños', 'Alejandra', 'chicle', 'leyendo', 'Katherine', 'quedas', 'kilo', 'perdió', 'noviembre', 'mató', 'buses', 'pies', 'organizadas', 'Ralph', 'rompieron', 'usó', '73', 'dia', 'Laura', 'pasó', 'alfajores', 'luego', '37', 'presta', 'Quiero', 'dar', 'Stephanie', 'ladrando', 'rocas', '42', 'Carolyn', 'pantalones', 'suficiente', '50', 'litros', 'Algunos', 'báscula', 'rebajas', 'indicaba', 'octubre', '1112', 'roja', 'cosechó', '78', 'mañana', 'transportará', 'material', 'bolsa', 'toma', 'plantadas', 'mes', 'Caja', 'necesitas', 'perfecta', 'de', 'fruta', 'poblaciones', 'grandes', 'lechugas', 'cartas', '300', '360', 'in', 'Aaron', 'Usó', 'cada', 'También', 'saltó', 'harina', 'golosinas', 'distancia', 'Shawn', 'repartirlas', 'bolsita', 'pesan', '24', 'tomates', 'vio', '229', '563', '43', 'paquetes', 'Paula', 'cesto', 'libro', 'asiento', 'jamon', 'torre', 'Edward', 'chocolate', 'furtivos', 'Timothy', 'repartió', 'ayudarán', 'granja', 'bolsitas', 'caramelos', 'encuentras', 'dinero', 'Harold', 'alquiló', 'aulas', 'ninguno', 'Ha', 'va', 'depósito', '900', 'goles', 'tucanes', 'tiene', 'cartera', 'caminaré', 'tendrán', 'Nathan', 'ellos', 'juegos', 'prepardo', 'primo', '342', 'Virginia', 'viajó', 'añade', 'cubo', 'botones', 'regalo', 'galón', 'Sumata', '77', '292', 'originalmente', 'camiseta', 'plantado', 'Has', 'André', 'equipos', 'rebajado', 'sentada', 'álbum', 'Come', 'frutilla', 'pudo', 'Gael', 'enrolló', 'edad', 'bien', 'lee', 'Joshua', 'volando', 'necesarios', '156', '257', 'jamón', 'hornear', 'peniques', 'volaron', 'reposeras', 'aceite', 'distribuidor', 'km', 'arenero', 'enrollaron', 'Adrián', '403', 'falta', 'CDs', '15', 'transportar', 'programa', '93', 'nadando', 'parque', 'entonces', '60', 'contienen', 'empaca', 'Brett', 'gallinas', 'sets', 'Del', 'Noelia', 'oídos', 'conduciría', 'amiga', 'Nina', 'Cuántas', 'estudiantes', 'pescado', 'Josh', '53', 'gasto', '72', 'Lemon', 'calle', 'corta', 'caramelo', 'haber', '55', 'viene', 'darle', 'pesa', 'color', 'allí', 'obtuvo', 'andado', 'Cody', 'guardan', 'vacaciones', '179', 'regalos', 'dio', 'espantar', 'hay', 'chupetines', '125', 'marcadores', 'salida', 'piña', '98', 'dejó', '51', 'bichos', 'crece', 'matemáticas', '31', '85', 'col', 'Km', 'gastó', 'socios', 'Durante', 'sobrarán', '6', 'Quieres', 'alcanzó', 'pelo', 'caja', 'bolígrafros', 'patos', 'bidón', 'Melissa', 'Jeremy', 'Mikey', 'locomotora', 'sientan', 'hoy', '354', 'Sandra', 'mascota', 'stickers', 'dividos', 'viajeros', '504', 'Norma', '23', 'ordenadores', 'Brandon', 'entran', 'debería', '2019', 'sección', 'barras', 'platos', 'butacas', 'creyones', 'cuadernos', 'Amy', 'porción', 'sobran', 'pétalos', 'Limpié', 'fútbol', 'buhos', 'cerdos', 'comparte', 'costaba', 'Halcones', 'Dylan', 'alrededor', 'mas', 'Ronald', 'derecho', 'Sue', 'mostraba', 'parte', 'Tienes', 'huevos', 'colegio', 'sombrero', 'todavía', 'regalaron', '144', 'ordenador', 'Fernando', 'escolar', '99', 'tiro', 'Jane', 'Raúl', 'distinto', 'devuelven', '998', 'invitado', 'perdí', 'termina', 'rojos', 'bosque', '465', 'Connie', 'panes', 'postal', 'película', 'venden', 'Scott', 'pelar', '19', 'retriever', 'llevó', 'comida', 'Kirsten', '90', 'minuto', 'quedara', 'juntaron', 'boa', 'ciclistas', 'latas', 'pesaba', 'odómetro', 'andar', 'Cuánta', 'libres', '47', '234', 'llevaba', '1', 'aritos', 'Green', 'Lisa', 'Mis', 'balsa', '625', '79', 'tablero', 'queda', 'alfombra', '362', '26', 'plátanos', '275', 'Cuantos', 'Maggi', '13', 'lanzado', '532', 'Marcos', 'paleta', 'salieron', 'obra', 'Él', 'divididas', 'quedaban', 'Kevin', 'transparentes', 'ponen', 'repetir', 'monedas', 'salió', 'gaseosas', 'primaria', 'Paso', 'tragó', 'vuelta', 'encuentra', '32', 'temporada', 'frascos', 'ultimo', 'marco', 'silla', 'consigue', 'Ernest', 'Brecknock', 'derriba', 'Por', 'recibió', 'Fernanda', 'Lenguaje', 'ajedrez', 'empaquetadas', 'taza', 'Javier', 'medio', '35', 'golden', 'realizó', 'mariquitas', 'sandías', 'Un', 'juntos', 'viajado', 'Darío', 'plato', 'durmiendo', '224', '375', 'Valentín', 'polo', 'llegado', 'lado', 'repartimos', 'iguales', 'naranjo', 'sala', 'vidrio', 'metieron', 'viajaban', 'comen', 'mamá', 'Shelby', 'mangos', 'miró', 'avión', 'subieron', '1028', 'vecino', '48', 'menos', 'borradores', 'Anna', 'Henry', 'reúnen', '36', '14240', 'Mariela', 'vivas', 'incompletas', 'plantas', '67', 'Evelyn', 'paseo', 'trabajo', '142', 'tejado', 'casamiento', 'debe', 'ladrar', 'pasos', '40', 'primero', 'negra', 'tartas', 'Si', '113', 'trabajar', 'veces', 'diez', '86', 'personas', '400', 'construir', 'recorrió', '187', 'Horacio', 'revistas', 'Lillian', 'patatas', 'realizado', 'silbidos', 'subido', 'partes', 'anotó', '62', 'último', 'peras', 'engordado', 'Utiliza', 'partido', 'triciclos', 'bolos', 'llenar', 'béisbol', '89', 'Botellas', 'obtuvimos', 'Habian', 'recibiría', 'garbanzos', 'bocadillos', 'Cenicienta', 'fin', 'nueces', 'Benjamin', '381', 'refresco', 'obtendrá', 'final', 'ramo', 'compitiendo', 'proyectiles', 'cuales', 'Valeria', 'Diana', 'aficionados', 'dará', 'usará', 'mascotas', 'encontrar', 'Compra', 'romper', 'tomate', 'divididos', 'cargar', '223', 'acertado', 'Marvin', '92', 'sobras', 'grillos', 'cayeron', 'terminarlo', 'repartido', 'Rachel', '70', 'ocho', '2650', 'tren', 'gastado', 'globos', 'conducía', 'galones', 'combinar', 'Víctor', 'pintura', '5', 'fabrican', 'cabalgó', 'vestirse', 'Ruth', 'Teresa', 'harán', 'eso', 'peces', 'entradas', 'capacidad', 'repetidos', 'armó', 'tiran', 'tortillas', 'torneo', 'pasada', 'Marta', 'camino', 'etapa', 'montones', '387', 'más', 'Gabriel', 'parcela', 'Marlee', 'faltan', 'empezó', 'lago', 'páginas', 'inicialmente', 'Juan', 'tengo', 'hombres', 'teja', '593', 'panadería', 'pintan', 'nacen', 'montón', 'basura', 'escritorio', 'gastan', 'sillas', 'chaquetas', 'bananas', 'dulces', 'cubre', 'hija', 'serpientes', 'marzo', 'Hasta', '550', 'comernos', 'juntan', 'compartiérais', 'masitas', 'tercera', 'Lucía', 'van', 'gasolina', '600', '124', '231', 'Brian', 'Cremes', 'lunes', 'alcanza', '294', 'tapitas', 'caballo', 'una', '7', 'Diane', 'frutera', 'silbato', 'entregar', 'costó', 'coloca', 'Son', '2438', 'invitando', 'granel', 'objetos', 'diferencia', 'practicaba', 'valen', 'treinta', 'cinco', 'cajas', 'árbol', 'Nancy', 'Compras', 'cena', 'enteros', 'doradas', 'pared', 'corrió', 'encestado', 'Después', 'borrar', 'Cuál', 'nadie', 'euro', 'jarra', '83', 'vestir', 'toque', 'selva', 'yoyo', 'Ana', 'detergente', 'recogió', 'hacerlas', '2778', 'cuento', 'almacenero', 'hermana', 'compartan', 'beben', 'discos', 'bandeja', 'rosa', 'sobra', 'estantes', 'ladrillos', 'Julian', 'total', 'pintar', 'tendra', 'recorrida', 'Manuel', 'Charles', 'cuantos', 'retiran', 'negro', 'Phillip', '4275', 'montados', 'rotas', 'litro', 'ello', 'colocamos', 'lotería', 'recoge', 'cuán', 'Cheryl', 'deberá', 'mide', 'botes', 'jueves', 'encargaron', 'caminas', 'estuche', 'hucha', 'marrones', '489', 'camisa', 'exámenes', 'ejército', '74', 'encargó', 'día', '27', 'resto', 'carta', 'galletas', 'pilas', 'bolígrafo', 'chicas', 'cuentos', 'quedó', 'ropa', 'Bay', 'escuelas', '309', 'Andrew', 'lugares', 'compró', 'Heather', 'colocados', 'Cuatro', 'ordenar', 'jugada', 'fabricará', 'Lawrence', 'envases', 'conseguido', 'Unos', 'Sheridan', 'consumido', 'trozos', 'George', 'cuentan', 'Sandy', 'Para', '65899', 'Cuantas', '212', 'café', 'cazadores', 'costará', 'profesor', 'Jacqueline', 'nuevo', 'Beberly', 'Ayer', 'touchdowns', 'irán', '145', 'pantano', 'necesitarán', 'suelo', 'corría', 'pastelitos', 'tienes', 'añaden', 'cuánto', 'Bonnie', 'lavar', 'finalizarlo', 'Ariel', 'remeras', 'descansar', 'mar', 'constrictora', '500', 'duró', 'Encuentra', 'terrible', 'boligrafos', 'clásica', 'meses', 'balcón', 'tejas', 'Roden', 'María', 'trampolín', 'Patricia', 'dividió', 'Cuanto', 'Tengo', 'podridos', 'Jovana', '115', '41', 'toda', 'desalojarse', 'invitó', '71', 'igualmente', 'paternos', 'macetas', 'recorrido', 'tíos', 'Sebastían', 'empanadas', 'cubrir', 'albañil', 'solamente', 'maternos', 'pelota', 'atletismo', 'tazas', 'Lucy', 'cuadras', 'Anne', 'manos', 'familiares', 'torta', 'compra', 'arma', 'Christopher', 'llego', 'rama', 'tenía', 'Walter', '17', 'confites', 'llenarla', 'batidos', 'regaló', 'rubias', 'paco', 'maneras', 'minutos', 'receta', 'primos', 'puntuación', '218', 'Rebecca', '344', 'patinaba', 'cochecitos', 'armar', 'mesa', 'leí', 'Jeff', 'mayor', 'Terry', 'grupo', 'naranja', 'silbatos', 'Acamparon', 'pastel', 'trabajando', 'Camino', 'tarde', 'informe', 'guardó', 'dibujo', 'Snyder', 'Carlos', 'Andres', 'Di', 'repartirlos', 'High', '69', 'deportes', 'manzano', 'Sarah', 'mitad', 'varios', 'Sean', 'tardarán', 'tulipanes', 'tantos', 'jugo', 'diferente', 'cesta', 'Helen', 'gallinero', 'calzas', 'Maria', 'tenia', '87', 'fila', 'iban', 'Wayne', 'comprarse', 'Mi', 'perdemos', 'comprarles', 'tomó', 'libra', 'viajan', 'juguetonas', 'postales', 'granos', 'marcó', 'maravilloso', 'ambos', 'Annie', 'correr', 'lápiz', 'compartir', 'doce', 'forma', 'flor', 'poner', 'pares', 'primer', 'creados', 'norma', 'mano', 'desayunos', 'parada', 'entregan', '312', 'gallina', 'cuatro', 'dieron', 'gusta', 'William', '28', 'recogieron', 'cedros', 'vecinos', 'Pepito', 'quedaran', 'coches', 'huevería', 'guardados', 'dulce', 'pone', 'todo', 'puntaje', 'lucieron', 'conchas', 'Nell', 'Santiago', 'Yo', 'Chalet', 'sábado', 'valla', 'marcha', 'tortilla', 'tendría', 'museo', 'Jazmín', 'alquiler', '648', 'rompe', 'detuvo', 'reparte', 'folios', 'Pinocho', 'porciones', 'frasco', 'vagó', 'Caminé', 'tia', 'Gavin', 'producirá', 'Pat', 'quieres', 'empieza', 'abuelos', 'restauración', 'Park', '105', 'Frances', 'Cuando', 'agricultor', '650', '479', 'vende', 'Kathryn', 'blanco', 'marcharon', 'número', 'Eugene', 'mariposas', 'ida', 'montar', 'Charlie', 'niño', 'guardadas', 'da', 'casa', 'vueltas', 'saltaron', 'fotos', 'dado', 'Julia', 'bolsas', 'lectura', 'tarro', 'Carol', 'Ya', 'Ruby', 'teatro', 'quieren', '129', 'remera', 'une', 'caminaba', 'Catherine', 'condecoraciones', 'sobrinas', 'Álvaro', 'tucán', '154', '139', 'reemplazando', 'puede', 'noche', 'inglés', 'tendré', 'Pablo', 'vale', 'centenar', 'hijo', 'planeando', 'Kyoko', 'decorar', 'disputa', 'cuantas', 'Steve', 'mercado', 'haciendo', 'año', 'Rosa', '100', 'realiza', 'peceras', 'Alejo', 'volvió', 'comprado', 'Caminas', 'Primaria', 'apples', 'amigo', 'regalan', 'Comió', 'cómic', 'Cada', 'colores', 'vienen', 'nieve', 'conducir', 'José', 'pedazos', '964', 'festiva', 'Tomás', 'completar', 'debemos', 'pizzas', 'Nuestras', 'venderlos', 'camping', 'dorados', '303', 'leche', 'recorrieron', 'aceitunas', 'reuní', 'Tréboles', '455', 'Escuela', 'comidas', '1354', 'habitación', 'entregado', 'bolitas', 'Esta', 'sellos', 'bote', 'leído', 'verduras', 'tarjetas', 'hará', 'pesado', 'entera', 'llenarán', 'ancho', 'Pedro', 'corazón', 'milla', 'seguidos', 'quiere', 'recibirá', 'heladería', 'pescó', 'Los', '542', 'profesores', 'ciencias', 'país', 'partidos', '175', 'Jean', 'nietas', 'autobuses', 'usa', '327', '217', 'rayan', 'pulseritas', 'añadir', 'jabón', 'cine', 'cuántos', 'papá', 'autos', 'Martin', 'Más', 'Garrett', 'obtiene', 'historia', 'participantes', 'zorro', 'apiladas', 'bajan', 'coche', 'castores', 'tranquilamente', 's', '1215', 'usaron', 'come', '136', 'tine', '706', 'quedarán', 'red', 'Bruce', 'centésimas', 'dormitorios', '203', 'comido', 'Tiene', 'ritmo', 'Hayley', 'principio', 'club', 'fiesta', 'atardecer', 'melones', 'vendaval', 'Emily', 'mete', 'segundos', '250', 'obtendré', 'Qué', 'vestido', 'quedaba', 'suben', '45', 'Tu', 'mujeres', 'Dos', 'longitud', 'tardará', '76', '65', 'surtidor', 'rosquillas', 'ticket', 'todos', 'colección', 'panera', '782', 'salen', 'dónde', 'coles', 'canastas', 'papa', 'A', 'viven', 'asomé', 'supermercado', 'llevado', 'horas', 'Carson', 'nutella', 'camisas', 'música', 'colocarón', 'cedro', '220', 'trajeron', 'Ann', 'Willie', 'él', 'azul', 'tienen', 'caminará', '46', 'lejos', 'patio', 'paquete', 'cerca', 'útiles', 'visitan', '9306', 'necesitó', 'trayecto', 'mercadito', 'transporte', '88', 'Tammy', 'visitando', 'John', 'Al', 'persona', 'décimas', 'Pamela', 'azúcar', 'relevos', 'retiraron', '2080', 'después', 'comieran', 'almacén', 'bus', 'unirse', 'prendas', '167', 'zapatos', 'fichas', 'semillas', 'sacos', 'Carl', 'quita', '000', '425', 'árboles', 'algodón', 'Entonces', 'juguete', 'Halloween', 'Ethan', 'cromos', 'Brad', 'nadar', 'Hizo', 'Darius', 'tercer', 'toca', 'truchas', '413', 'jarras', 'Hilt', 'vinieron', '0', 'chico', 'muebles', 'seis', 'Sra', '2987', 'curso', 'marinas', 'invitados', 'confitería', 'llenó', 'Jose', 'Pagó', 'Mildred', '49', 'entrará', 'filas', 'deberes', 'kilogramos', 'bolígrafos', 'todas', '146', '80', 'días', 'conos', '20', 'demás', 'requiere', 'Douglas', 'regala', 'también', 'Ms', 'escaparon', 'Mónica', 'perdido', 'diferentes', 'Bajaron', 'faltado', 'estante', 'sido', 'llegar', 'hora', 'viejas', 'Judy', 'Dentro', 'bloques', 'estación', 'Carter', 'verde', 'ciruelas', '29', 'puso', '966', 'adornar', 'Mary', 'Trabajó', 'pongo', 'cambio', '84', 'piruleta', 'guardará', 'recibe', 'camina', 'mph', 'Alyssa', 'Bob', 'Billy', 'hizo', 'banana', 'Joe', 'elefantes', '81', 'escuela', 'capturada', 'Algunas', 'murieron', 'niñas', 'hechas', '64', 'puesto', 'secciones', 'Joyce', 'hago', 'Antonio', 'vendido', '140', 'animales', 'chicos', 'cristales', 'Comparte', 'aumenta', 'rojas', 'varias', 'ahorrados', 'Bridget', 'serpiente', 'gramos', 'tío', 'unidades', 'mejores', 'sacó', 'añadió', 'conseguir', '52', 'Puso', 'Patrick', 'muñecas', 'Su', 'vasos', '388', 'gente', 'local', 'miércoles', 'mataron', 'gansos', 'pelotas', 'guarda', 'baloncesto', 'oficial', 'sobrina', 'amigas', 'glaseado', 'pagado', 'botella', 'sacan', 'viajes', 'produce', 'Nuestra', 'comparado', 'hebillas', 'visitar', 'amarillo', '39', '75', '82', 'Martha', 'Todd', 'nueve', 'albóndigas', 'nietos', '97', 'partió', 'si', 'jaula', 'céntimos', 'patrulla', 'función', 'revista', 'piñata', 'grande', 'mochila', 'figuritas', 'seran', 'virginia', 'rebanadas', 'lapices', 'bidones', 'periódicos', '304', 'asistieron', 'exactamente', 'Con', 'Louise', 'venta', 'largo', '750', 'jugadores', 'Había', 'necesitan', 'jugar', 'Gary', '208', 'colecciona', 'docena', 'bombones', '263', 'as', 'puertas', 'reunión', 'pelado', 'lisos', 'vender', 'frutero', 'panteras', 'tercero', 'edificio', '367', 'Matías', '110', 'padre', 'formamos', 'Eric', 'Condujeron', 'Y', 'cuántas', '3840', 'pipas', 'Esa', 'trajes', 'trasero', 'El', 'viajo', 'hijos', 'Cuánto', 'menor', 'Joan', 'luces', 'caben', 'juego', '180', '356', 'cestas', 'dolor', 'carga', '<OOP>'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulario.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pregunta in enumerate(frases_test):\n",
    "    for j, palabras in enumerate(pregunta):\n",
    "        if palabras in vocabulario:\n",
    "            test_X[i, tamanoTotal - len(pregunta) + j, vocabulario[palabras]] = 1.0\n",
    "        else:\n",
    "            test_X[i, tamanoTotal - len(pregunta) + j, vocabulario['<OOP>']] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importante, no tokenizar las palabras del conjunto de test para evitar overfitting y crear espacios para\n",
    "# palabras no vistas.\n",
    "\n",
    "training_y = np.asarray(respuestas3[:8500])\n",
    "test_y = np.asarray(respuestas3[8500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Embedding(len(vocabulario), embedding_dim),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dense(36, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-5f15418b3570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m   2349\u001b[0m     \"\"\"\n\u001b[1;32m   2350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2351\u001b[0;31m       raise ValueError('This model has not yet been built. '\n\u001b[0m\u001b[1;32m   2352\u001b[0m                        \u001b[0;34m'Build the model first by calling `build()` or calling '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2353\u001b[0m                        \u001b[0;34m'`fit()` with some data, or specify '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/140\n",
      "266/266 [==============================] - 60s 226ms/step - loss: 1.3445 - accuracy: 0.3722 - val_loss: 1.3409 - val_accuracy: 0.3722\n",
      "Epoch 2/140\n",
      "266/266 [==============================] - 61s 230ms/step - loss: 1.3423 - accuracy: 0.3728 - val_loss: 1.3440 - val_accuracy: 0.3722\n",
      "Epoch 3/140\n",
      "266/266 [==============================] - 60s 224ms/step - loss: 1.3421 - accuracy: 0.3728 - val_loss: 1.3410 - val_accuracy: 0.3722\n",
      "Epoch 4/140\n",
      "266/266 [==============================] - 60s 226ms/step - loss: 1.3421 - accuracy: 0.3728 - val_loss: 1.3410 - val_accuracy: 0.3722\n",
      "Epoch 5/140\n",
      "266/266 [==============================] - 61s 228ms/step - loss: 1.3413 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 6/140\n",
      "266/266 [==============================] - 60s 227ms/step - loss: 1.3416 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 7/140\n",
      "266/266 [==============================] - 62s 231ms/step - loss: 1.3416 - accuracy: 0.3728 - val_loss: 1.3413 - val_accuracy: 0.3722\n",
      "Epoch 8/140\n",
      "266/266 [==============================] - 61s 228ms/step - loss: 1.3414 - accuracy: 0.3728 - val_loss: 1.3409 - val_accuracy: 0.3722\n",
      "Epoch 9/140\n",
      "266/266 [==============================] - 59s 220ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3407 - val_accuracy: 0.3722\n",
      "Epoch 10/140\n",
      "266/266 [==============================] - 55s 207ms/step - loss: 1.3406 - accuracy: 0.3728 - val_loss: 1.3412 - val_accuracy: 0.3722\n",
      "Epoch 11/140\n",
      "266/266 [==============================] - 55s 206ms/step - loss: 1.3412 - accuracy: 0.3728 - val_loss: 1.3415 - val_accuracy: 0.3722\n",
      "Epoch 12/140\n",
      "266/266 [==============================] - 54s 204ms/step - loss: 1.3411 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 13/140\n",
      "266/266 [==============================] - 55s 205ms/step - loss: 1.3410 - accuracy: 0.3728 - val_loss: 1.3410 - val_accuracy: 0.3722\n",
      "Epoch 14/140\n",
      "266/266 [==============================] - 54s 201ms/step - loss: 1.3416 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 15/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3415 - accuracy: 0.3728 - val_loss: 1.3412 - val_accuracy: 0.3722\n",
      "Epoch 16/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3410 - accuracy: 0.3728 - val_loss: 1.3409 - val_accuracy: 0.3722\n",
      "Epoch 17/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3410 - accuracy: 0.3728 - val_loss: 1.3409 - val_accuracy: 0.3722\n",
      "Epoch 18/140\n",
      "266/266 [==============================] - 57s 216ms/step - loss: 1.3409 - accuracy: 0.3728 - val_loss: 1.3410 - val_accuracy: 0.3722\n",
      "Epoch 19/140\n",
      "266/266 [==============================] - 54s 203ms/step - loss: 1.3407 - accuracy: 0.3728 - val_loss: 1.3414 - val_accuracy: 0.3722\n",
      "Epoch 20/140\n",
      "266/266 [==============================] - 54s 204ms/step - loss: 1.3408 - accuracy: 0.3728 - val_loss: 1.3409 - val_accuracy: 0.3722\n",
      "Epoch 21/140\n",
      "266/266 [==============================] - 54s 204ms/step - loss: 1.3411 - accuracy: 0.3728 - val_loss: 1.3409 - val_accuracy: 0.3722\n",
      "Epoch 22/140\n",
      "266/266 [==============================] - 54s 204ms/step - loss: 1.3412 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 23/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3408 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 24/140\n",
      "266/266 [==============================] - 54s 203ms/step - loss: 1.3407 - accuracy: 0.3728 - val_loss: 1.3413 - val_accuracy: 0.3722\n",
      "Epoch 25/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3407 - accuracy: 0.3728 - val_loss: 1.3411 - val_accuracy: 0.3722\n",
      "Epoch 26/140\n",
      "266/266 [==============================] - 54s 201ms/step - loss: 1.3406 - accuracy: 0.3728 - val_loss: 1.3411 - val_accuracy: 0.3722\n",
      "Epoch 27/140\n",
      "266/266 [==============================] - 54s 204ms/step - loss: 1.3411 - accuracy: 0.3728 - val_loss: 1.3407 - val_accuracy: 0.3722\n",
      "Epoch 28/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3413 - val_accuracy: 0.3722\n",
      "Epoch 29/140\n",
      "266/266 [==============================] - 54s 204ms/step - loss: 1.3413 - accuracy: 0.3728 - val_loss: 1.3409 - val_accuracy: 0.3722\n",
      "Epoch 30/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3409 - accuracy: 0.3728 - val_loss: 1.3409 - val_accuracy: 0.3722\n",
      "Epoch 31/140\n",
      "266/266 [==============================] - 54s 203ms/step - loss: 1.3409 - accuracy: 0.3728 - val_loss: 1.3409 - val_accuracy: 0.3722\n",
      "Epoch 32/140\n",
      "266/266 [==============================] - 53s 200ms/step - loss: 1.3412 - accuracy: 0.3728 - val_loss: 1.3410 - val_accuracy: 0.3722\n",
      "Epoch 33/140\n",
      "266/266 [==============================] - 55s 208ms/step - loss: 1.3410 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 34/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3409 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 35/140\n",
      "266/266 [==============================] - 54s 203ms/step - loss: 1.3409 - accuracy: 0.3728 - val_loss: 1.3407 - val_accuracy: 0.3722\n",
      "Epoch 36/140\n",
      "266/266 [==============================] - 56s 209ms/step - loss: 1.3409 - accuracy: 0.3728 - val_loss: 1.3409 - val_accuracy: 0.3722\n",
      "Epoch 37/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3410 - accuracy: 0.3728 - val_loss: 1.3409 - val_accuracy: 0.3722\n",
      "Epoch 38/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3409 - val_accuracy: 0.3722\n",
      "Epoch 39/140\n",
      "266/266 [==============================] - 53s 200ms/step - loss: 1.3409 - accuracy: 0.3728 - val_loss: 1.3409 - val_accuracy: 0.3722\n",
      "Epoch 40/140\n",
      "266/266 [==============================] - 54s 201ms/step - loss: 1.3409 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 41/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3412 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 42/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3409 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 43/140\n",
      "266/266 [==============================] - 53s 200ms/step - loss: 1.3409 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 44/140\n",
      "266/266 [==============================] - 55s 206ms/step - loss: 1.3410 - accuracy: 0.3728 - val_loss: 1.3410 - val_accuracy: 0.3722\n",
      "Epoch 45/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3409 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 46/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3407 - accuracy: 0.3728 - val_loss: 1.3410 - val_accuracy: 0.3722\n",
      "Epoch 47/140\n",
      "266/266 [==============================] - 53s 201ms/step - loss: 1.3410 - accuracy: 0.3728 - val_loss: 1.3409 - val_accuracy: 0.3722\n",
      "Epoch 48/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3410 - accuracy: 0.3728 - val_loss: 1.3411 - val_accuracy: 0.3722\n",
      "Epoch 49/140\n",
      "266/266 [==============================] - 53s 201ms/step - loss: 1.3409 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 50/140\n",
      "266/266 [==============================] - 53s 201ms/step - loss: 1.3406 - accuracy: 0.3728 - val_loss: 1.3409 - val_accuracy: 0.3722\n",
      "Epoch 51/140\n",
      "266/266 [==============================] - 53s 201ms/step - loss: 1.3410 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 52/140\n",
      "266/266 [==============================] - 53s 201ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3413 - val_accuracy: 0.3722\n",
      "Epoch 53/140\n",
      "266/266 [==============================] - 55s 207ms/step - loss: 1.3406 - accuracy: 0.3728 - val_loss: 1.3410 - val_accuracy: 0.3722\n",
      "Epoch 54/140\n",
      "266/266 [==============================] - 55s 208ms/step - loss: 1.3411 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 55/140\n",
      "266/266 [==============================] - 54s 204ms/step - loss: 1.3404 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 56/140\n",
      "266/266 [==============================] - 54s 201ms/step - loss: 1.3412 - accuracy: 0.3728 - val_loss: 1.3409 - val_accuracy: 0.3722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/140\n",
      "266/266 [==============================] - 53s 201ms/step - loss: 1.3409 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 58/140\n",
      "266/266 [==============================] - 54s 204ms/step - loss: 1.3404 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 59/140\n",
      "266/266 [==============================] - 53s 201ms/step - loss: 1.3407 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 60/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 61/140\n",
      "266/266 [==============================] - 54s 201ms/step - loss: 1.3409 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 62/140\n",
      "266/266 [==============================] - 53s 200ms/step - loss: 1.3406 - accuracy: 0.3728 - val_loss: 1.3413 - val_accuracy: 0.3722\n",
      "Epoch 63/140\n",
      "266/266 [==============================] - 53s 201ms/step - loss: 1.3406 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 64/140\n",
      "266/266 [==============================] - 53s 200ms/step - loss: 1.3406 - accuracy: 0.3728 - val_loss: 1.3407 - val_accuracy: 0.3722\n",
      "Epoch 65/140\n",
      "266/266 [==============================] - 53s 201ms/step - loss: 1.3409 - accuracy: 0.3728 - val_loss: 1.3409 - val_accuracy: 0.3722\n",
      "Epoch 66/140\n",
      "266/266 [==============================] - 55s 206ms/step - loss: 1.3407 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 67/140\n",
      "266/266 [==============================] - 53s 200ms/step - loss: 1.3407 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 68/140\n",
      "266/266 [==============================] - 54s 201ms/step - loss: 1.3408 - accuracy: 0.3728 - val_loss: 1.3409 - val_accuracy: 0.3722\n",
      "Epoch 69/140\n",
      "266/266 [==============================] - 54s 201ms/step - loss: 1.3408 - accuracy: 0.3728 - val_loss: 1.3407 - val_accuracy: 0.3722\n",
      "Epoch 70/140\n",
      "266/266 [==============================] - 53s 201ms/step - loss: 1.3408 - accuracy: 0.3728 - val_loss: 1.3409 - val_accuracy: 0.3722\n",
      "Epoch 71/140\n",
      "266/266 [==============================] - 55s 208ms/step - loss: 1.3408 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 72/140\n",
      "266/266 [==============================] - 53s 200ms/step - loss: 1.3409 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 73/140\n",
      "266/266 [==============================] - 53s 201ms/step - loss: 1.3407 - accuracy: 0.3728 - val_loss: 1.3407 - val_accuracy: 0.3722\n",
      "Epoch 74/140\n",
      "266/266 [==============================] - 53s 201ms/step - loss: 1.3407 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 75/140\n",
      "266/266 [==============================] - 53s 201ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 76/140\n",
      "266/266 [==============================] - 53s 200ms/step - loss: 1.3410 - accuracy: 0.3728 - val_loss: 1.3409 - val_accuracy: 0.3722\n",
      "Epoch 77/140\n",
      "266/266 [==============================] - 56s 211ms/step - loss: 1.3408 - accuracy: 0.3728 - val_loss: 1.3407 - val_accuracy: 0.3722\n",
      "Epoch 78/140\n",
      "266/266 [==============================] - 53s 201ms/step - loss: 1.3407 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 79/140\n",
      "266/266 [==============================] - 53s 200ms/step - loss: 1.3407 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 80/140\n",
      "266/266 [==============================] - 53s 200ms/step - loss: 1.3406 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 81/140\n",
      "266/266 [==============================] - 53s 198ms/step - loss: 1.3406 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 82/140\n",
      "266/266 [==============================] - 53s 200ms/step - loss: 1.3407 - accuracy: 0.3728 - val_loss: 1.3409 - val_accuracy: 0.3722\n",
      "Epoch 83/140\n",
      "266/266 [==============================] - 53s 199ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3411 - val_accuracy: 0.3722\n",
      "Epoch 84/140\n",
      "266/266 [==============================] - 53s 200ms/step - loss: 1.3407 - accuracy: 0.3728 - val_loss: 1.3409 - val_accuracy: 0.3722\n",
      "Epoch 85/140\n",
      "266/266 [==============================] - 53s 200ms/step - loss: 1.3406 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 86/140\n",
      "266/266 [==============================] - 53s 198ms/step - loss: 1.3406 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 87/140\n",
      "266/266 [==============================] - 53s 200ms/step - loss: 1.3409 - accuracy: 0.3728 - val_loss: 1.3407 - val_accuracy: 0.3722\n",
      "Epoch 88/140\n",
      "266/266 [==============================] - 54s 201ms/step - loss: 1.3408 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 89/140\n",
      "266/266 [==============================] - 57s 215ms/step - loss: 1.3409 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 90/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3408 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 91/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3406 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 92/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3408 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 93/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3407 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 94/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 95/140\n",
      "266/266 [==============================] - 53s 201ms/step - loss: 1.3403 - accuracy: 0.3728 - val_loss: 1.3412 - val_accuracy: 0.3722\n",
      "Epoch 96/140\n",
      "266/266 [==============================] - 54s 204ms/step - loss: 1.3407 - accuracy: 0.3728 - val_loss: 1.3409 - val_accuracy: 0.3722\n",
      "Epoch 97/140\n",
      "266/266 [==============================] - 53s 201ms/step - loss: 1.3406 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 98/140\n",
      "266/266 [==============================] - 54s 203ms/step - loss: 1.3406 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 99/140\n",
      "266/266 [==============================] - 55s 208ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 100/140\n",
      "266/266 [==============================] - 54s 204ms/step - loss: 1.3406 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 101/140\n",
      "266/266 [==============================] - 54s 203ms/step - loss: 1.3407 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 102/140\n",
      "266/266 [==============================] - 54s 203ms/step - loss: 1.3407 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 103/140\n",
      "266/266 [==============================] - 54s 201ms/step - loss: 1.3407 - accuracy: 0.3728 - val_loss: 1.3409 - val_accuracy: 0.3722\n",
      "Epoch 104/140\n",
      "266/266 [==============================] - 54s 203ms/step - loss: 1.3406 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 105/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3409 - val_accuracy: 0.3722\n",
      "Epoch 106/140\n",
      "266/266 [==============================] - 53s 200ms/step - loss: 1.3407 - accuracy: 0.3728 - val_loss: 1.3407 - val_accuracy: 0.3722\n",
      "Epoch 107/140\n",
      "266/266 [==============================] - 57s 213ms/step - loss: 1.3404 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 108/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 109/140\n",
      "266/266 [==============================] - 54s 204ms/step - loss: 1.3404 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 110/140\n",
      "266/266 [==============================] - 55s 206ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3407 - val_accuracy: 0.3722\n",
      "Epoch 111/140\n",
      "266/266 [==============================] - 54s 203ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3409 - val_accuracy: 0.3722\n",
      "Epoch 112/140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 113/140\n",
      "266/266 [==============================] - 54s 203ms/step - loss: 1.3406 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 114/140\n",
      "266/266 [==============================] - 53s 201ms/step - loss: 1.3407 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 115/140\n",
      "266/266 [==============================] - 54s 203ms/step - loss: 1.3406 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 116/140\n",
      "266/266 [==============================] - 54s 203ms/step - loss: 1.3404 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 117/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3407 - val_accuracy: 0.3722\n",
      "Epoch 118/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 119/140\n",
      "266/266 [==============================] - 53s 200ms/step - loss: 1.3404 - accuracy: 0.3728 - val_loss: 1.3407 - val_accuracy: 0.3722\n",
      "Epoch 120/140\n",
      "266/266 [==============================] - 53s 201ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 121/140\n",
      "266/266 [==============================] - 55s 206ms/step - loss: 1.3406 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 122/140\n",
      "266/266 [==============================] - 54s 203ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 123/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3406 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 124/140\n",
      "266/266 [==============================] - 55s 208ms/step - loss: 1.3404 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 125/140\n",
      "266/266 [==============================] - 56s 210ms/step - loss: 1.3404 - accuracy: 0.3728 - val_loss: 1.3407 - val_accuracy: 0.3722\n",
      "Epoch 126/140\n",
      "266/266 [==============================] - 54s 203ms/step - loss: 1.3404 - accuracy: 0.3728 - val_loss: 1.3407 - val_accuracy: 0.3722\n",
      "Epoch 127/140\n",
      "266/266 [==============================] - 53s 201ms/step - loss: 1.3404 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 128/140\n",
      "266/266 [==============================] - 53s 200ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3407 - val_accuracy: 0.3722\n",
      "Epoch 129/140\n",
      "266/266 [==============================] - 54s 203ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 130/140\n",
      "266/266 [==============================] - 54s 205ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 131/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3407 - val_accuracy: 0.3722\n",
      "Epoch 132/140\n",
      "266/266 [==============================] - 55s 207ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 133/140\n",
      "266/266 [==============================] - 54s 203ms/step - loss: 1.3406 - accuracy: 0.3728 - val_loss: 1.3407 - val_accuracy: 0.3722\n",
      "Epoch 134/140\n",
      "266/266 [==============================] - 54s 203ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3407 - val_accuracy: 0.3722\n",
      "Epoch 135/140\n",
      "266/266 [==============================] - 54s 203ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3407 - val_accuracy: 0.3722\n",
      "Epoch 136/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3407 - val_accuracy: 0.3722\n",
      "Epoch 137/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3404 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 138/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3404 - accuracy: 0.3728 - val_loss: 1.3407 - val_accuracy: 0.3722\n",
      "Epoch 139/140\n",
      "266/266 [==============================] - 54s 202ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3408 - val_accuracy: 0.3722\n",
      "Epoch 140/140\n",
      "266/266 [==============================] - 54s 203ms/step - loss: 1.3405 - accuracy: 0.3728 - val_loss: 1.3407 - val_accuracy: 0.3722\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(training_X, training_y, epochs = 140, validation_data=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.Sequential([\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(128, dropout = 0.2, input_shape=[len(vocabulario), training_X.shape[1], training_X.shape[2]])),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "opt = keras.optimizers.RMSprop(learning_rate=0.001,rho=0.9, momentum=0.0, epsilon=1e-07, centered=False)\n",
    "\n",
    "#opt = keras.optimizers.SGD(lr=0.00001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#opt = keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "model2.compile(loss='sparse_categorical_crossentropy',optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/240\n",
      "67/67 [==============================] - 66s 988ms/step - loss: 0.7078 - accuracy: 0.7116 - val_loss: 0.3008 - val_accuracy: 0.8794\n",
      "Epoch 2/240\n",
      "67/67 [==============================] - 79s 1s/step - loss: 0.2277 - accuracy: 0.9267 - val_loss: 0.1213 - val_accuracy: 0.9666\n",
      "Epoch 3/240\n",
      "67/67 [==============================] - 77s 1s/step - loss: 0.1279 - accuracy: 0.9619 - val_loss: 0.0507 - val_accuracy: 0.9855\n",
      "Epoch 4/240\n",
      "67/67 [==============================] - 78s 1s/step - loss: 0.1424 - accuracy: 0.9688 - val_loss: 0.0319 - val_accuracy: 0.9905\n",
      "Epoch 5/240\n",
      "67/67 [==============================] - 73s 1s/step - loss: 0.0587 - accuracy: 0.9834 - val_loss: 0.0199 - val_accuracy: 0.9959\n",
      "Epoch 6/240\n",
      "67/67 [==============================] - 75s 1s/step - loss: 0.0487 - accuracy: 0.9856 - val_loss: 0.0234 - val_accuracy: 0.9986\n",
      "Epoch 7/240\n",
      "67/67 [==============================] - 84s 1s/step - loss: 0.0432 - accuracy: 0.9862 - val_loss: 0.0178 - val_accuracy: 0.9964\n",
      "Epoch 8/240\n",
      "67/67 [==============================] - 79s 1s/step - loss: 0.0327 - accuracy: 0.9888 - val_loss: 0.0118 - val_accuracy: 0.9946\n",
      "Epoch 9/240\n",
      "67/67 [==============================] - 90s 1s/step - loss: 0.0316 - accuracy: 0.9907 - val_loss: 0.0057 - val_accuracy: 0.9986\n",
      "Epoch 10/240\n",
      "67/67 [==============================] - 75s 1s/step - loss: 0.0220 - accuracy: 0.9927 - val_loss: 0.0113 - val_accuracy: 0.9964\n",
      "Epoch 11/240\n",
      "67/67 [==============================] - 77s 1s/step - loss: 0.0192 - accuracy: 0.9942 - val_loss: 0.0178 - val_accuracy: 0.9937\n",
      "Epoch 12/240\n",
      "67/67 [==============================] - 77s 1s/step - loss: 0.0218 - accuracy: 0.9941 - val_loss: 0.0051 - val_accuracy: 0.9977\n",
      "Epoch 13/240\n",
      "67/67 [==============================] - 76s 1s/step - loss: 0.0241 - accuracy: 0.9938 - val_loss: 0.0061 - val_accuracy: 0.9968\n",
      "Epoch 14/240\n",
      "67/67 [==============================] - 78s 1s/step - loss: 0.0183 - accuracy: 0.9949 - val_loss: 0.0057 - val_accuracy: 0.9973\n",
      "Epoch 15/240\n",
      "67/67 [==============================] - 83s 1s/step - loss: 0.0152 - accuracy: 0.9947 - val_loss: 0.0031 - val_accuracy: 0.9991\n",
      "Epoch 16/240\n",
      "67/67 [==============================] - 82s 1s/step - loss: 0.0140 - accuracy: 0.9954 - val_loss: 0.0087 - val_accuracy: 0.9968\n",
      "Epoch 17/240\n",
      "67/67 [==============================] - 87s 1s/step - loss: 0.0129 - accuracy: 0.9954 - val_loss: 0.0036 - val_accuracy: 0.9982\n",
      "Epoch 18/240\n",
      "67/67 [==============================] - 88s 1s/step - loss: 0.0113 - accuracy: 0.9958 - val_loss: 0.0132 - val_accuracy: 0.9973\n",
      "Epoch 19/240\n",
      "67/67 [==============================] - 100s 1s/step - loss: 0.0118 - accuracy: 0.9958 - val_loss: 0.0048 - val_accuracy: 0.9968\n",
      "Epoch 20/240\n",
      "67/67 [==============================] - 99s 1s/step - loss: 0.0110 - accuracy: 0.9961 - val_loss: 0.0370 - val_accuracy: 0.9914\n",
      "Epoch 21/240\n",
      "67/67 [==============================] - 74s 1s/step - loss: 0.0101 - accuracy: 0.9968 - val_loss: 0.0037 - val_accuracy: 0.9982\n",
      "Epoch 22/240\n",
      "67/67 [==============================] - 70s 1s/step - loss: 0.0104 - accuracy: 0.9964 - val_loss: 0.0091 - val_accuracy: 0.9968\n",
      "Epoch 23/240\n",
      "67/67 [==============================] - 73s 1s/step - loss: 0.0085 - accuracy: 0.9967 - val_loss: 0.0063 - val_accuracy: 0.9977\n",
      "Epoch 24/240\n",
      "67/67 [==============================] - 75s 1s/step - loss: 0.0100 - accuracy: 0.9966 - val_loss: 0.0086 - val_accuracy: 0.9973\n",
      "Epoch 25/240\n",
      "67/67 [==============================] - 71s 1s/step - loss: 0.0088 - accuracy: 0.9962 - val_loss: 0.0037 - val_accuracy: 0.9982\n",
      "Epoch 26/240\n",
      "67/67 [==============================] - 72s 1s/step - loss: 0.0094 - accuracy: 0.9972 - val_loss: 0.0053 - val_accuracy: 0.9973\n",
      "Epoch 27/240\n",
      "67/67 [==============================] - 69s 1s/step - loss: 0.0069 - accuracy: 0.9971 - val_loss: 0.0048 - val_accuracy: 0.9986\n",
      "Epoch 28/240\n",
      "67/67 [==============================] - 64s 953ms/step - loss: 0.0087 - accuracy: 0.9972 - val_loss: 0.0034 - val_accuracy: 0.9977\n",
      "Epoch 29/240\n",
      "67/67 [==============================] - 64s 950ms/step - loss: 0.0069 - accuracy: 0.9978 - val_loss: 0.0036 - val_accuracy: 0.9986\n",
      "Epoch 30/240\n",
      "67/67 [==============================] - 65s 966ms/step - loss: 0.0098 - accuracy: 0.9964 - val_loss: 0.0058 - val_accuracy: 0.9973\n",
      "Epoch 31/240\n",
      "67/67 [==============================] - 64s 949ms/step - loss: 0.0072 - accuracy: 0.9978 - val_loss: 0.0471 - val_accuracy: 0.9887\n",
      "Epoch 32/240\n",
      "67/67 [==============================] - 66s 989ms/step - loss: 0.0082 - accuracy: 0.9972 - val_loss: 0.0433 - val_accuracy: 0.9883\n",
      "Epoch 33/240\n",
      "67/67 [==============================] - 68s 1s/step - loss: 0.0066 - accuracy: 0.9980 - val_loss: 0.0057 - val_accuracy: 0.9977\n",
      "Epoch 34/240\n",
      "67/67 [==============================] - 65s 971ms/step - loss: 0.0074 - accuracy: 0.9979 - val_loss: 0.0049 - val_accuracy: 0.9982\n",
      "Epoch 35/240\n",
      "67/67 [==============================] - 64s 962ms/step - loss: 0.0052 - accuracy: 0.9975 - val_loss: 0.0058 - val_accuracy: 0.9982\n",
      "Epoch 36/240\n",
      "67/67 [==============================] - 63s 946ms/step - loss: 0.0064 - accuracy: 0.9979 - val_loss: 0.0094 - val_accuracy: 0.9973\n",
      "Epoch 37/240\n",
      "67/67 [==============================] - 65s 969ms/step - loss: 0.0036 - accuracy: 0.9987 - val_loss: 0.0073 - val_accuracy: 0.9986\n",
      "Epoch 38/240\n",
      "67/67 [==============================] - 63s 937ms/step - loss: 0.0100 - accuracy: 0.9974 - val_loss: 0.0064 - val_accuracy: 0.9977\n",
      "Epoch 39/240\n",
      "67/67 [==============================] - 64s 955ms/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 0.0076 - val_accuracy: 0.9977\n",
      "Epoch 40/240\n",
      "67/67 [==============================] - 63s 939ms/step - loss: 0.0074 - accuracy: 0.9971 - val_loss: 0.0068 - val_accuracy: 0.9977\n",
      "Epoch 41/240\n",
      "67/67 [==============================] - 64s 956ms/step - loss: 0.0063 - accuracy: 0.9980 - val_loss: 0.0064 - val_accuracy: 0.9977\n",
      "Epoch 42/240\n",
      "67/67 [==============================] - 65s 971ms/step - loss: 0.0062 - accuracy: 0.9979 - val_loss: 0.0260 - val_accuracy: 0.9923\n",
      "Epoch 43/240\n",
      "67/67 [==============================] - 66s 988ms/step - loss: 0.0065 - accuracy: 0.9979 - val_loss: 0.0086 - val_accuracy: 0.9973\n",
      "Epoch 44/240\n",
      "67/67 [==============================] - 64s 960ms/step - loss: 0.0059 - accuracy: 0.9976 - val_loss: 0.0081 - val_accuracy: 0.9982\n",
      "Epoch 45/240\n",
      "67/67 [==============================] - 63s 934ms/step - loss: 0.0045 - accuracy: 0.9985 - val_loss: 0.0054 - val_accuracy: 0.9982\n",
      "Epoch 46/240\n",
      "67/67 [==============================] - 64s 950ms/step - loss: 0.0067 - accuracy: 0.9980 - val_loss: 0.0091 - val_accuracy: 0.9968\n",
      "Epoch 47/240\n",
      "67/67 [==============================] - 62s 931ms/step - loss: 0.0056 - accuracy: 0.9972 - val_loss: 0.0066 - val_accuracy: 0.9977\n",
      "Epoch 48/240\n",
      "67/67 [==============================] - 79s 1s/step - loss: 0.0039 - accuracy: 0.9987 - val_loss: 0.0089 - val_accuracy: 0.9973\n",
      "Epoch 49/240\n",
      "67/67 [==============================] - 67s 1s/step - loss: 0.0065 - accuracy: 0.9978 - val_loss: 0.0061 - val_accuracy: 0.9982\n",
      "Epoch 50/240\n",
      "67/67 [==============================] - 68s 1s/step - loss: 0.0063 - accuracy: 0.9976 - val_loss: 0.0082 - val_accuracy: 0.9973\n",
      "Epoch 51/240\n",
      "67/67 [==============================] - 72s 1s/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.0059 - val_accuracy: 0.9986\n",
      "Epoch 52/240\n",
      "67/67 [==============================] - 68s 1s/step - loss: 0.0036 - accuracy: 0.9985 - val_loss: 0.0076 - val_accuracy: 0.9973\n",
      "Epoch 53/240\n",
      "67/67 [==============================] - 71s 1s/step - loss: 0.0052 - accuracy: 0.9986 - val_loss: 0.0058 - val_accuracy: 0.9986\n",
      "Epoch 54/240\n",
      "67/67 [==============================] - 68s 1s/step - loss: 0.0038 - accuracy: 0.9978 - val_loss: 0.0065 - val_accuracy: 0.9986\n",
      "Epoch 55/240\n",
      "67/67 [==============================] - 70s 1s/step - loss: 0.0050 - accuracy: 0.9981 - val_loss: 0.0082 - val_accuracy: 0.9977\n",
      "Epoch 56/240\n",
      "67/67 [==============================] - 65s 969ms/step - loss: 0.0036 - accuracy: 0.9986 - val_loss: 0.0060 - val_accuracy: 0.9982\n",
      "Epoch 57/240\n",
      "67/67 [==============================] - 64s 960ms/step - loss: 0.0057 - accuracy: 0.9984 - val_loss: 0.0085 - val_accuracy: 0.9973\n",
      "Epoch 58/240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 62s 929ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.0090 - val_accuracy: 0.9973\n",
      "Epoch 59/240\n",
      "67/67 [==============================] - 62s 923ms/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.0045 - val_accuracy: 0.9986\n",
      "Epoch 60/240\n",
      "67/67 [==============================] - 65s 977ms/step - loss: 0.0041 - accuracy: 0.9987 - val_loss: 0.0067 - val_accuracy: 0.9982\n",
      "Epoch 61/240\n",
      "67/67 [==============================] - 64s 952ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.0063 - val_accuracy: 0.9982\n",
      "Epoch 62/240\n",
      "67/67 [==============================] - 67s 998ms/step - loss: 0.0040 - accuracy: 0.9985 - val_loss: 0.0068 - val_accuracy: 0.9982\n",
      "Epoch 63/240\n",
      "67/67 [==============================] - 63s 936ms/step - loss: 0.0050 - accuracy: 0.9978 - val_loss: 0.0064 - val_accuracy: 0.9982\n",
      "Epoch 64/240\n",
      "67/67 [==============================] - 64s 959ms/step - loss: 0.0055 - accuracy: 0.9981 - val_loss: 0.0146 - val_accuracy: 0.9964\n",
      "Epoch 65/240\n",
      "67/67 [==============================] - 62s 927ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.0143 - val_accuracy: 0.9968\n",
      "Epoch 66/240\n",
      "67/67 [==============================] - 64s 955ms/step - loss: 0.0042 - accuracy: 0.9985 - val_loss: 0.0064 - val_accuracy: 0.9977\n",
      "Epoch 67/240\n",
      "67/67 [==============================] - 63s 942ms/step - loss: 0.0057 - accuracy: 0.9986 - val_loss: 0.0064 - val_accuracy: 0.9973\n",
      "Epoch 68/240\n",
      "67/67 [==============================] - 63s 935ms/step - loss: 0.0028 - accuracy: 0.9987 - val_loss: 0.0094 - val_accuracy: 0.9982\n",
      "Epoch 69/240\n",
      "67/67 [==============================] - 65s 967ms/step - loss: 0.0035 - accuracy: 0.9988 - val_loss: 0.0138 - val_accuracy: 0.9973\n",
      "Epoch 70/240\n",
      "67/67 [==============================] - 70s 1s/step - loss: 0.0032 - accuracy: 0.9987 - val_loss: 0.0058 - val_accuracy: 0.9977\n",
      "Epoch 71/240\n",
      "67/67 [==============================] - 62s 932ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0083 - val_accuracy: 0.9977\n",
      "Epoch 72/240\n",
      "67/67 [==============================] - 63s 936ms/step - loss: 0.0023 - accuracy: 0.9991 - val_loss: 0.0127 - val_accuracy: 0.9973\n",
      "Epoch 73/240\n",
      "67/67 [==============================] - 62s 925ms/step - loss: 0.0049 - accuracy: 0.9981 - val_loss: 0.0093 - val_accuracy: 0.9977\n",
      "Epoch 74/240\n",
      "67/67 [==============================] - 64s 951ms/step - loss: 0.0033 - accuracy: 0.9987 - val_loss: 0.0081 - val_accuracy: 0.9986\n",
      "Epoch 75/240\n",
      "67/67 [==============================] - 62s 933ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.0064 - val_accuracy: 0.9986\n",
      "Epoch 76/240\n",
      "67/67 [==============================] - 63s 933ms/step - loss: 0.0037 - accuracy: 0.9991 - val_loss: 0.0071 - val_accuracy: 0.9986\n",
      "Epoch 77/240\n",
      "67/67 [==============================] - 67s 995ms/step - loss: 0.0034 - accuracy: 0.9991 - val_loss: 0.0113 - val_accuracy: 0.9973\n",
      "Epoch 78/240\n",
      "67/67 [==============================] - 65s 969ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.0098 - val_accuracy: 0.9982\n",
      "Epoch 79/240\n",
      "67/67 [==============================] - 63s 935ms/step - loss: 0.0019 - accuracy: 0.9992 - val_loss: 0.0079 - val_accuracy: 0.9982\n",
      "Epoch 80/240\n",
      "67/67 [==============================] - 62s 927ms/step - loss: 0.0059 - accuracy: 0.9986 - val_loss: 0.0049 - val_accuracy: 0.9986\n",
      "Epoch 81/240\n",
      "67/67 [==============================] - 62s 925ms/step - loss: 0.0017 - accuracy: 0.9993 - val_loss: 0.0126 - val_accuracy: 0.9973\n",
      "Epoch 82/240\n",
      "67/67 [==============================] - 62s 929ms/step - loss: 0.0032 - accuracy: 0.9989 - val_loss: 0.0108 - val_accuracy: 0.9982\n",
      "Epoch 83/240\n",
      "67/67 [==============================] - 62s 921ms/step - loss: 0.0035 - accuracy: 0.9987 - val_loss: 0.0117 - val_accuracy: 0.9973\n",
      "Epoch 84/240\n",
      "67/67 [==============================] - 63s 938ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0114 - val_accuracy: 0.9973\n",
      "Epoch 85/240\n",
      "67/67 [==============================] - 64s 962ms/step - loss: 0.0025 - accuracy: 0.9987 - val_loss: 0.0099 - val_accuracy: 0.9982\n",
      "Epoch 86/240\n",
      "67/67 [==============================] - 64s 953ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0109 - val_accuracy: 0.9977\n",
      "Epoch 87/240\n",
      "67/67 [==============================] - 65s 963ms/step - loss: 0.0044 - accuracy: 0.9982 - val_loss: 0.0107 - val_accuracy: 0.9982\n",
      "Epoch 88/240\n",
      "67/67 [==============================] - 62s 928ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0126 - val_accuracy: 0.9982\n",
      "Epoch 89/240\n",
      "67/67 [==============================] - 62s 933ms/step - loss: 0.0037 - accuracy: 0.9986 - val_loss: 0.0118 - val_accuracy: 0.9973\n",
      "Epoch 90/240\n",
      "67/67 [==============================] - 64s 957ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.0062 - val_accuracy: 0.9977\n",
      "Epoch 91/240\n",
      "67/67 [==============================] - 65s 972ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 0.0083 - val_accuracy: 0.9982\n",
      "Epoch 92/240\n",
      "67/67 [==============================] - 69s 1s/step - loss: 0.0027 - accuracy: 0.9987 - val_loss: 0.0073 - val_accuracy: 0.9986\n",
      "Epoch 93/240\n",
      "67/67 [==============================] - 63s 942ms/step - loss: 0.0017 - accuracy: 0.9993 - val_loss: 0.0135 - val_accuracy: 0.9982\n",
      "Epoch 94/240\n",
      "67/67 [==============================] - 63s 940ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0097 - val_accuracy: 0.9973\n",
      "Epoch 95/240\n",
      "67/67 [==============================] - 63s 936ms/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.0125 - val_accuracy: 0.9977\n",
      "Epoch 96/240\n",
      "67/67 [==============================] - 63s 936ms/step - loss: 0.0020 - accuracy: 0.9992 - val_loss: 0.0119 - val_accuracy: 0.9977\n",
      "Epoch 97/240\n",
      "67/67 [==============================] - 66s 990ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 0.0107 - val_accuracy: 0.9982\n",
      "Epoch 98/240\n",
      "67/67 [==============================] - 62s 921ms/step - loss: 0.0038 - accuracy: 0.9981 - val_loss: 0.0142 - val_accuracy: 0.9968\n",
      "Epoch 99/240\n",
      "67/67 [==============================] - 63s 937ms/step - loss: 0.0048 - accuracy: 0.9984 - val_loss: 0.0102 - val_accuracy: 0.9982\n",
      "Epoch 100/240\n",
      "67/67 [==============================] - 62s 927ms/step - loss: 0.0027 - accuracy: 0.9989 - val_loss: 0.0130 - val_accuracy: 0.9973\n",
      "Epoch 101/240\n",
      "67/67 [==============================] - 63s 936ms/step - loss: 0.0021 - accuracy: 0.9993 - val_loss: 0.0082 - val_accuracy: 0.9986\n",
      "Epoch 102/240\n",
      "67/67 [==============================] - 62s 933ms/step - loss: 0.0042 - accuracy: 0.9984 - val_loss: 0.0138 - val_accuracy: 0.9977\n",
      "Epoch 103/240\n",
      "67/67 [==============================] - 63s 944ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.0149 - val_accuracy: 0.9973\n",
      "Epoch 104/240\n",
      "67/67 [==============================] - 63s 935ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.0155 - val_accuracy: 0.9982\n",
      "Epoch 105/240\n",
      "67/67 [==============================] - 64s 948ms/step - loss: 0.0016 - accuracy: 0.9994 - val_loss: 0.0126 - val_accuracy: 0.9977\n",
      "Epoch 106/240\n",
      "67/67 [==============================] - 64s 948ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0100 - val_accuracy: 0.9991\n",
      "Epoch 107/240\n",
      "67/67 [==============================] - 68s 1s/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.0067 - val_accuracy: 0.9986\n",
      "Epoch 108/240\n",
      "67/67 [==============================] - 65s 968ms/step - loss: 0.0029 - accuracy: 0.9988 - val_loss: 0.0113 - val_accuracy: 0.9982\n",
      "Epoch 109/240\n",
      "67/67 [==============================] - 63s 941ms/step - loss: 0.0037 - accuracy: 0.9991 - val_loss: 0.0112 - val_accuracy: 0.9977\n",
      "Epoch 110/240\n",
      "67/67 [==============================] - 62s 930ms/step - loss: 0.0051 - accuracy: 0.9987 - val_loss: 0.0106 - val_accuracy: 0.9982\n",
      "Epoch 111/240\n",
      "67/67 [==============================] - 63s 935ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.0079 - val_accuracy: 0.9982\n",
      "Epoch 112/240\n",
      "67/67 [==============================] - 62s 930ms/step - loss: 0.0040 - accuracy: 0.9985 - val_loss: 0.0105 - val_accuracy: 0.9973\n",
      "Epoch 113/240\n",
      "67/67 [==============================] - 61s 916ms/step - loss: 0.0024 - accuracy: 0.9991 - val_loss: 0.0127 - val_accuracy: 0.9982\n",
      "Epoch 114/240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 62s 929ms/step - loss: 0.0022 - accuracy: 0.9991 - val_loss: 0.0105 - val_accuracy: 0.9973\n",
      "Epoch 115/240\n",
      "67/67 [==============================] - 65s 963ms/step - loss: 0.0030 - accuracy: 0.9986 - val_loss: 0.0166 - val_accuracy: 0.9968\n",
      "Epoch 116/240\n",
      "67/67 [==============================] - 63s 943ms/step - loss: 0.0052 - accuracy: 0.9987 - val_loss: 0.0151 - val_accuracy: 0.9982\n",
      "Epoch 117/240\n",
      "67/67 [==============================] - 63s 941ms/step - loss: 0.0038 - accuracy: 0.9989 - val_loss: 0.0110 - val_accuracy: 0.9982\n",
      "Epoch 118/240\n",
      "67/67 [==============================] - 63s 937ms/step - loss: 0.0023 - accuracy: 0.9992 - val_loss: 0.0131 - val_accuracy: 0.9977\n",
      "Epoch 119/240\n",
      "67/67 [==============================] - 63s 941ms/step - loss: 0.0033 - accuracy: 0.9988 - val_loss: 0.0124 - val_accuracy: 0.9982\n",
      "Epoch 120/240\n",
      "67/67 [==============================] - 67s 995ms/step - loss: 0.0019 - accuracy: 0.9993 - val_loss: 0.0157 - val_accuracy: 0.9977\n",
      "Epoch 121/240\n",
      "67/67 [==============================] - 63s 941ms/step - loss: 0.0036 - accuracy: 0.9987 - val_loss: 0.0115 - val_accuracy: 0.9982\n",
      "Epoch 122/240\n",
      "67/67 [==============================] - 70s 1s/step - loss: 0.0033 - accuracy: 0.9987 - val_loss: 0.0108 - val_accuracy: 0.9977\n",
      "Epoch 123/240\n",
      "67/67 [==============================] - 63s 943ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0117 - val_accuracy: 0.9982\n",
      "Epoch 124/240\n",
      "67/67 [==============================] - 63s 947ms/step - loss: 0.0049 - accuracy: 0.9982 - val_loss: 0.0128 - val_accuracy: 0.9977\n",
      "Epoch 125/240\n",
      "67/67 [==============================] - 67s 994ms/step - loss: 0.0032 - accuracy: 0.9987 - val_loss: 0.0059 - val_accuracy: 0.9977\n",
      "Epoch 126/240\n",
      "67/67 [==============================] - 70s 1s/step - loss: 0.0022 - accuracy: 0.9991 - val_loss: 0.0124 - val_accuracy: 0.9982\n",
      "Epoch 127/240\n",
      "67/67 [==============================] - 84s 1s/step - loss: 0.0059 - accuracy: 0.9988 - val_loss: 0.0137 - val_accuracy: 0.9977\n",
      "Epoch 128/240\n",
      "67/67 [==============================] - 91s 1s/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.0160 - val_accuracy: 0.9977\n",
      "Epoch 129/240\n",
      "67/67 [==============================] - 90s 1s/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.0085 - val_accuracy: 0.9986\n",
      "Epoch 130/240\n",
      "67/67 [==============================] - 90s 1s/step - loss: 0.0034 - accuracy: 0.9987 - val_loss: 0.0106 - val_accuracy: 0.9977\n",
      "Epoch 131/240\n",
      "67/67 [==============================] - 81s 1s/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.0107 - val_accuracy: 0.9973\n",
      "Epoch 132/240\n",
      "67/67 [==============================] - 77s 1s/step - loss: 0.0041 - accuracy: 0.9989 - val_loss: 0.0130 - val_accuracy: 0.9982\n",
      "Epoch 133/240\n",
      "67/67 [==============================] - 81s 1s/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.0113 - val_accuracy: 0.9977\n",
      "Epoch 134/240\n",
      "67/67 [==============================] - 82s 1s/step - loss: 0.0032 - accuracy: 0.9987 - val_loss: 0.0119 - val_accuracy: 0.9977\n",
      "Epoch 135/240\n",
      "67/67 [==============================] - 75s 1s/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.0132 - val_accuracy: 0.9977\n",
      "Epoch 136/240\n",
      "67/67 [==============================] - 73s 1s/step - loss: 0.0024 - accuracy: 0.9991 - val_loss: 0.0110 - val_accuracy: 0.9977\n",
      "Epoch 137/240\n",
      "67/67 [==============================] - 68s 1s/step - loss: 0.0049 - accuracy: 0.9984 - val_loss: 0.0118 - val_accuracy: 0.9977\n",
      "Epoch 138/240\n",
      "67/67 [==============================] - 63s 940ms/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.0106 - val_accuracy: 0.9982\n",
      "Epoch 139/240\n",
      "67/67 [==============================] - 62s 925ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.0139 - val_accuracy: 0.9982\n",
      "Epoch 140/240\n",
      "67/67 [==============================] - 65s 964ms/step - loss: 0.0017 - accuracy: 0.9992 - val_loss: 0.0178 - val_accuracy: 0.9973\n",
      "Epoch 141/240\n",
      "67/67 [==============================] - 66s 979ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0121 - val_accuracy: 0.9977\n",
      "Epoch 142/240\n",
      "67/67 [==============================] - 66s 981ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.0128 - val_accuracy: 0.9977\n",
      "Epoch 143/240\n",
      "67/67 [==============================] - 63s 936ms/step - loss: 0.0017 - accuracy: 0.9991 - val_loss: 0.0123 - val_accuracy: 0.9982\n",
      "Epoch 144/240\n",
      "67/67 [==============================] - 61s 912ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.0156 - val_accuracy: 0.9977\n",
      "Epoch 145/240\n",
      "67/67 [==============================] - 61s 913ms/step - loss: 0.0021 - accuracy: 0.9991 - val_loss: 0.0137 - val_accuracy: 0.9982\n",
      "Epoch 146/240\n",
      "67/67 [==============================] - 66s 988ms/step - loss: 0.0041 - accuracy: 0.9985 - val_loss: 0.0110 - val_accuracy: 0.9982\n",
      "Epoch 147/240\n",
      "67/67 [==============================] - 63s 941ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.0095 - val_accuracy: 0.9982\n",
      "Epoch 148/240\n",
      "67/67 [==============================] - 65s 971ms/step - loss: 0.0024 - accuracy: 0.9989 - val_loss: 0.0095 - val_accuracy: 0.9982\n",
      "Epoch 149/240\n",
      "67/67 [==============================] - 67s 995ms/step - loss: 0.0019 - accuracy: 0.9992 - val_loss: 0.0181 - val_accuracy: 0.9977\n",
      "Epoch 150/240\n",
      "67/67 [==============================] - 63s 938ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0118 - val_accuracy: 0.9982\n",
      "Epoch 151/240\n",
      "67/67 [==============================] - 68s 1s/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.0120 - val_accuracy: 0.9977\n",
      "Epoch 152/240\n",
      "67/67 [==============================] - 62s 930ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0137 - val_accuracy: 0.9982\n",
      "Epoch 153/240\n",
      "67/67 [==============================] - 62s 931ms/step - loss: 0.0021 - accuracy: 0.9992 - val_loss: 0.0136 - val_accuracy: 0.9977\n",
      "Epoch 154/240\n",
      "67/67 [==============================] - 62s 929ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.0139 - val_accuracy: 0.9977\n",
      "Epoch 155/240\n",
      "67/67 [==============================] - 62s 930ms/step - loss: 0.0018 - accuracy: 0.9993 - val_loss: 0.0095 - val_accuracy: 0.9982\n",
      "Epoch 156/240\n",
      "67/67 [==============================] - 62s 933ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.0116 - val_accuracy: 0.9982\n",
      "Epoch 157/240\n",
      "67/67 [==============================] - 62s 929ms/step - loss: 0.0011 - accuracy: 0.9995 - val_loss: 0.0086 - val_accuracy: 0.9977\n",
      "Epoch 158/240\n",
      "67/67 [==============================] - 76s 1s/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.0185 - val_accuracy: 0.9968\n",
      "Epoch 159/240\n",
      "67/67 [==============================] - 95s 1s/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0123 - val_accuracy: 0.9977\n",
      "Epoch 160/240\n",
      "67/67 [==============================] - 77s 1s/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 0.0118 - val_accuracy: 0.9977\n",
      "Epoch 161/240\n",
      "67/67 [==============================] - 73s 1s/step - loss: 0.0020 - accuracy: 0.9991 - val_loss: 0.0088 - val_accuracy: 0.9986\n",
      "Epoch 162/240\n",
      "67/67 [==============================] - 79s 1s/step - loss: 0.0021 - accuracy: 0.9992 - val_loss: 0.0090 - val_accuracy: 0.9982\n",
      "Epoch 163/240\n",
      "67/67 [==============================] - 83s 1s/step - loss: 0.0031 - accuracy: 0.9987 - val_loss: 0.0094 - val_accuracy: 0.9977\n",
      "Epoch 164/240\n",
      "67/67 [==============================] - 80s 1s/step - loss: 0.0053 - accuracy: 0.9989 - val_loss: 0.0121 - val_accuracy: 0.9977\n",
      "Epoch 165/240\n",
      "67/67 [==============================] - 82s 1s/step - loss: 0.0020 - accuracy: 0.9991 - val_loss: 0.0114 - val_accuracy: 0.9977\n",
      "Epoch 166/240\n",
      "67/67 [==============================] - 89s 1s/step - loss: 0.0013 - accuracy: 0.9993 - val_loss: 0.0126 - val_accuracy: 0.9977\n",
      "Epoch 167/240\n",
      "67/67 [==============================] - 83s 1s/step - loss: 0.0043 - accuracy: 0.9987 - val_loss: 0.0085 - val_accuracy: 0.9977\n",
      "Epoch 168/240\n",
      "67/67 [==============================] - 82s 1s/step - loss: 0.0012 - accuracy: 0.9995 - val_loss: 0.0152 - val_accuracy: 0.9977\n",
      "Epoch 169/240\n",
      "67/67 [==============================] - 87s 1s/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0153 - val_accuracy: 0.9977\n",
      "Epoch 170/240\n",
      "67/67 [==============================] - 83s 1s/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.0128 - val_accuracy: 0.9973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/240\n",
      "67/67 [==============================] - 86s 1s/step - loss: 0.0029 - accuracy: 0.9988 - val_loss: 0.0146 - val_accuracy: 0.9977\n",
      "Epoch 172/240\n",
      "67/67 [==============================] - 88s 1s/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.0149 - val_accuracy: 0.9982\n",
      "Epoch 173/240\n",
      "67/67 [==============================] - 92s 1s/step - loss: 0.0034 - accuracy: 0.9991 - val_loss: 0.0163 - val_accuracy: 0.9982\n",
      "Epoch 174/240\n",
      "67/67 [==============================] - 87s 1s/step - loss: 0.0012 - accuracy: 0.9995 - val_loss: 0.0166 - val_accuracy: 0.9977\n",
      "Epoch 175/240\n",
      "67/67 [==============================] - 85s 1s/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.0101 - val_accuracy: 0.9986\n",
      "Epoch 176/240\n",
      "67/67 [==============================] - 81s 1s/step - loss: 0.0063 - accuracy: 0.9986 - val_loss: 0.0147 - val_accuracy: 0.9982\n",
      "Epoch 177/240\n",
      "67/67 [==============================] - 76s 1s/step - loss: 0.0017 - accuracy: 0.9992 - val_loss: 0.0189 - val_accuracy: 0.9982\n",
      "Epoch 178/240\n",
      "67/67 [==============================] - 74s 1s/step - loss: 0.0016 - accuracy: 0.9994 - val_loss: 0.0115 - val_accuracy: 0.9982\n",
      "Epoch 179/240\n",
      "67/67 [==============================] - 76s 1s/step - loss: 0.0011 - accuracy: 0.9994 - val_loss: 0.0212 - val_accuracy: 0.9973\n",
      "Epoch 180/240\n",
      "67/67 [==============================] - 79s 1s/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 0.0158 - val_accuracy: 0.9977\n",
      "Epoch 181/240\n",
      "67/67 [==============================] - 77s 1s/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0114 - val_accuracy: 0.9977\n",
      "Epoch 182/240\n",
      "67/67 [==============================] - 77s 1s/step - loss: 0.0029 - accuracy: 0.9989 - val_loss: 0.0166 - val_accuracy: 0.9977\n",
      "Epoch 183/240\n",
      "67/67 [==============================] - 79s 1s/step - loss: 9.2693e-04 - accuracy: 0.9995 - val_loss: 0.0141 - val_accuracy: 0.9977\n",
      "Epoch 184/240\n",
      "67/67 [==============================] - 82s 1s/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.0180 - val_accuracy: 0.9977\n",
      "Epoch 185/240\n",
      "67/67 [==============================] - 94s 1s/step - loss: 0.0029 - accuracy: 0.9989 - val_loss: 0.0161 - val_accuracy: 0.9977\n",
      "Epoch 186/240\n",
      "67/67 [==============================] - 88s 1s/step - loss: 0.0016 - accuracy: 0.9993 - val_loss: 0.0153 - val_accuracy: 0.9977\n",
      "Epoch 187/240\n",
      "67/67 [==============================] - 81s 1s/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.0194 - val_accuracy: 0.9977\n",
      "Epoch 188/240\n",
      "67/67 [==============================] - 76s 1s/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 0.0143 - val_accuracy: 0.9982\n",
      "Epoch 189/240\n",
      "67/67 [==============================] - 81s 1s/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.0151 - val_accuracy: 0.9982\n",
      "Epoch 190/240\n",
      "67/67 [==============================] - 79s 1s/step - loss: 0.0039 - accuracy: 0.9989 - val_loss: 0.0223 - val_accuracy: 0.9977\n",
      "Epoch 191/240\n",
      "67/67 [==============================] - 84s 1s/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.0195 - val_accuracy: 0.9977\n",
      "Epoch 192/240\n",
      "67/67 [==============================] - 77s 1s/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.0151 - val_accuracy: 0.9977\n",
      "Epoch 193/240\n",
      "67/67 [==============================] - 75s 1s/step - loss: 0.0031 - accuracy: 0.9989 - val_loss: 0.0136 - val_accuracy: 0.9977\n",
      "Epoch 194/240\n",
      "67/67 [==============================] - 75s 1s/step - loss: 0.0022 - accuracy: 0.9992 - val_loss: 0.0205 - val_accuracy: 0.9977\n",
      "Epoch 195/240\n",
      "67/67 [==============================] - 84s 1s/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.0149 - val_accuracy: 0.9977\n",
      "Epoch 196/240\n",
      "67/67 [==============================] - 78s 1s/step - loss: 0.0013 - accuracy: 0.9994 - val_loss: 0.0163 - val_accuracy: 0.9973\n",
      "Epoch 197/240\n",
      "67/67 [==============================] - 84s 1s/step - loss: 0.0019 - accuracy: 0.9992 - val_loss: 0.0149 - val_accuracy: 0.9977\n",
      "Epoch 198/240\n",
      "67/67 [==============================] - 74s 1s/step - loss: 9.0272e-04 - accuracy: 0.9996 - val_loss: 0.0131 - val_accuracy: 0.9977\n",
      "Epoch 199/240\n",
      "67/67 [==============================] - 70s 1s/step - loss: 0.0044 - accuracy: 0.9984 - val_loss: 0.0127 - val_accuracy: 0.9982\n",
      "Epoch 200/240\n",
      "67/67 [==============================] - 71s 1s/step - loss: 0.0012 - accuracy: 0.9995 - val_loss: 0.0141 - val_accuracy: 0.9977\n",
      "Epoch 201/240\n",
      "67/67 [==============================] - 70s 1s/step - loss: 0.0023 - accuracy: 0.9991 - val_loss: 0.0152 - val_accuracy: 0.9977\n",
      "Epoch 202/240\n",
      "67/67 [==============================] - 69s 1s/step - loss: 0.0033 - accuracy: 0.9986 - val_loss: 0.0136 - val_accuracy: 0.9982\n",
      "Epoch 203/240\n",
      "67/67 [==============================] - 73s 1s/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.0113 - val_accuracy: 0.9982\n",
      "Epoch 204/240\n",
      "67/67 [==============================] - 69s 1s/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 0.0143 - val_accuracy: 0.9986\n",
      "Epoch 205/240\n",
      "67/67 [==============================] - 72s 1s/step - loss: 0.0034 - accuracy: 0.9989 - val_loss: 0.0143 - val_accuracy: 0.9982\n",
      "Epoch 206/240\n",
      "67/67 [==============================] - 70s 1s/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 0.0135 - val_accuracy: 0.9986\n",
      "Epoch 207/240\n",
      "67/67 [==============================] - 70s 1s/step - loss: 0.0018 - accuracy: 0.9991 - val_loss: 0.0089 - val_accuracy: 0.9986\n",
      "Epoch 208/240\n",
      "67/67 [==============================] - 70s 1s/step - loss: 0.0030 - accuracy: 0.9989 - val_loss: 0.0103 - val_accuracy: 0.9982\n",
      "Epoch 209/240\n",
      "67/67 [==============================] - 76s 1s/step - loss: 0.0011 - accuracy: 0.9995 - val_loss: 0.0187 - val_accuracy: 0.9968\n",
      "Epoch 210/240\n",
      "67/67 [==============================] - 79s 1s/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0158 - val_accuracy: 0.9982\n",
      "Epoch 211/240\n",
      "67/67 [==============================] - 79s 1s/step - loss: 0.0018 - accuracy: 0.9991 - val_loss: 0.0129 - val_accuracy: 0.9977\n",
      "Epoch 212/240\n",
      "67/67 [==============================] - 76s 1s/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.0174 - val_accuracy: 0.9977\n",
      "Epoch 213/240\n",
      "67/67 [==============================] - 76s 1s/step - loss: 0.0012 - accuracy: 0.9995 - val_loss: 0.0125 - val_accuracy: 0.9982\n",
      "Epoch 214/240\n",
      "67/67 [==============================] - 76s 1s/step - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.0135 - val_accuracy: 0.9986\n",
      "Epoch 215/240\n",
      "67/67 [==============================] - 75s 1s/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.0175 - val_accuracy: 0.9982\n",
      "Epoch 216/240\n",
      "67/67 [==============================] - 82s 1s/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.0189 - val_accuracy: 0.9982\n",
      "Epoch 217/240\n",
      "67/67 [==============================] - 65s 976ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0144 - val_accuracy: 0.9982\n",
      "Epoch 218/240\n",
      "67/67 [==============================] - 64s 956ms/step - loss: 0.0015 - accuracy: 0.9992 - val_loss: 0.0161 - val_accuracy: 0.9982\n",
      "Epoch 219/240\n",
      "67/67 [==============================] - 68s 1s/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0136 - val_accuracy: 0.9982\n",
      "Epoch 220/240\n",
      "67/67 [==============================] - 63s 936ms/step - loss: 0.0047 - accuracy: 0.9989 - val_loss: 0.0144 - val_accuracy: 0.9982\n",
      "Epoch 221/240\n",
      "67/67 [==============================] - 64s 959ms/step - loss: 0.0018 - accuracy: 0.9993 - val_loss: 0.0182 - val_accuracy: 0.9977\n",
      "Epoch 222/240\n",
      "67/67 [==============================] - 63s 942ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0089 - val_accuracy: 0.9982\n",
      "Epoch 223/240\n",
      "67/67 [==============================] - 67s 994ms/step - loss: 0.0047 - accuracy: 0.9989 - val_loss: 0.0074 - val_accuracy: 0.9986\n",
      "Epoch 224/240\n",
      "67/67 [==============================] - 64s 959ms/step - loss: 0.0022 - accuracy: 0.9992 - val_loss: 0.0085 - val_accuracy: 0.9982\n",
      "Epoch 225/240\n",
      "67/67 [==============================] - 63s 944ms/step - loss: 0.0019 - accuracy: 0.9991 - val_loss: 0.0076 - val_accuracy: 0.9982\n",
      "Epoch 226/240\n",
      "67/67 [==============================] - 63s 934ms/step - loss: 0.0029 - accuracy: 0.9989 - val_loss: 0.0113 - val_accuracy: 0.9982\n",
      "Epoch 227/240\n",
      "67/67 [==============================] - 63s 946ms/step - loss: 0.0031 - accuracy: 0.9987 - val_loss: 0.0157 - val_accuracy: 0.9977\n",
      "Epoch 228/240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 66s 980ms/step - loss: 0.0027 - accuracy: 0.9989 - val_loss: 0.0155 - val_accuracy: 0.9986\n",
      "Epoch 229/240\n",
      "67/67 [==============================] - 62s 929ms/step - loss: 0.0048 - accuracy: 0.9992 - val_loss: 0.0151 - val_accuracy: 0.9982\n",
      "Epoch 230/240\n",
      "67/67 [==============================] - 63s 947ms/step - loss: 0.0018 - accuracy: 0.9993 - val_loss: 0.0156 - val_accuracy: 0.9982\n",
      "Epoch 231/240\n",
      "67/67 [==============================] - 63s 935ms/step - loss: 0.0022 - accuracy: 0.9993 - val_loss: 0.0159 - val_accuracy: 0.9982\n",
      "Epoch 232/240\n",
      "67/67 [==============================] - 63s 940ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0207 - val_accuracy: 0.9968\n",
      "Epoch 233/240\n",
      "67/67 [==============================] - 62s 926ms/step - loss: 0.0039 - accuracy: 0.9992 - val_loss: 0.0207 - val_accuracy: 0.9977\n",
      "Epoch 234/240\n",
      "67/67 [==============================] - 62s 931ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0176 - val_accuracy: 0.9977\n",
      "Epoch 235/240\n",
      "67/67 [==============================] - 63s 943ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.0284 - val_accuracy: 0.9968\n",
      "Epoch 236/240\n",
      "67/67 [==============================] - 65s 973ms/step - loss: 0.0012 - accuracy: 0.9993 - val_loss: 0.0168 - val_accuracy: 0.9977\n",
      "Epoch 237/240\n",
      "67/67 [==============================] - 65s 975ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 0.0149 - val_accuracy: 0.9977\n",
      "Epoch 238/240\n",
      "67/67 [==============================] - 68s 1s/step - loss: 8.1925e-04 - accuracy: 0.9998 - val_loss: 0.0137 - val_accuracy: 0.9982\n",
      "Epoch 239/240\n",
      "67/67 [==============================] - 64s 956ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.0165 - val_accuracy: 0.9982\n",
      "Epoch 240/240\n",
      "67/67 [==============================] - 63s 939ms/step - loss: 0.0034 - accuracy: 0.9991 - val_loss: 0.0190 - val_accuracy: 0.9982\n"
     ]
    }
   ],
   "source": [
    "history2 = model2.fit(training_X, training_y, epochs = 240, batch_size= 128, validation_data=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "resultados(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def resultados(history):\n",
    "    pd.DataFrame(history.history).plot(figsize = (8,5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0,1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "resultados(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Pedro tiene 5 monedas y su abuela le da 3 más, ¿Cuántas monedas tiene en total?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Si Juan tiene 3 cajas y pierde 1, Cuantas le quedan?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = [word for word in question.split(' ') if word not in nomb]\n",
    "question = ' '.join(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En el conjunto de predicción nos podemos encontrar palabras que no estén\n",
    "bag_q = np.zeros([1, tamanoTotal, len(palabras_unicas)+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulario['5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_question = question.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(palabras_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tamanoTotal-len(palabras_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, palabras in enumerate(palabras_question):\n",
    "    print('palabra', palabras)\n",
    "    print(tamanoTotal - len(palabras_question) +j)\n",
    "    if palabras in vocabulario: \n",
    "        bag_q[0, tamanoTotal - len(palabras_question) + j, vocabulario[palabras]] = 1.0\n",
    "    else:\n",
    "        bag_q[0, tamanoTotal - len(palabras_question) + j, vocabulario['<OOP>']] = 1.0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.predict(bag_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(loss='sparse_categorical_crossentropy',optimizer='SGD',metrics=['accuracy'])\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='RMSProp',metrics=['accuracy'])\n",
    "#model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(training_X, training_y, epochs = 140, validation_data=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='SGD',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(training_X, training_y, epochs = 140, validation_data=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.SGD(lr=0.00001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#opt = keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "model2.compile(loss='sparse_categorical_crossentropy',optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model2.fit(training_X, training_y, epochs = 140, batch_size= 128, validation_data=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt = keras.optimizers.SGD(lr=0.00001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "opt = keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "model2.compile(loss='sparse_categorical_crossentropy',optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model2.fit(training_X, training_y, epochs = 140, batch_size= 128, validation_data=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados(history2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
