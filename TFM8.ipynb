{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import keras\n",
    "import six\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import TimeDistributed,Conv1D,Dense,Embedding,Input,Dropout,LSTM,Bidirectional,AveragePooling1D,Flatten,concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.optimizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/roberto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/roberto/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/roberto/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/roberto/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abro el archivo en el que se encuentra el dataset de los problemas\n",
    "with open('singleop.json', 'r') as f:\n",
    "    datastore = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archivo que contiene un listado de nombres\n",
    "nombres = pd.read_csv('nombres-2015.csv')\n",
    "names = pd.read_csv('yob2019.txt', header=None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = pd.read_csv('problemas_adicionales2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preguntas = []\n",
    "respuestas = []\n",
    "ecuaciones = []\n",
    "alineacion = []\n",
    "\n",
    "for item in datastore:\n",
    "    preguntas.append(item['sQuestion'])\n",
    "    respuestas.append(item['lSolutions'])\n",
    "    ecuaciones.append(item['lEquations'])\n",
    "    alineacion.append(item['lAlignments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tengo  159  sumas  162  restas,  117  multiplicaciones,  124  divisiones y otras operaciones  0\n"
     ]
    }
   ],
   "source": [
    "# Necesito convertir el dataset en un problema de clasificacion para que la red neuronal pueda identificar\n",
    "# si estoy tratando de resolver un problema de sumas, restas, multiplicaciones o divisiones.\n",
    "# Esto va a crear una lista con el tipo de operacion y que va a ser el resultado a inferir.\n",
    "operaciones = []\n",
    "sumas =0\n",
    "restas =0\n",
    "multiplicaciones =0\n",
    "divisiones = 0\n",
    "otras = 0\n",
    "#Clasifico las operaciones en 0 para sumas, 1 para restas, 2 para multiplicaciones, 3 para divisiones y 4 sino lo encuentro.\n",
    "for operacion in ecuaciones:\n",
    "    if (operacion[0].find('+')>=0):\n",
    "        operaciones.append(0)\n",
    "        sumas = sumas + 1\n",
    "    elif (operacion[0].find('-') >= 0 ):\n",
    "        operaciones.append(1)\n",
    "        restas = restas + 1\n",
    "    elif(operacion[0].find('*') >=0):\n",
    "        operaciones.append(2)\n",
    "        multiplicaciones = multiplicaciones + 1\n",
    "    elif(operacion[0].find('/')):\n",
    "        operaciones.append(3)\n",
    "        divisiones = divisiones + 1\n",
    "    else:\n",
    "        operaciones.append(4)\n",
    "        otras = otras + 1\n",
    "\n",
    "print('Tengo ', sumas, ' sumas ', restas, ' restas, ', multiplicaciones, ' multiplicaciones, ', divisiones, ' divisiones y otras operaciones ', otras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preguntas2 = dataset2['Preguntas'].tolist()\n",
    "respuestas2 = dataset2['respuestas'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preguntas3 = preguntas + preguntas2\n",
    "respuestas3 = operaciones + respuestas2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#El listado de nombres lo voy a truncar a los 15K primeros, dado que el resto son nombres muy residuales.\n",
    "nombres_ = nombres['nombre'][:15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_= names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nombres_ = nombres_.append(names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_words = list(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nom = nombres['nombre'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomb =  nom + st_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'durante'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nomb[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El vector preguntas_sin, consiste en las preguntas a las que voy a eliminar todos los nombres propios que no\n",
    "# anaden ningun valor al conjunto de preguntas. No quiero que esos nombres se procesen y por tanto los elimino.\n",
    "def eliminar_palabras(dataset, stopw):\n",
    "    preguntas_sin = []\n",
    "    for palabras in dataset:\n",
    "        frases = [word for word in palabras.split(' ') if word not in stopw]\n",
    "        frases = \" \".join(frases)\n",
    "        preguntas_sin.append(frases)\n",
    "    return preguntas_sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preguntas_sin = eliminar_palabras(preguntas3, nomb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def mezclarPalabras(frase):\n",
    "    separar = frase.split()\n",
    "    shuffle(separar)\n",
    "    return ' '.join(separar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "974"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(respuestas3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "12\n",
      "2\n",
      "23\n",
      "3\n",
      "34\n",
      "4\n",
      "45\n",
      "5\n",
      "56\n",
      "6\n",
      "67\n",
      "7\n",
      "78\n",
      "8\n",
      "89\n",
      "9\n",
      "100\n",
      "10\n",
      "111\n",
      "11\n",
      "122\n",
      "12\n",
      "133\n",
      "13\n",
      "144\n",
      "14\n",
      "155\n",
      "15\n",
      "166\n",
      "16\n",
      "177\n",
      "17\n",
      "188\n",
      "18\n",
      "199\n",
      "19\n",
      "210\n",
      "20\n",
      "221\n",
      "21\n",
      "232\n",
      "22\n",
      "243\n",
      "23\n",
      "254\n",
      "24\n",
      "265\n",
      "25\n",
      "276\n",
      "26\n",
      "287\n",
      "27\n",
      "298\n",
      "28\n",
      "309\n",
      "29\n",
      "320\n",
      "30\n",
      "331\n",
      "31\n",
      "342\n",
      "32\n",
      "353\n",
      "33\n",
      "364\n",
      "34\n",
      "375\n",
      "35\n",
      "386\n",
      "36\n",
      "397\n",
      "37\n",
      "408\n",
      "38\n",
      "419\n",
      "39\n",
      "430\n",
      "40\n",
      "441\n",
      "41\n",
      "452\n",
      "42\n",
      "463\n",
      "43\n",
      "474\n",
      "44\n",
      "485\n",
      "45\n",
      "496\n",
      "46\n",
      "507\n",
      "47\n",
      "518\n",
      "48\n",
      "529\n",
      "49\n",
      "540\n",
      "50\n",
      "551\n",
      "51\n",
      "562\n",
      "52\n",
      "573\n",
      "53\n",
      "584\n",
      "54\n",
      "595\n",
      "55\n",
      "606\n",
      "56\n",
      "617\n",
      "57\n",
      "628\n",
      "58\n",
      "639\n",
      "59\n",
      "650\n",
      "60\n",
      "661\n",
      "61\n",
      "672\n",
      "62\n",
      "683\n",
      "63\n",
      "694\n",
      "64\n",
      "705\n",
      "65\n",
      "716\n",
      "66\n",
      "727\n",
      "67\n",
      "738\n",
      "68\n",
      "749\n",
      "69\n",
      "760\n",
      "70\n",
      "771\n",
      "71\n",
      "782\n",
      "72\n",
      "793\n",
      "73\n",
      "804\n",
      "74\n",
      "815\n",
      "75\n",
      "826\n",
      "76\n",
      "837\n",
      "77\n",
      "848\n",
      "78\n",
      "859\n",
      "79\n",
      "870\n",
      "80\n",
      "881\n",
      "81\n",
      "892\n",
      "82\n",
      "903\n",
      "83\n",
      "914\n",
      "84\n",
      "925\n",
      "85\n",
      "936\n",
      "86\n",
      "947\n",
      "87\n",
      "958\n",
      "88\n",
      "969\n",
      "89\n",
      "980\n",
      "90\n",
      "991\n",
      "91\n",
      "1002\n",
      "92\n",
      "1013\n",
      "93\n",
      "1024\n",
      "94\n",
      "1035\n",
      "95\n",
      "1046\n",
      "96\n",
      "1057\n",
      "97\n",
      "1068\n",
      "98\n",
      "1079\n",
      "99\n",
      "1090\n",
      "100\n",
      "1101\n",
      "101\n",
      "1112\n",
      "102\n",
      "1123\n",
      "103\n",
      "1134\n",
      "104\n",
      "1145\n",
      "105\n",
      "1156\n",
      "106\n",
      "1167\n",
      "107\n",
      "1178\n",
      "108\n",
      "1189\n",
      "109\n",
      "1200\n",
      "110\n",
      "1211\n",
      "111\n",
      "1222\n",
      "112\n",
      "1233\n",
      "113\n",
      "1244\n",
      "114\n",
      "1255\n",
      "115\n",
      "1266\n",
      "116\n",
      "1277\n",
      "117\n",
      "1288\n",
      "118\n",
      "1299\n",
      "119\n",
      "1310\n",
      "120\n",
      "1321\n",
      "121\n",
      "1332\n",
      "122\n",
      "1343\n",
      "123\n",
      "1354\n",
      "124\n",
      "1365\n",
      "125\n",
      "1376\n",
      "126\n",
      "1387\n",
      "127\n",
      "1398\n",
      "128\n",
      "1409\n",
      "129\n",
      "1420\n",
      "130\n",
      "1431\n",
      "131\n",
      "1442\n",
      "132\n",
      "1453\n",
      "133\n",
      "1464\n",
      "134\n",
      "1475\n",
      "135\n",
      "1486\n",
      "136\n",
      "1497\n",
      "137\n",
      "1508\n",
      "138\n",
      "1519\n",
      "139\n",
      "1530\n",
      "140\n",
      "1541\n",
      "141\n",
      "1552\n",
      "142\n",
      "1563\n",
      "143\n",
      "1574\n",
      "144\n",
      "1585\n",
      "145\n",
      "1596\n",
      "146\n",
      "1607\n",
      "147\n",
      "1618\n",
      "148\n",
      "1629\n",
      "149\n",
      "1640\n",
      "150\n",
      "1651\n",
      "151\n",
      "1662\n",
      "152\n",
      "1673\n",
      "153\n",
      "1684\n",
      "154\n",
      "1695\n",
      "155\n",
      "1706\n",
      "156\n",
      "1717\n",
      "157\n",
      "1728\n",
      "158\n",
      "1739\n",
      "159\n",
      "1750\n",
      "160\n",
      "1761\n",
      "161\n",
      "1772\n",
      "162\n",
      "1783\n",
      "163\n",
      "1794\n",
      "164\n",
      "1805\n",
      "165\n",
      "1816\n",
      "166\n",
      "1827\n",
      "167\n",
      "1838\n",
      "168\n",
      "1849\n",
      "169\n",
      "1860\n",
      "170\n",
      "1871\n",
      "171\n",
      "1882\n",
      "172\n",
      "1893\n",
      "173\n",
      "1904\n",
      "174\n",
      "1915\n",
      "175\n",
      "1926\n",
      "176\n",
      "1937\n",
      "177\n",
      "1948\n",
      "178\n",
      "1959\n",
      "179\n",
      "1970\n",
      "180\n",
      "1981\n",
      "181\n",
      "1992\n",
      "182\n",
      "2003\n",
      "183\n",
      "2014\n",
      "184\n",
      "2025\n",
      "185\n",
      "2036\n",
      "186\n",
      "2047\n",
      "187\n",
      "2058\n",
      "188\n",
      "2069\n",
      "189\n",
      "2080\n",
      "190\n",
      "2091\n",
      "191\n",
      "2102\n",
      "192\n",
      "2113\n",
      "193\n",
      "2124\n",
      "194\n",
      "2135\n",
      "195\n",
      "2146\n",
      "196\n",
      "2157\n",
      "197\n",
      "2168\n",
      "198\n",
      "2179\n",
      "199\n",
      "2190\n",
      "200\n",
      "2201\n",
      "201\n",
      "2212\n",
      "202\n",
      "2223\n",
      "203\n",
      "2234\n",
      "204\n",
      "2245\n",
      "205\n",
      "2256\n",
      "206\n",
      "2267\n",
      "207\n",
      "2278\n",
      "208\n",
      "2289\n",
      "209\n",
      "2300\n",
      "210\n",
      "2311\n",
      "211\n",
      "2322\n",
      "212\n",
      "2333\n",
      "213\n",
      "2344\n",
      "214\n",
      "2355\n",
      "215\n",
      "2366\n",
      "216\n",
      "2377\n",
      "217\n",
      "2388\n",
      "218\n",
      "2399\n",
      "219\n",
      "2410\n",
      "220\n",
      "2421\n",
      "221\n",
      "2432\n",
      "222\n",
      "2443\n",
      "223\n",
      "2454\n",
      "224\n",
      "2465\n",
      "225\n",
      "2476\n",
      "226\n",
      "2487\n",
      "227\n",
      "2498\n",
      "228\n",
      "2509\n",
      "229\n",
      "2520\n",
      "230\n",
      "2531\n",
      "231\n",
      "2542\n",
      "232\n",
      "2553\n",
      "233\n",
      "2564\n",
      "234\n",
      "2575\n",
      "235\n",
      "2586\n",
      "236\n",
      "2597\n",
      "237\n",
      "2608\n",
      "238\n",
      "2619\n",
      "239\n",
      "2630\n",
      "240\n",
      "2641\n",
      "241\n",
      "2652\n",
      "242\n",
      "2663\n",
      "243\n",
      "2674\n",
      "244\n",
      "2685\n",
      "245\n",
      "2696\n",
      "246\n",
      "2707\n",
      "247\n",
      "2718\n",
      "248\n",
      "2729\n",
      "249\n",
      "2740\n",
      "250\n",
      "2751\n",
      "251\n",
      "2762\n",
      "252\n",
      "2773\n",
      "253\n",
      "2784\n",
      "254\n",
      "2795\n",
      "255\n",
      "2806\n",
      "256\n",
      "2817\n",
      "257\n",
      "2828\n",
      "258\n",
      "2839\n",
      "259\n",
      "2850\n",
      "260\n",
      "2861\n",
      "261\n",
      "2872\n",
      "262\n",
      "2883\n",
      "263\n",
      "2894\n",
      "264\n",
      "2905\n",
      "265\n",
      "2916\n",
      "266\n",
      "2927\n",
      "267\n",
      "2938\n",
      "268\n",
      "2949\n",
      "269\n",
      "2960\n",
      "270\n",
      "2971\n",
      "271\n",
      "2982\n",
      "272\n",
      "2993\n",
      "273\n",
      "3004\n",
      "274\n",
      "3015\n",
      "275\n",
      "3026\n",
      "276\n",
      "3037\n",
      "277\n",
      "3048\n",
      "278\n",
      "3059\n",
      "279\n",
      "3070\n",
      "280\n",
      "3081\n",
      "281\n",
      "3092\n",
      "282\n",
      "3103\n",
      "283\n",
      "3114\n",
      "284\n",
      "3125\n",
      "285\n",
      "3136\n",
      "286\n",
      "3147\n",
      "287\n",
      "3158\n",
      "288\n",
      "3169\n",
      "289\n",
      "3180\n",
      "290\n",
      "3191\n",
      "291\n",
      "3202\n",
      "292\n",
      "3213\n",
      "293\n",
      "3224\n",
      "294\n",
      "3235\n",
      "295\n",
      "3246\n",
      "296\n",
      "3257\n",
      "297\n",
      "3268\n",
      "298\n",
      "3279\n",
      "299\n",
      "3290\n",
      "300\n",
      "3301\n",
      "301\n",
      "3312\n",
      "302\n",
      "3323\n",
      "303\n",
      "3334\n",
      "304\n",
      "3345\n",
      "305\n",
      "3356\n",
      "306\n",
      "3367\n",
      "307\n",
      "3378\n",
      "308\n",
      "3389\n",
      "309\n",
      "3400\n",
      "310\n",
      "3411\n",
      "311\n",
      "3422\n",
      "312\n",
      "3433\n",
      "313\n",
      "3444\n",
      "314\n",
      "3455\n",
      "315\n",
      "3466\n",
      "316\n",
      "3477\n",
      "317\n",
      "3488\n",
      "318\n",
      "3499\n",
      "319\n",
      "3510\n",
      "320\n",
      "3521\n",
      "321\n",
      "3532\n",
      "322\n",
      "3543\n",
      "323\n",
      "3554\n",
      "324\n",
      "3565\n",
      "325\n",
      "3576\n",
      "326\n",
      "3587\n",
      "327\n",
      "3598\n",
      "328\n",
      "3609\n",
      "329\n",
      "3620\n",
      "330\n",
      "3631\n",
      "331\n",
      "3642\n",
      "332\n",
      "3653\n",
      "333\n",
      "3664\n",
      "334\n",
      "3675\n",
      "335\n",
      "3686\n",
      "336\n",
      "3697\n",
      "337\n",
      "3708\n",
      "338\n",
      "3719\n",
      "339\n",
      "3730\n",
      "340\n",
      "3741\n",
      "341\n",
      "3752\n",
      "342\n",
      "3763\n",
      "343\n",
      "3774\n",
      "344\n",
      "3785\n",
      "345\n",
      "3796\n",
      "346\n",
      "3807\n",
      "347\n",
      "3818\n",
      "348\n",
      "3829\n",
      "349\n",
      "3840\n",
      "350\n",
      "3851\n",
      "351\n",
      "3862\n",
      "352\n",
      "3873\n",
      "353\n",
      "3884\n",
      "354\n",
      "3895\n",
      "355\n",
      "3906\n",
      "356\n",
      "3917\n",
      "357\n",
      "3928\n",
      "358\n",
      "3939\n",
      "359\n",
      "3950\n",
      "360\n",
      "3961\n",
      "361\n",
      "3972\n",
      "362\n",
      "3983\n",
      "363\n",
      "3994\n",
      "364\n",
      "4005\n",
      "365\n",
      "4016\n",
      "366\n",
      "4027\n",
      "367\n",
      "4038\n",
      "368\n",
      "4049\n",
      "369\n",
      "4060\n",
      "370\n",
      "4071\n",
      "371\n",
      "4082\n",
      "372\n",
      "4093\n",
      "373\n",
      "4104\n",
      "374\n",
      "4115\n",
      "375\n",
      "4126\n",
      "376\n",
      "4137\n",
      "377\n",
      "4148\n",
      "378\n",
      "4159\n",
      "379\n",
      "4170\n",
      "380\n",
      "4181\n",
      "381\n",
      "4192\n",
      "382\n",
      "4203\n",
      "383\n",
      "4214\n",
      "384\n",
      "4225\n",
      "385\n",
      "4236\n",
      "386\n",
      "4247\n",
      "387\n",
      "4258\n",
      "388\n",
      "4269\n",
      "389\n",
      "4280\n",
      "390\n",
      "4291\n",
      "391\n",
      "4302\n",
      "392\n",
      "4313\n",
      "393\n",
      "4324\n",
      "394\n",
      "4335\n",
      "395\n",
      "4346\n",
      "396\n",
      "4357\n",
      "397\n",
      "4368\n",
      "398\n",
      "4379\n",
      "399\n",
      "4390\n",
      "400\n",
      "4401\n",
      "401\n",
      "4412\n",
      "402\n",
      "4423\n",
      "403\n",
      "4434\n",
      "404\n",
      "4445\n",
      "405\n",
      "4456\n",
      "406\n",
      "4467\n",
      "407\n",
      "4478\n",
      "408\n",
      "4489\n",
      "409\n",
      "4500\n",
      "410\n",
      "4511\n",
      "411\n",
      "4522\n",
      "412\n",
      "4533\n",
      "413\n",
      "4544\n",
      "414\n",
      "4555\n",
      "415\n",
      "4566\n",
      "416\n",
      "4577\n",
      "417\n",
      "4588\n",
      "418\n",
      "4599\n",
      "419\n",
      "4610\n",
      "420\n",
      "4621\n",
      "421\n",
      "4632\n",
      "422\n",
      "4643\n",
      "423\n",
      "4654\n",
      "424\n",
      "4665\n",
      "425\n",
      "4676\n",
      "426\n",
      "4687\n",
      "427\n",
      "4698\n",
      "428\n",
      "4709\n",
      "429\n",
      "4720\n",
      "430\n",
      "4731\n",
      "431\n",
      "4742\n",
      "432\n",
      "4753\n",
      "433\n",
      "4764\n",
      "434\n",
      "4775\n",
      "435\n",
      "4786\n",
      "436\n",
      "4797\n",
      "437\n",
      "4808\n",
      "438\n",
      "4819\n",
      "439\n",
      "4830\n",
      "440\n",
      "4841\n",
      "441\n",
      "4852\n",
      "442\n",
      "4863\n",
      "443\n",
      "4874\n",
      "444\n",
      "4885\n",
      "445\n",
      "4896\n",
      "446\n",
      "4907\n",
      "447\n",
      "4918\n",
      "448\n",
      "4929\n",
      "449\n",
      "4940\n",
      "450\n",
      "4951\n",
      "451\n",
      "4962\n",
      "452\n",
      "4973\n",
      "453\n",
      "4984\n",
      "454\n",
      "4995\n",
      "455\n",
      "5006\n",
      "456\n",
      "5017\n",
      "457\n",
      "5028\n",
      "458\n",
      "5039\n",
      "459\n",
      "5050\n",
      "460\n",
      "5061\n",
      "461\n",
      "5072\n",
      "462\n",
      "5083\n",
      "463\n",
      "5094\n",
      "464\n",
      "5105\n",
      "465\n",
      "5116\n",
      "466\n",
      "5127\n",
      "467\n",
      "5138\n",
      "468\n",
      "5149\n",
      "469\n",
      "5160\n",
      "470\n",
      "5171\n",
      "471\n",
      "5182\n",
      "472\n",
      "5193\n",
      "473\n",
      "5204\n",
      "474\n",
      "5215\n",
      "475\n",
      "5226\n",
      "476\n",
      "5237\n",
      "477\n",
      "5248\n",
      "478\n",
      "5259\n",
      "479\n",
      "5270\n",
      "480\n",
      "5281\n",
      "481\n",
      "5292\n",
      "482\n",
      "5303\n",
      "483\n",
      "5314\n",
      "484\n",
      "5325\n",
      "485\n",
      "5336\n",
      "486\n",
      "5347\n",
      "487\n",
      "5358\n",
      "488\n",
      "5369\n",
      "489\n",
      "5380\n",
      "490\n",
      "5391\n",
      "491\n",
      "5402\n",
      "492\n",
      "5413\n",
      "493\n",
      "5424\n",
      "494\n",
      "5435\n",
      "495\n",
      "5446\n",
      "496\n",
      "5457\n",
      "497\n",
      "5468\n",
      "498\n",
      "5479\n",
      "499\n",
      "5490\n",
      "500\n",
      "5501\n",
      "501\n",
      "5512\n",
      "502\n",
      "5523\n",
      "503\n",
      "5534\n",
      "504\n",
      "5545\n",
      "505\n",
      "5556\n",
      "506\n",
      "5567\n",
      "507\n",
      "5578\n",
      "508\n",
      "5589\n",
      "509\n",
      "5600\n",
      "510\n",
      "5611\n",
      "511\n",
      "5622\n",
      "512\n",
      "5633\n",
      "513\n",
      "5644\n",
      "514\n",
      "5655\n",
      "515\n",
      "5666\n",
      "516\n",
      "5677\n",
      "517\n",
      "5688\n",
      "518\n",
      "5699\n",
      "519\n",
      "5710\n",
      "520\n",
      "5721\n",
      "521\n",
      "5732\n",
      "522\n",
      "5743\n",
      "523\n",
      "5754\n",
      "524\n",
      "5765\n",
      "525\n",
      "5776\n",
      "526\n",
      "5787\n",
      "527\n",
      "5798\n",
      "528\n",
      "5809\n",
      "529\n",
      "5820\n",
      "530\n",
      "5831\n",
      "531\n",
      "5842\n",
      "532\n",
      "5853\n",
      "533\n",
      "5864\n",
      "534\n",
      "5875\n",
      "535\n",
      "5886\n",
      "536\n",
      "5897\n",
      "537\n",
      "5908\n",
      "538\n",
      "5919\n",
      "539\n",
      "5930\n",
      "540\n",
      "5941\n",
      "541\n",
      "5952\n",
      "542\n",
      "5963\n",
      "543\n",
      "5974\n",
      "544\n",
      "5985\n",
      "545\n",
      "5996\n",
      "546\n",
      "6007\n",
      "547\n",
      "6018\n",
      "548\n",
      "6029\n",
      "549\n",
      "6040\n",
      "550\n",
      "6051\n",
      "551\n",
      "6062\n",
      "552\n",
      "6073\n",
      "553\n",
      "6084\n",
      "554\n",
      "6095\n",
      "555\n",
      "6106\n",
      "556\n",
      "6117\n",
      "557\n",
      "6128\n",
      "558\n",
      "6139\n",
      "559\n",
      "6150\n",
      "560\n",
      "6161\n",
      "561\n",
      "6172\n",
      "562\n",
      "6183\n",
      "563\n",
      "6194\n",
      "564\n",
      "6205\n",
      "565\n",
      "6216\n",
      "566\n",
      "6227\n",
      "567\n",
      "6238\n",
      "568\n",
      "6249\n",
      "569\n",
      "6260\n",
      "570\n",
      "6271\n",
      "571\n",
      "6282\n",
      "572\n",
      "6293\n",
      "573\n",
      "6304\n",
      "574\n",
      "6315\n",
      "575\n",
      "6326\n",
      "576\n",
      "6337\n",
      "577\n",
      "6348\n",
      "578\n",
      "6359\n",
      "579\n",
      "6370\n",
      "580\n",
      "6381\n",
      "581\n",
      "6392\n",
      "582\n",
      "6403\n",
      "583\n",
      "6414\n",
      "584\n",
      "6425\n",
      "585\n",
      "6436\n",
      "586\n",
      "6447\n",
      "587\n",
      "6458\n",
      "588\n",
      "6469\n",
      "589\n",
      "6480\n",
      "590\n",
      "6491\n",
      "591\n",
      "6502\n",
      "592\n",
      "6513\n",
      "593\n",
      "6524\n",
      "594\n",
      "6535\n",
      "595\n",
      "6546\n",
      "596\n",
      "6557\n",
      "597\n",
      "6568\n",
      "598\n",
      "6579\n",
      "599\n",
      "6590\n",
      "600\n",
      "6601\n",
      "601\n",
      "6612\n",
      "602\n",
      "6623\n",
      "603\n",
      "6634\n",
      "604\n",
      "6645\n",
      "605\n",
      "6656\n",
      "606\n",
      "6667\n",
      "607\n",
      "6678\n",
      "608\n",
      "6689\n",
      "609\n",
      "6700\n",
      "610\n",
      "6711\n",
      "611\n",
      "6722\n",
      "612\n",
      "6733\n",
      "613\n",
      "6744\n",
      "614\n",
      "6755\n",
      "615\n",
      "6766\n",
      "616\n",
      "6777\n",
      "617\n",
      "6788\n",
      "618\n",
      "6799\n",
      "619\n",
      "6810\n",
      "620\n",
      "6821\n",
      "621\n",
      "6832\n",
      "622\n",
      "6843\n",
      "623\n",
      "6854\n",
      "624\n",
      "6865\n",
      "625\n",
      "6876\n",
      "626\n",
      "6887\n",
      "627\n",
      "6898\n",
      "628\n",
      "6909\n",
      "629\n",
      "6920\n",
      "630\n",
      "6931\n",
      "631\n",
      "6942\n",
      "632\n",
      "6953\n",
      "633\n",
      "6964\n",
      "634\n",
      "6975\n",
      "635\n",
      "6986\n",
      "636\n",
      "6997\n",
      "637\n",
      "7008\n",
      "638\n",
      "7019\n",
      "639\n",
      "7030\n",
      "640\n",
      "7041\n",
      "641\n",
      "7052\n",
      "642\n",
      "7063\n",
      "643\n",
      "7074\n",
      "644\n",
      "7085\n",
      "645\n",
      "7096\n",
      "646\n",
      "7107\n",
      "647\n",
      "7118\n",
      "648\n",
      "7129\n",
      "649\n",
      "7140\n",
      "650\n",
      "7151\n",
      "651\n",
      "7162\n",
      "652\n",
      "7173\n",
      "653\n",
      "7184\n",
      "654\n",
      "7195\n",
      "655\n",
      "7206\n",
      "656\n",
      "7217\n",
      "657\n",
      "7228\n",
      "658\n",
      "7239\n",
      "659\n",
      "7250\n",
      "660\n",
      "7261\n",
      "661\n",
      "7272\n",
      "662\n",
      "7283\n",
      "663\n",
      "7294\n",
      "664\n",
      "7305\n",
      "665\n",
      "7316\n",
      "666\n",
      "7327\n",
      "667\n",
      "7338\n",
      "668\n",
      "7349\n",
      "669\n",
      "7360\n",
      "670\n",
      "7371\n",
      "671\n",
      "7382\n",
      "672\n",
      "7393\n",
      "673\n",
      "7404\n",
      "674\n",
      "7415\n",
      "675\n",
      "7426\n",
      "676\n",
      "7437\n",
      "677\n",
      "7448\n",
      "678\n",
      "7459\n",
      "679\n",
      "7470\n",
      "680\n",
      "7481\n",
      "681\n",
      "7492\n",
      "682\n",
      "7503\n",
      "683\n",
      "7514\n",
      "684\n",
      "7525\n",
      "685\n",
      "7536\n",
      "686\n",
      "7547\n",
      "687\n",
      "7558\n",
      "688\n",
      "7569\n",
      "689\n",
      "7580\n",
      "690\n",
      "7591\n",
      "691\n",
      "7602\n",
      "692\n",
      "7613\n",
      "693\n",
      "7624\n",
      "694\n",
      "7635\n",
      "695\n",
      "7646\n",
      "696\n",
      "7657\n",
      "697\n",
      "7668\n",
      "698\n",
      "7679\n",
      "699\n",
      "7690\n",
      "700\n",
      "7701\n",
      "701\n",
      "7712\n",
      "702\n",
      "7723\n",
      "703\n",
      "7734\n",
      "704\n",
      "7745\n",
      "705\n",
      "7756\n",
      "706\n",
      "7767\n",
      "707\n",
      "7778\n",
      "708\n",
      "7789\n",
      "709\n",
      "7800\n",
      "710\n",
      "7811\n",
      "711\n",
      "7822\n",
      "712\n",
      "7833\n",
      "713\n",
      "7844\n",
      "714\n",
      "7855\n",
      "715\n",
      "7866\n",
      "716\n",
      "7877\n",
      "717\n",
      "7888\n",
      "718\n",
      "7899\n",
      "719\n",
      "7910\n",
      "720\n",
      "7921\n",
      "721\n",
      "7932\n",
      "722\n",
      "7943\n",
      "723\n",
      "7954\n",
      "724\n",
      "7965\n",
      "725\n",
      "7976\n",
      "726\n",
      "7987\n",
      "727\n",
      "7998\n",
      "728\n",
      "8009\n",
      "729\n",
      "8020\n",
      "730\n",
      "8031\n",
      "731\n",
      "8042\n",
      "732\n",
      "8053\n",
      "733\n",
      "8064\n",
      "734\n",
      "8075\n",
      "735\n",
      "8086\n",
      "736\n",
      "8097\n",
      "737\n",
      "8108\n",
      "738\n",
      "8119\n",
      "739\n",
      "8130\n",
      "740\n",
      "8141\n",
      "741\n",
      "8152\n",
      "742\n",
      "8163\n",
      "743\n",
      "8174\n",
      "744\n",
      "8185\n",
      "745\n",
      "8196\n",
      "746\n",
      "8207\n",
      "747\n",
      "8218\n",
      "748\n",
      "8229\n",
      "749\n",
      "8240\n",
      "750\n",
      "8251\n",
      "751\n",
      "8262\n",
      "752\n",
      "8273\n",
      "753\n",
      "8284\n",
      "754\n",
      "8295\n",
      "755\n",
      "8306\n",
      "756\n",
      "8317\n",
      "757\n",
      "8328\n",
      "758\n",
      "8339\n",
      "759\n",
      "8350\n",
      "760\n",
      "8361\n",
      "761\n",
      "8372\n",
      "762\n",
      "8383\n",
      "763\n",
      "8394\n",
      "764\n",
      "8405\n",
      "765\n",
      "8416\n",
      "766\n",
      "8427\n",
      "767\n",
      "8438\n",
      "768\n",
      "8449\n",
      "769\n",
      "8460\n",
      "770\n",
      "8471\n",
      "771\n",
      "8482\n",
      "772\n",
      "8493\n",
      "773\n",
      "8504\n",
      "774\n",
      "8515\n",
      "775\n",
      "8526\n",
      "776\n",
      "8537\n",
      "777\n",
      "8548\n",
      "778\n",
      "8559\n",
      "779\n",
      "8570\n",
      "780\n",
      "8581\n",
      "781\n",
      "8592\n",
      "782\n",
      "8603\n",
      "783\n",
      "8614\n",
      "784\n",
      "8625\n",
      "785\n",
      "8636\n",
      "786\n",
      "8647\n",
      "787\n",
      "8658\n",
      "788\n",
      "8669\n",
      "789\n",
      "8680\n",
      "790\n",
      "8691\n",
      "791\n",
      "8702\n",
      "792\n",
      "8713\n",
      "793\n",
      "8724\n",
      "794\n",
      "8735\n",
      "795\n",
      "8746\n",
      "796\n",
      "8757\n",
      "797\n",
      "8768\n",
      "798\n",
      "8779\n",
      "799\n",
      "8790\n",
      "800\n",
      "8801\n",
      "801\n",
      "8812\n",
      "802\n",
      "8823\n",
      "803\n",
      "8834\n",
      "804\n",
      "8845\n",
      "805\n",
      "8856\n",
      "806\n",
      "8867\n",
      "807\n",
      "8878\n",
      "808\n",
      "8889\n",
      "809\n",
      "8900\n",
      "810\n",
      "8911\n",
      "811\n",
      "8922\n",
      "812\n",
      "8933\n",
      "813\n",
      "8944\n",
      "814\n",
      "8955\n",
      "815\n",
      "8966\n",
      "816\n",
      "8977\n",
      "817\n",
      "8988\n",
      "818\n",
      "8999\n",
      "819\n",
      "9010\n",
      "820\n",
      "9021\n",
      "821\n",
      "9032\n",
      "822\n",
      "9043\n",
      "823\n",
      "9054\n",
      "824\n",
      "9065\n",
      "825\n",
      "9076\n",
      "826\n",
      "9087\n",
      "827\n",
      "9098\n",
      "828\n",
      "9109\n",
      "829\n",
      "9120\n",
      "830\n",
      "9131\n",
      "831\n",
      "9142\n",
      "832\n",
      "9153\n",
      "833\n",
      "9164\n",
      "834\n",
      "9175\n",
      "835\n",
      "9186\n",
      "836\n",
      "9197\n",
      "837\n",
      "9208\n",
      "838\n",
      "9219\n",
      "839\n",
      "9230\n",
      "840\n",
      "9241\n",
      "841\n",
      "9252\n",
      "842\n",
      "9263\n",
      "843\n",
      "9274\n",
      "844\n",
      "9285\n",
      "845\n",
      "9296\n",
      "846\n",
      "9307\n",
      "847\n",
      "9318\n",
      "848\n",
      "9329\n",
      "849\n",
      "9340\n",
      "850\n",
      "9351\n",
      "851\n",
      "9362\n",
      "852\n",
      "9373\n",
      "853\n",
      "9384\n",
      "854\n",
      "9395\n",
      "855\n",
      "9406\n",
      "856\n",
      "9417\n",
      "857\n",
      "9428\n",
      "858\n",
      "9439\n",
      "859\n",
      "9450\n",
      "860\n",
      "9461\n",
      "861\n",
      "9472\n",
      "862\n",
      "9483\n",
      "863\n",
      "9494\n",
      "864\n",
      "9505\n",
      "865\n",
      "9516\n",
      "866\n",
      "9527\n",
      "867\n",
      "9538\n",
      "868\n",
      "9549\n",
      "869\n",
      "9560\n",
      "870\n",
      "9571\n",
      "871\n",
      "9582\n",
      "872\n",
      "9593\n",
      "873\n",
      "9604\n",
      "874\n",
      "9615\n",
      "875\n",
      "9626\n",
      "876\n",
      "9637\n",
      "877\n",
      "9648\n",
      "878\n",
      "9659\n",
      "879\n",
      "9670\n",
      "880\n",
      "9681\n",
      "881\n",
      "9692\n",
      "882\n",
      "9703\n",
      "883\n",
      "9714\n",
      "884\n",
      "9725\n",
      "885\n",
      "9736\n",
      "886\n",
      "9747\n",
      "887\n",
      "9758\n",
      "888\n",
      "9769\n",
      "889\n",
      "9780\n",
      "890\n",
      "9791\n",
      "891\n",
      "9802\n",
      "892\n",
      "9813\n",
      "893\n",
      "9824\n",
      "894\n",
      "9835\n",
      "895\n",
      "9846\n",
      "896\n",
      "9857\n",
      "897\n",
      "9868\n",
      "898\n",
      "9879\n",
      "899\n",
      "9890\n",
      "900\n",
      "9901\n",
      "901\n",
      "9912\n",
      "902\n",
      "9923\n",
      "903\n",
      "9934\n",
      "904\n",
      "9945\n",
      "905\n",
      "9956\n",
      "906\n",
      "9967\n",
      "907\n",
      "9978\n",
      "908\n",
      "9989\n",
      "909\n",
      "10000\n",
      "910\n",
      "10011\n",
      "911\n",
      "10022\n",
      "912\n",
      "10033\n",
      "913\n",
      "10044\n",
      "914\n",
      "10055\n",
      "915\n",
      "10066\n",
      "916\n",
      "10077\n",
      "917\n",
      "10088\n",
      "918\n",
      "10099\n",
      "919\n",
      "10110\n",
      "920\n",
      "10121\n",
      "921\n",
      "10132\n",
      "922\n",
      "10143\n",
      "923\n",
      "10154\n",
      "924\n",
      "10165\n",
      "925\n",
      "10176\n",
      "926\n",
      "10187\n",
      "927\n",
      "10198\n",
      "928\n",
      "10209\n",
      "929\n",
      "10220\n",
      "930\n",
      "10231\n",
      "931\n",
      "10242\n",
      "932\n",
      "10253\n",
      "933\n",
      "10264\n",
      "934\n",
      "10275\n",
      "935\n",
      "10286\n",
      "936\n",
      "10297\n",
      "937\n",
      "10308\n",
      "938\n",
      "10319\n",
      "939\n",
      "10330\n",
      "940\n",
      "10341\n",
      "941\n",
      "10352\n",
      "942\n",
      "10363\n",
      "943\n",
      "10374\n",
      "944\n",
      "10385\n",
      "945\n",
      "10396\n",
      "946\n",
      "10407\n",
      "947\n",
      "10418\n",
      "948\n",
      "10429\n",
      "949\n",
      "10440\n",
      "950\n",
      "10451\n",
      "951\n",
      "10462\n",
      "952\n",
      "10473\n",
      "953\n",
      "10484\n",
      "954\n",
      "10495\n",
      "955\n",
      "10506\n",
      "956\n",
      "10517\n",
      "957\n",
      "10528\n",
      "958\n",
      "10539\n",
      "959\n",
      "10550\n",
      "960\n",
      "10561\n",
      "961\n",
      "10572\n",
      "962\n",
      "10583\n",
      "963\n",
      "10594\n",
      "964\n",
      "10605\n",
      "965\n",
      "10616\n",
      "966\n",
      "10627\n",
      "967\n",
      "10638\n",
      "968\n",
      "10649\n",
      "969\n",
      "10660\n",
      "970\n",
      "10671\n",
      "971\n",
      "10682\n",
      "972\n",
      "10693\n",
      "973\n",
      "10704\n"
     ]
    }
   ],
   "source": [
    "preguntas_mezcla = []\n",
    "resp_mezcla = []\n",
    "\n",
    "for i, pregunta in enumerate(preguntas_sin):\n",
    "    preguntas_mezcla.append(pregunta)\n",
    "    resp_mezcla.append(respuestas3[i])\n",
    "    print(i)\n",
    "    print(len(resp_mezcla))\n",
    "    for j in range(10):\n",
    "        preguntas_mezcla.append(mezclarPalabras(pregunta))\n",
    "        resp_mezcla.append(respuestas3[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10714"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preguntas_mezcla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10714"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(resp_mezcla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "preguntas3, respuestas3 = shuffle(preguntas_mezcla,resp_mezcla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminarStopWords(preg):\n",
    "    frases = []\n",
    "    palabra_sola = []\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    for i,pregunta in enumerate(preg):\n",
    "        palabras = tokenizer.tokenize(pregunta)\n",
    "        preguntas_w = []\n",
    "        for j, palabra in enumerate(reversed(palabras)):\n",
    "            preguntas_w.append(palabra)\n",
    "            palabra_sola.append(palabra)\n",
    "            if(j == 10):\n",
    "                continue\n",
    "        frases.append(preguntas_w)\n",
    "    return frases, palabra_sola"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Es necesario extraer las palabras del conjunto de test.\n",
    "\n",
    "frases, palabra_sola = eliminarStopWords(preguntas3[:8500])\n",
    "frases_test, _ = eliminarStopWords(preguntas3[8500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_unicas = set(palabra_sola)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2305"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(palabras_unicas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_unicas = list(palabras_unicas)\n",
    "\n",
    "#Voy a convertir los indices a escala logaritmica para evitar que puedan reventar los pesos en la red neuronal\n",
    "vocabulario = {p:i for i, p in enumerate(p_unicas)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulario['<OOP>'] = len(vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El tamano max es  31  y la media de la longitud de las frases es de 12.839058823529411\n"
     ]
    }
   ],
   "source": [
    "tamanoMedio = 0\n",
    "tamanoTotal = 0\n",
    "\n",
    "for pregunta in frases:\n",
    "    if(len(pregunta) > tamanoTotal):\n",
    "        tamanoTotal = len(pregunta)\n",
    "    tamanoMedio = tamanoMedio + len(pregunta)\n",
    "\n",
    "print(\"El tamano max es \", tamanoTotal, \" y la media de la longitud de las frases es de\", tamanoMedio/len(frases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voy a meter en este vector todas mis preguntas y todas las palabras.\n",
    "# Para esta prueba, vamos a poner en la posición de la frase, el número de la palabra que estamos procesando.\n",
    "# El objetivo es procesar las palabras teniendo en cuenta el orden secuencial de la frase.\n",
    "training_X = np.zeros([len(frases), tamanoTotal, len(palabras_unicas)+1])\n",
    "test_X = np.zeros([len(frases_test), tamanoTotal, len(palabras_unicas)+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8500, 31, 2306)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_X.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pregunta in enumerate(frases):\n",
    "    for j, palabras in enumerate(pregunta):\n",
    "        training_X[i, tamanoTotal - len(pregunta) + j, vocabulario[palabras]] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['toca', 'sardinas', '110', 'milla', 'discos', 'Cuál', 'aula', 'dibujo', 'cuadrados', 'Paso', '342', 'Alyssa', 'guardan', 'juntan', 'hombres', 'comienza', 'Harold', 'usa', 'pelirroja', '320', 'Lillian', 'llego', 'Jill', 'Pusiste', '75', 'señora', 'sección', 'azulejos', 'aluminio', 'páginas', '32', 'capturada', 'Recibe', 'sellos', 'sentaron', 'trabajo', '275', 'Encuentra', '28', 'Por', 'Catherine', 'Connie', '93', 'inglés', 'hornear', 'omnibus', 'conchas', 'segunda', 'garrafas', 'asiento', 'plantas', 'compuesta', 'cuántas', 'organizados', 'Comió', 'Tim', 'Pablo', 'centavos', '8', 'castores', 'nuez', 'mesas', 'Anna', '99', 'Lino', '79', 'roja', 'Katy', 'pastel', 'autos', 'reunión', 'sobran', '59', '3409', 'armó', 'zorra', 'peras', 'Pedro', 'clásica', 'kilómetros', 'material', 'ciruelas', 'llevar', 'luces', 'fichas', 'necesito', 'usaron', 'Jane', 'touchdowns', '82', 'Bajaron', 'dividió', '750', '261', 'patos', 'galones', 'pulgadas', 'días', 'Agustín', '95', 'hijos', 'Carol', 'conduciría', 'ida', 'Diego', 'tucán', 'compró', 'DVDs', 'vivas', 'queda', 'El', 'oídos', 'cromos', 'tienda', '67', '231', 'quedara', 'sales', 'verdulería', 'Cuántas', 'Marilyn', 'marcado', 'quiere', 'sándwiches', 'rotas', 'aritos', 'saltaron', 'Limpié', 'Su', 'asistieron', 'gasto', 'utilizó', 'visitar', 'desayunos', 'huevería', 'chupetines', 'harina', 'nacen', 'kiosco', 'camión', 'negra', '309', 'necesitó', 'libreta', 'formas', 'remera', 'Cuando', '160', 'plátanos', 'perro', 'viene', 'Carson', 'media', 'recibiría', 'vienen', 'juntaran', 'A', 'Thomas', 'cabía', 'tarro', 'él', 'socios', 'euros', 'jugada', 'Gloria', 'secan', 'Sofía', 'tendrá', 'hamburguesas', 'zanahoria', 'diez', 'fabrican', 'tulipanes', 'rompen', 'arriba', 'ladrar', 'rosquillas', 'Mark', '20', 'Entonces', 'contenedor', 'pipas', 'postal', 'caminas', 'camiseta', 'Cremes', 'Pierde', 'habian', 'hucha', '327', 'avena', 'llevaba', 'almacén', 'contenido', '43', 'km', 'comidos', 'puertas', '37', 'Horacio', 'Lleva', 'alrededor', 'Fred', 'fabricarlos', 'Algunas', 'da', 'zapateros', 'muebles', 'llenarla', 'come', 'Nancy', 'borde', 'planeando', 'albóndigas', 'juguetonas', 'último', 'tendría', 'ramos', 'mató', 'paquete', 'sentada', 'estudiantes', 'col', 'tener', 'Pedimos', 'queso', 'pastelería', '465', 'Después', '3', 'vestirse', '294', 'pegan', 'con', 'Necesita', '179', 'gaseosas', 'Hoy', 'libra', '50', 'Quiero', 'Phillip', 'Víctor', '9306', 'Hasta', 'fabricado', 'dado', 'cubo', 'crayones', 'dos', 'vendio', 'fruta', 'John', 'repartió', 'escribiendo', 'pequeños', 'velitas', 'longitud', '68', 'siguen', 'peso', 'comerciante', 'leer', 'cosecha', '375', 'colocar', 'serpientes', 'pintura', 'sábado', 'magdalenas', 'anotaría', 'conducía', 'alcanzó', 'árboles', 'grupo', 'Andrew', 'papel', 'llenar', 'transporta', 'formamos', '24', 'pantano', '27', 'comprando', 'montón', '44', 'paternos', 'pagado', '70', 'dejó', 'ayer', 'juego', 'George', 'dólares', 'rubias', 'podrán', 'amiga', '544', 'panecillos', 'oficial', 'botella', 'Los', 'Cuánto', 'ramo', 'dados', 'navideños', 'encontré', 'combinar', 'hija', 'Clarence', 'corazón', 'acuario', 'chico', 'tazas', 'armario', 'sola', 'En', 'cuento', 'volaron', 'colección', 'Tenemos', 'concierto', 'caramelo', 'ladrillos', 'noria', 'bajan', '292', '12', 'bote', 'grande', 'postales', 'gustaría', 'hicieron', 'hipopótamos', 'corrió', 'cómic', 'papa', '64', 'polo', 'bolas', 'Sra', 'usará', '250', 'velocidad', 'envases', 'octubre', 'midió', 'Sr', '29', 'pasteles', 'niñas', 'recorrió', 'Calcula', 'problemas', 'marinas', 'vestido', 'frambuesas', 'merluzas', 'alguna', 'tine', '000', 'dar', 'sombrero', 'metros', 'pétalos', 'regalado', 'lechugas', 'colores', 'Quién', 'manzana', 'cuesta', '998', 'paco', 'seguidos', 'añadió', 'Cuanto', 'ritmo', 'Matemáticas', 'llegó', '52', 'unieron', 'pinta', '23', '263', 'arma', 'rebanadas', 'Harry', 'Brian', 'Relevo', 'ventanas', 'Joaquín', 'porción', '97', 'competición', 'practicaba', '168', 'Brandon', 'ambos', 'casilleros', 'Mis', 'Kirsten', 'parada', 'guardará', 'cinco', 'necesitará', 'Eliana', 'perdido', 'gran', 'atardecer', 'reserva', '237', 'apples', 'cubre', '304', 'recibe', 'tardó', 'capítulos', 'Primero', 'repartir', 'Jackson', 'empaquetadas', 'bidones', 'Compra', 'ancho', 'seran', 'dormitorios', 'peregrino', 'venden', 'decidieron', 'Sumata', 'avanzó', 'flor', 'Kenia', 'menta', 'asientos', 'Cuantos', 'Sobraron', '648', 'granjero', 'cerezas', 'Kimberly', 'contienen', 'etapa', 'dieron', 'in', 'ciudad', 'montones', 'niños', 'haber', 'Beberly', 'periódicos', 'Park', 'tia', 'ninguno', 'queden', '200', 'Gael', 'Mikey', 'té', 'CUantas', 'nuevo', 'objetos', 'Ahora', 'litro', '87', 'ganó', '92', 'bolitas', 'caballos', 'Iesha', 'buhos', 'canastas', '300', 'valen', 'gradas', 'transportar', 'hecho', 'bloques', 'Más', '55', 'crecido', 'Nuestras', 'enrolló', 'Cual', 'gallina', 'obra', 'sobra', 'sobrarán', '210', 'sobrina', '5', '344', '276', 'bicicleta', 'creados', 'Cuatro', 'Chalet', 'alfajores', 'divididas', 'mamá', 'dividos', '100', 'alquiló', '62', 'mesa', 'Timothy', 'sentados', 'enrollaron', 'marcadores', 'vestir', '69', 'andar', 'viejas', 'Cada', 'Doris', 'entrará', 'cazadores', '489', 'cambio', 'comerse', '234', 'José', 'pantalón', 'arvejas', 'tornillos', 'presta', 'metieron', 'compartan', 'siempre', 'comprado', '1750', '270', 'cada', 'Scott', 'preparó', 'Diana', 'sobraron', 'Paula', 'rompe', 'empieza', 'calzas', 'barco', '2186', 'colocamos', 'saca', '47', 'canasta', 'locomotora', '966', 'surtidor', 'salen', 'gramos', '74', 'yoyo', 'Cody', 'obtiene', 'cuarta', 'ambas', 'estudiante', 'ayudarán', 'DVD', 'Trabajó', 'perdí', 'truchas', 'jarras', 'pintar', 'pesan', 'nevera', 'patas', 'mariquitas', 'morocha', 'día', 'quieres', 'ello', 'gastó', 'transportará', 'Sebastían', 'fans', 'mitad', 'casillero', 'profesores', 'cuadernos', '42', '83', 'comprarles', 'partió', 'mataron', '147', 'manos', 'mes', 'familia', 'viajes', 'jarra', 'puedo', 'Sheridan', 'tenia', 'tantos', '134', 'entregar', 'blanco', 'Quiere', 'tranquilamente', 'Donald', 'tendrían', 'detuvo', 'debemos', 'Pagó', 'Heine', 'impresionó', 'Alrededor', 'almacenero', 'horas', 'quedó', 'Comparte', '91', 'medias', 'colegio', 'creyones', 'camisas', 'lago', 'indicaba', 'unirse', 'Caminé', 'Roden', 'tiene', 'juguete', 'Ruth', '356', 'reposeras', 'acertado', 'retiran', 'obrero', '455', 'recorrieron', 'bananas', 'Natalia', 'originalmente', 'obtendré', 'cerdos', 'Valeria', 'país', 'alumnos', 'duraznos', 'sacó', 'regaló', 'pueden', 'She', '154', 'pedazos', 'guardas', 'entran', '600', 'primero', 'Nicolas', 'quedaba', 'encestado', 'conducir', 'botones', 'fruteros', 'cartero', 'He', 'banana', 'habita', 'promedio', 'Brett', 'garaje', 'vagó', 'Sara', 'rocas', 'paseo', 'también', 'Estás', 'Carl', 'agrega', 'larga', 'comida', '81', 'montados', '142', 'conseguir', 'tizas', 'ganar', 'irán', 'camina', 'camiones', 'demás', 'auto', 'varios', 'ballena', 'pelar', '117', 'María', 'puesto', 'reparado', 'club', 'pesada', 'playa', 'perfecta', '183', 'mph', 'maestra', 'chicle', 'coco', 'miércoles', 'otra', 'Gabriel', 'pongo', 'Kyoko', 'Juanjo', '873', 'Susana', 'hambrientas', 'bien', 'cuadra', 'suben', 'quita', 'tenis', 'recogieron', 'comparado', 'cantidad', 'paseó', 'hacerlas', 'son', 'bolos', 'pisos', 'puntos', 'compañeros', '109', '532', 'paleta', 'quedo', 'bandeja', 'Toda', 'organizadas', 'rollos', 'todas', 'libres', 'torneo', 'inflados', 'pizza', 'tapas', 'nutella', 'Cenicienta', 'venderlos', 'alcanzara', 'mide', 'huevos', 'zapatos', 'masitas', 'cabezas', 'Usó', 'requiere', 'remeras', 'vendaval', 'todo', 'albañil', 'nada', 'afila', 'tercero', 'nadando', 'fallado', 'jugando', 'marco', 'repartirlos', 'ordenadores', 'patio', 'coleccion', '96', '119', 'blusa', 'cien', 'juntaron', 'libro', 'dará', 'empanadas', 'exámenes', 'flores', 'tengo', 'subieron', 'comer', 'botes', '0', 'Mercedes', 'mantengo', 'pelo', 'kilo', '2438', 'chicles', '34', 'Jennifer', 'nietos', 'conseguido', 'pan', 'Deborah', 'forma', 'cabello', 'chicas', 'tíos', 'Katherine', 'cabían', 'asomé', 'have', '60', 'toda', 'lucieron', 'Ted', 'viajó', 'Benjamin', 'recorrerá', 'marrones', 'vacíos', 'parque', 'había', 'negro', 'naranja', 'Una', 'dónde', 'Misha', 'quinto', '48', 'campo', 'Andre', 'Susan', 'primera', 'siguiente', 'trampolín', '218', 'compartirlas', 'Marvin', 'restaurant', 'lanzado', 'Ashley', 'pesa', 'toma', 'torre', 'autitos', 'hago', 'bolsillo', '698', 'Ambos', 'Quieres', 'almacenista', 'rubia', 'guardadas', 'piso', '360', 'helado', 'añaden', '124', 'ajedrez', 'temporada', 'edad', 'abandonaron', 'amarillo', 'repartirlas', 'cuatro', 'pasado', 'mostraba', 'lado', 'pasó', 'fuente', 'Iván', 'supermercado', 'maceta', 'Guzmán', 'almorzar', 'camping', 'cacahuetes', 'une', '187', 'segundos', 'buses', 'grandes', 'bolígrafros', 'mandado', 'Nicholas', '21', 'cuentra', '19', 'hermana', 'hoy', 'corresponden', 'desenterró', '175', 'Jeremy', '324', 'hora', 'Virginia', '900', 'termino', 'peregrinación', 'bolsa', 'lápices', 'kilogramos', 'largo', '61', 'meses', '53', 'garbanzos', 'compra', 'tragó', 'Cuánta', 'final', 'necesitarán', 'bandejas', 'uvas', '78', 'Están', 'Legos', 'Karen', 'inflado', 'Rosa', 'decorar', 'pegar', 'Y', 'peniques', 'patrulla', 'tarjetas', 'Las', 'azules', 'Ramón', 'Jacqueline', 'vacías', 'Garrett', 'saco', 'pescado', 'tendré', 'Álvaro', 'pared', 'perrito', 'corría', '121', 'as', 'utiliza', 'granel', 'pulgas', 'báscula', 'allí', 'ahorrados', 'transparentes', 'Christine', 'nueve', '65', 'comen', '1112', 'niño', 'ponen', 'colocados', 'equipos', 'Edward', 'quedas', 'robó', 'perros', 'recoge', '706', 'conejos', 'vendió', 'Cuantas', 'Así', '33', 'derriba', 'empezar', '387', 'Douglas', 'costado', 'bolígrafo', 'igual', 'primer', 'kilos', 'arenero', 'colocará', 'bombones', 'abuelos', 'festiva', 'cumpleaños', 'llenan', 'toque', 'Recogió', 'tío', 'fin', 'animales', 'bocadillos', 'nuevas', 'Mónica', 'puestos', 'monedero', 'boa', 'llenaré', 'Louise', '45', 'Utiliza', 'escolar', 'juntos', 'escritorio', 'colecciona', '157', 'secciones', 'aficionados', 'plato', 'La', 'pecera', 'cedro', 'Lucía', 'mano', 'debería', 'primo', 'faltado', 'polleras', 'Va', '252', 'naranjas', 'cucurucho', 'melones', 'lotes', 'goles', 'granero', 'palitos', 'doradas', '5256', 'recogido', 'avión', '115', 'Mi', 'tiempo', 'nadar', 'millas', '504', 'factura', 'cesta', 'salida', 'gatos', '782', '26', 'latas', 'empaca', 'será', 'rápido', 'casa', 'Walter', 'repartido', 'pelado', 'anotó', 'haciendo', 'agricultor', 'estantería', 'silbato', 'Kristi', 'beben', 'Dentro', 'camino', 'Tienes', 'primeros', 'lectura', 'triciclos', 'Christopher', 'Un', 'Fernando', 'lugares', 'Brad', 'frutilla', 'frutera', 'Esta', '550', 'vez', '167', '1028', 'ocho', 'cochecitos', 'Wong', 'euro', 'vendido', 'Esa', 'Charlie', 'Vida', 'trabajando', 'Algunos', 'Steve', 'pista', 'huesos', 'tejado', 'obtendrá', 'engordado', 'salud', '63', 'amigo', '150', 'más', 'necesarios', 'Come', 'agregan', 'materiales', 'mayor', 'vendida', 'comidas', 'piruleta', 'quedan', 'batidos', 'velas', 'relevos', 'Jovana', 'partido', 'tortas', 'mete', 'receta', 'alcanza', 'recoger', 'calle', 'doce', 'descansar', 'ultimo', 'ahorró', 'oficina', 'viajeros', 'acto', 'llevaron', '17', 'terminó', 'zoológico', 'escuela', 'prendas', 'compitiendo', 'llenó', 'familiares', 'planta', 'galletas', 'paginas', 'litros', 'trajo', 'fútbol', 'guardados', 'hospital', 'coches', 'peral', 'hechas', 'blancas', 'música', 'reparan', 'golden', 'Primaria', '362', 'Ceasar', 'Larry', '16', 'ordenar', 'caen', 'Añadió', 'harán', 'galón', 'Norma', 'obtuvimos', 'aulas', 'Ms', 'divididos', 'llevó', 'Brecknock', 'Arma', 'paquetes', 'pintan', '73', 'mejores', 'jueves', 'montar', '38', 'finalizarlo', 'Madrid', 'tablero', 'primaria', 'rayan', 'derecho', 'Carolyn', 'prepardo', '269', 'podrían', 'Noelia', 'etiquetas', 'ciclistas', 'Green', '9', 'pegatinas', 'arroz', 'estuche', 'constrictora', 'pollitos', 'primos', 'Adrián', 'Julian', 'trasero', 'Nina', 'Nintendo', 'Kevin', 'Pega', 'colocaron', 'caminará', 'ticket', 'empaqueta', 'pastillas', 'Lori', 'orejas', 'viajo', 'Teresa', 'desalojarse', 'basura', 'cuentan', 'regalo', 'caben', 'tocan', 'muñecas', 'comía', 'vecinos', 'viendo', 'distribuidor', 'total', '203', '476', 'meta', 'rojos', 'camisetas', 'jabón', 'de', 'vacaciones', 'cartas', 'Joan', 'Raúl', 'pila', 'Eugene', 'panes', 'cedros', 'encuentras', 'tortilla', 'moderna', 'reuní', 'Yanina', 'margaritas', 'frasco', 'repetir', 'tobogán', 'Tiene', 'set', 'Tréboles', 'turistas', '578', '964', 'restauración', 'nueces', 'béisbol', 'sólo', 'ejército', 'Ana', 'darle', 'Anne', 'verduleria', 'formar', 'Pat', 'Puso', '65899', 'jaulas', 'uva', 'B', 'dio', 'Pinocho', 'izquierdo', 'También', '7', 'partes', 'vuelan', 'mil', 'Theresa', '120', 'Gastó', 'Terry', 'hacer', 'peceras', 'viaje', 'armar', '679', 'jardín', 'tres', '542', 'bichos', 'juegos', 'lavar', 'dorados', 'negocio', 'gris', 'Acamparon', 'dividir', 'camisa', 'retiraron', 'medialunas', 'países', 'Cuántos', 'explotado', '425', 'marcharon', 'historia', 'serpiente', 'carta', 'panteras', 'año', 'lisos', 'Willie', 'tomaron', 'devuelven', 'balones', 'tercer', 'mañana', 'oído', 'pizzas', 'revista', 'cabalgó', 'céntimos', '593', 'grillos', '101', 'fotogramas', 'lapices', 'cogido', 'amarillas', 'tan', 'pasajeros', 'Sarah', 'canicas', 'Aaron', '105', 'deportes', 'bolsas', 'y', 'uno', '500', 'piezas', 'lleva', 'rompieron', 'iguales', 'Di', 'clase', '126', '56', 'Shirley', 'Calcular', 'depósitos', 'Joyce', 'lápiz', 'junta', 'carga', 'confites', 'romper', 'viajaban', 'pagó', 'cereales', 'siembra', 'alfombra', 'Paradise', 'Jack', 'confitería', 'cine', 'teníamos', 'estantes', '4', 'invitando', 'comido', 'Darío', 'Marie', 'diferente', 'vinieron', 'contener', 'ayudar', 'bolsitas', 'tercera', 'añadir', 'corta', 'otro', 'Habian', 'hijo', 'catedral', 'invitados', 'huevo', 'condujo', '2650', 'Ronald', 'gasolina', 'semana', 'puntuación', 'poblaciones', 'sientan', 'encontrar', 'pilas', 'décimas', 'Santiago', 'útiles', '2987', '46', 'Tomás', 'trajes', 'algodón', 'tardará', 'panadería', 'parte', 'silbatos', 'calendarios', 'donuts', 'trabajó', 'tiro', 'llevará', '125', '139', 'rojas', '13', 'compartiérais', 'recorre', 'pone', 'cuales', 'encuentra', 'debe', 'trayecto', 'viajado', 'fila', 'padre', 'Cuan', 'puntaje', 'presentes', '208', 'balsa', 'segundo', 'comprarse', 'bebe', 'aceite', 'Repartió', 'Kathryn', 'rosas', 'tejas', 'torta', 'puede', 'Perdió', 'taza', 'contó', 'carrera', 'plantó', 'España', 'curso', 'ladrillo', 'Se', 'menor', 'Antonio', 'libras', 'semanas', 'Durante', 'leí', 'entonces', 'panera', 'sets', 'puso', 'Con', 'mascota', 'teatro', 'Halloween', 'construir', 'dulces', 'granja', 'ordenador', 'Cade', 'Tu', 'tomando', 'escuelas', 'pagar', 'costarían', 'trozos', 'marcos', 'cerca', 'Bob', 'costará', 'trajeron', 'ser', 'frutero', '403', 'azul', 'murieron', 'necesitas', 'condujeron', 'Maria', 'bajaron', 'Heather', 'libros', 'tierra', 'sobras', 'marcó', 'amigas', 'chocolates', 'tren', 'borrar', 'collares', 'sacado', 'encargaron', 'Wayne', 'estación', 'balcón', 'billete', 'elefantes', 'Michelle', 'plantado', 'entradas', 'insectos', 'norma', 'fresa', 'dia', 'No', 'nidos', 'tomates', 'compro', 'Ha', 'juguetería', '280', 'visitan', 'tomó', 'mañanas', 'mar', 'Janice', 'trabajan', 'plantadas', 'pasos', 'Cheryl', 'barra', 'bolígrafos', '11', 'traje', 'baldosas', 'Mary', 'tienen', 'aterrizan', '36', 'gastan', 'produce', 'Helen', 'cartera', '1215', '136', 'escuala', 'Pepito', 'Ernest', 'bocina', 'tapa', 'centésimas', 'autobuses', 'abejas', '312', 'abetos', 'coles', 'Jazmín', 'Gaby', 'maduras', 'iban', '39', 'participan', 'amarillos', 'tendero', 'ahora', 'prepara', 'Javier', 'ropa', 'recibirá', 'Darius', 'cabe', 'disputa', 'Lemon', 'persona', 'globos', 'agua', 'claveles', 'Charles', 'resuelto', 'Bruce', 'anillos', 'rebajas', 'selva', 'suministros', 'Rupert', 'rotos', 'chocolate', 'furtivos', '563', 'ellos', '127', 'Roy', 'Marlee', 'faltan', 'quedarán', 'golosinas', 'Frances', 'treinta', 'trae', 'monedas', 'principio', 'duró', 'tartas', 'gusta', 'maneras', 'Bay', 'Jose', '143', 'Unos', 'Del', 'jugo', 'caballo', 'autobús', 'Juan', 'regala', '113', 'guardó', 'Compras', 'entera', 'Lenguaje', 'leche', '792', 'Nuestra', 'Marta', 'coloca', 'pescó', 'fábrica', 'frutas', 'hotel', 'ballenero', 'primarias', 'granos', '58', '219', 'cachorros', 'tienes', 'Hilt', 'organiza', '14', '89', 'alquiler', '2080', 'pulseritas', 'viajaba', 'completas', 'comieron', 'madre', 'Patricia', 'frascos', 'grupos', 'Laura', 'usan', 'Brenda', 'frutales', 'edificio', 'realizado', 'tendrán', 'baraja', 'cuenta', 'rojo', 'costó', 'peces', 'van', 'conos', 'visitando', 'va', 'refresco', 'autobus', 'equipo', 'folios', 'examen', 'juntarán', 'Louis', 'logramos', 'jugadores', 'Melissa', 'Bridget', 'limón', 'perdemos', 'suma', '625', 'recorrida', 'caracoles', 'regalan', 'comprar', 'dárselos', 'cuánto', 'corral', 'compañero', 'Nathan', 'viajar', 'patatas', 'Lucas', 'grado', 'misma', 'bebé', 'recreo', 'líneas', 'cortó', 'tomar', 'cajas', 'tirados', 'depósito', 'maternos', '2436', 'Fede', 'pino', 'cestas', 'Beka', 'costurero', 'Hayley', 'anteojos', 'Qué', 'pescadero', '217', 'vio', 'fotos', 'leyendo', '25', 'azúcar', '240', '568', 'Tengo', 'baloncesto', 'varias', 'jugar', '4275', 'veces', 'árbol', 'macetas', 'rosa', '381', '256', 'profesor', 'durmiendo', 'Peter', 'violeta', '413', 'llenarán', 'completar', 'Había', 'comemos', 'pelota', '220', 'botellas', 'invitado', 'apiladas', 'Germán', '98', 'terminar', 'cristales', 'red', 'verdes', 'entregan', 'correr', '72', 'pares', 'quedaran', 'abuela', 'siete', 'cayeron', 'Tienen', 'estallar', 'preparado', '77', 'Compró', 'negros', 'Patrick', 'Jeff', 'hojas', 'Ya', 'borradores', 'costaba', 'subido', 'gallinero', 'Ethan', 'centenar', 'blusas', 'repetidos', 'comieran', 'caramelos', 'dentro', 'Rebecca', 'matemática', '229', 'jaula', 'fresas', 'Sue', 'Ariel', 'repartimos', 'glaseado', 'puerta', 'sacos', 'lenguaje', 'Stephanie', 'gallinas', '2778', 'Mildred', 'quedaban', '84', 'eso', 'Martin', 'búhos', 'color', 'Annie', 'marcha', '397', 'estrellas', 'unidades', 'odómetro', '257', 'naranjo', 'personas', 'André', 'hoja', 'sandías', 'Has', '400', 'Botellas', 'noche', 'fotografías', 'platos', 'Henry', '212', '2', 'anotaron', '10', 'atunes', 'mochila', 'cubrir', 'lee', 'Hizo', 'Lawrence', 'compartir', 'Julia', 'Son', 'mismo', 'pesado', 'cesto', '303', 'James', 'muñecos', 'adultos', '756', 'High', '222', 'termina', '232', '80', 'todavía', '146', 'De', 'Halcones', 'exactamente', 'chispas', 'padres', 'deterioradas', 'ruta', 'una', 'solamente', '1209', 'paradas', 'inicialmente', 'Zach', 'Al', 'Rose', 'butacas', 'invitó', 'añade', 'boligrafos', 'Sandra', 'cuántos', 'caliente', 'contiene', 'todos', 'Marcos', 'hacia', 'nadie', 'empezó', 'verduras', 'construyendo', 'doble', '41', 'mercadito', 'Craig', 'mayorista', 'separar', 'barras', 'Jesse', 'empiezan', 'manzanas', 'pesaba', 'museo', 'aumenta', 'Dave', 'Valentín', 'prepararon', 'local', 'hace', 'comparte', 'fabricará', 'dátiles', 'crece', 'recogió', 'desayunar', 'café', '6', '76', 'encargó', 'gastado', 'piñata', 'dinero', '31', 'Julían', 'tiran', '57', 'detuvieron', 'tarde', '14240', 'blanca', 'medio', 'Alejandra', 'jugaron', 'andado', 'informe', '85', 'caja', 'Dylan', 'noviembre', 'parcela', 'virginia', 'dan', 'fideos', 'calientes', 'ocupadas', 'página', 'Teníamos', 'gomas', 'obtuvo', 'maravilloso', 'tardarán', 'transporte', 'Yo', 'CDs', 'Robin', 'realiza', 'porciones', 'melocotones', 'llegan', 'necesitan', 'partidos', 'semillas', 'abeja', 'deben', 'cuadras', '650', 'perdió', 'Hound', 'terminarlo', 'tamaño', 'confeccionar', 'matemáticas', '140', 'dulce', 'colocarón', 'mascotas', 's', 'deberá', 'sala', 'cuantos', 'Si', 'afuera', 'reúnen', 'quedaron', 'sobrinas', 'Josh', 'combinaciones', 'patinaba', 'guarda', '223', 'llegaron', 'escultura', '112', 'enteras', 'atletismo', 'saltó', 'lunes', 'corredores', 'aceitunas', 'ardillas', 'cuantas', '860', 'enteros', 'hay', 'años', '144', 'silla', 'frijoles', 'joven', 'regalaron', 'cuentos', 'escaparon', 'piña', '15', 'Ruby', 'viajan', 'volado', 'tendra', 'programa', 'adornar', 'reemplazando', 'pegado', 'gato', 'llena', 'Judy', 'madera', 'Alejo', 'Él', 'distintas', 'papá', 'regalos', '129', 'comedor', 'minuto', 'vender', 'consumido', 'después', 'voló', '86', 'Roger', 'directo', 'necesita', 'Ganó', 'espantar', 'Escuela', 'coche', 'miembro', 'pies', 'rama', 'caminó', 'vecino', 'Tammy', 'Para', 'Km', 'sido', '30', 'reparte', 'preparando', 'stickers', 'Camino', 'igualmente', 'Snyder', 'goma', 'tela', 'usar', 'gente', 'zorro', 'tía', '51', 'ladrando', 'mal', 'consigue', 'Amy', 'realizó', 'función', 'Lansing', 'Shawn', 'condecoraciones', 'Ayer', 'recibió', 'pesarán', 'vidrio', 'salieron', 'Lucy', 'Caja', 'Nell', '2315', 'miró', 'pasada', '247', 'payaso', '3840', 'cuestan', 'biblioteca', 'quieren', 'nieve', 'pieza', '1', 'podridos', 'Maggi', 'pierde', 'centímetros', 'falta', 'llegar', 'usó', 'Sandy', 'minutos', 'salió', 'volvió', '388', 'mariposa', 'Carlos', 'paga', 'viernes', 'Gavin', '479', 'Andres', '2019', 'distinto', 'Mariela', 'retriever', 'fiesta', 'boletos', 'Pamela', 'regalar', 'onzas', 'premios', 'pastelitos', 'Barcelona', 'poner', 'volver', 'racimo', 'cena', 'nació', 'Sean', '18', 'Bonnie', 'Shelby', 'Matías', 'nietas', 'ciencias', 'Billy', 'Joe', 'hizo', 'chaquetas', 'teja', 'caminaré', 'diferentes', 'Fernanda', 'producirá', 'monos', 'jamón', 'lejos', 'álbum', 'relojes', 'tomate', 'Caminas', 'filas', '40', 'ir', 'bosque', 'Rachel', 'momento', 'vende', 'mercado', 'extra', 'luego', '180', 'Martha', 'Tenía', 'Luego', 'sacan', 'Arriba', 'horneó', 'viven', 'manzano', 'Ann', 'Kathy', 'silbidos', 'participantes', 'bolsita', 'valla', 'pantalones', 'Hay', '35', '22', 'tapitas', 'William', 'leído', '156', 'menos', 'mujeres', 'seis', 'suficiente', 'tebeos', 'Joshua', 'película', 'Nicolás', 'distancia', 'llegado', 'Bobby', 'hermano', '71', 'unas', 'leyó', 'reciba', '145', 'rebajado', 'tenía', 'suelo', '90', 'cosechó', '54', 'columnas', 'Eric', 'resto', '00', 'sientas', 'terrible', 'Diane', 'Gary', 'vuelta', 'gansos', 'sillas', 'gastar', 'vagaba', 'vueltas', 'chicos', 'pelotas', '380', 'vale', 'Dos', 'vasos', 'continúan', 'Manuel', 'bus', 'deberes', 'recetas', 'baño', 'tucanes', 'tocado', 'Ralph', 'recibido', 'dispuestos', 'tortillas', 'pinos', 'perfectos', '94', 'habitación', '224', 'ponerlas', 'detergente', 'compras', 'recorrido', 'comernos', 'Jean', '1354', '88', 'lotería', 'acelgas', 'pudo', 'pegó', '372', 'bidón', 'mas', 'amigos', 'hará', 'colmena', 'proyectiles', 'Lisa', 'Carter', 'naranjos', 'venta', 'reparten', 'obtendra', 'Todd', 'Evelyn', 'Condujeron', 'dolor', 'figuritas', 'plazas', 'jamon', 'heladería', 'mangos', '49', 'planos', 'hebillas', 'caminaba', 'Herrera', 'mariposas', 'comió', 'Le', 'tickets', '367', 'Antes', 'pájaros', 'revistas', 'número', 'trabajar', 'estante', 'si', '287', 'incompletas', 'cuán', 'volando', 'juguetes', 'preparar', 'diferencia', '354', 'Warren', 'Emily', 'verde', 'M', 'cargar', 'sera', 'orilla', 'huerto', 'docena', 'entregado', 'marzo', 'Dorothy', 'hipopotamo', 'casamiento', 'capacidad', 'llevado', '<OOP>'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulario.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pregunta in enumerate(frases_test):\n",
    "    for j, palabras in enumerate(pregunta):\n",
    "        if palabras in vocabulario:\n",
    "            test_X[i, tamanoTotal - len(pregunta) + j, vocabulario[palabras]] = 1.0\n",
    "        else:\n",
    "            test_X[i, tamanoTotal - len(pregunta) + j, vocabulario['<OOP>']] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importante, no tokenizar las palabras del conjunto de test para evitar overfitting y crear espacios para\n",
    "# palabras no vistas.\n",
    "\n",
    "training_y = np.asarray(respuestas3[:8500])\n",
    "test_y = np.asarray(respuestas3[8500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(len(vocabulario), embedding_dim, input_length=training_X.shape[1]),\n",
    "    keras.layers.GlobalAveragePooling1D(),\n",
    "    keras.layers.Dense(36, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "history = model.fit(training_X, training_y, epochs = 140, validation_data=[test_X, test_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.Sequential([\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(128, dropout = 0.2, input_shape=[len(vocabulario), training_X.shape[1], training_X.shape[2]])),\n",
    "    keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "opt = keras.optimizers.RMSprop(learning_rate=0.001,rho=0.9, momentum=0.0, epsilon=1e-07, centered=False)\n",
    "\n",
    "#opt = keras.optimizers.SGD(lr=0.00001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#opt = keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "model2.compile(loss='sparse_categorical_crossentropy',optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/240\n",
      "67/67 [==============================] - 65s 966ms/step - loss: 0.7660 - accuracy: 0.6860 - val_loss: 0.6515 - val_accuracy: 0.7502\n",
      "Epoch 2/240\n",
      "67/67 [==============================] - 62s 931ms/step - loss: 0.2550 - accuracy: 0.9262 - val_loss: 0.1003 - val_accuracy: 0.9801\n",
      "Epoch 3/240\n",
      "67/67 [==============================] - 62s 930ms/step - loss: 0.1028 - accuracy: 0.9695 - val_loss: 0.0248 - val_accuracy: 0.9955\n",
      "Epoch 4/240\n",
      "67/67 [==============================] - 63s 944ms/step - loss: 0.0783 - accuracy: 0.9759 - val_loss: 0.0274 - val_accuracy: 0.9932\n",
      "Epoch 5/240\n",
      "67/67 [==============================] - 63s 944ms/step - loss: 0.0484 - accuracy: 0.9854 - val_loss: 0.0148 - val_accuracy: 0.9973\n",
      "Epoch 6/240\n",
      "67/67 [==============================] - 64s 959ms/step - loss: 0.0392 - accuracy: 0.9873 - val_loss: 0.0314 - val_accuracy: 0.9914\n",
      "Epoch 7/240\n",
      "67/67 [==============================] - 67s 1s/step - loss: 0.0303 - accuracy: 0.9895 - val_loss: 0.0163 - val_accuracy: 0.9973\n",
      "Epoch 8/240\n",
      "67/67 [==============================] - 66s 982ms/step - loss: 0.0290 - accuracy: 0.9905 - val_loss: 0.0175 - val_accuracy: 0.9968\n",
      "Epoch 9/240\n",
      "67/67 [==============================] - 66s 992ms/step - loss: 0.0211 - accuracy: 0.9932 - val_loss: 0.0304 - val_accuracy: 0.9896\n",
      "Epoch 10/240\n",
      "67/67 [==============================] - 66s 989ms/step - loss: 0.0193 - accuracy: 0.9941 - val_loss: 0.0091 - val_accuracy: 0.9968\n",
      "Epoch 11/240\n",
      "67/67 [==============================] - 65s 972ms/step - loss: 0.0188 - accuracy: 0.9938 - val_loss: 0.0036 - val_accuracy: 0.9986\n",
      "Epoch 12/240\n",
      "67/67 [==============================] - 65s 969ms/step - loss: 0.0138 - accuracy: 0.9952 - val_loss: 0.0065 - val_accuracy: 0.9977\n",
      "Epoch 13/240\n",
      "67/67 [==============================] - 65s 964ms/step - loss: 0.0148 - accuracy: 0.9945 - val_loss: 0.0076 - val_accuracy: 0.9982\n",
      "Epoch 14/240\n",
      "67/67 [==============================] - 60s 900ms/step - loss: 0.0119 - accuracy: 0.9956 - val_loss: 0.0076 - val_accuracy: 0.9977\n",
      "Epoch 15/240\n",
      "67/67 [==============================] - 58s 871ms/step - loss: 0.0100 - accuracy: 0.9972 - val_loss: 0.0134 - val_accuracy: 0.9964\n",
      "Epoch 16/240\n",
      "67/67 [==============================] - 58s 873ms/step - loss: 0.0147 - accuracy: 0.9952 - val_loss: 0.0131 - val_accuracy: 0.9955\n",
      "Epoch 17/240\n",
      "67/67 [==============================] - 64s 951ms/step - loss: 0.0089 - accuracy: 0.9969 - val_loss: 0.0077 - val_accuracy: 0.9977\n",
      "Epoch 18/240\n",
      "67/67 [==============================] - 68s 1s/step - loss: 0.0111 - accuracy: 0.9975 - val_loss: 0.0064 - val_accuracy: 0.9977\n",
      "Epoch 19/240\n",
      "67/67 [==============================] - 68s 1s/step - loss: 0.0082 - accuracy: 0.9969 - val_loss: 0.0070 - val_accuracy: 0.9977\n",
      "Epoch 20/240\n",
      "67/67 [==============================] - 67s 994ms/step - loss: 0.0090 - accuracy: 0.9973 - val_loss: 0.0095 - val_accuracy: 0.9973\n",
      "Epoch 21/240\n",
      "67/67 [==============================] - 70s 1s/step - loss: 0.0103 - accuracy: 0.9956 - val_loss: 0.0075 - val_accuracy: 0.9977\n",
      "Epoch 22/240\n",
      "67/67 [==============================] - 66s 987ms/step - loss: 0.0074 - accuracy: 0.9974 - val_loss: 0.0085 - val_accuracy: 0.9973\n",
      "Epoch 23/240\n",
      "67/67 [==============================] - 67s 1s/step - loss: 0.0070 - accuracy: 0.9973 - val_loss: 0.0043 - val_accuracy: 0.9986\n",
      "Epoch 24/240\n",
      "67/67 [==============================] - 78s 1s/step - loss: 0.0090 - accuracy: 0.9967 - val_loss: 0.0059 - val_accuracy: 0.9982\n",
      "Epoch 25/240\n",
      "67/67 [==============================] - 92s 1s/step - loss: 0.0094 - accuracy: 0.9967 - val_loss: 0.0043 - val_accuracy: 0.9982\n",
      "Epoch 26/240\n",
      "67/67 [==============================] - 93s 1s/step - loss: 0.0082 - accuracy: 0.9968 - val_loss: 0.0129 - val_accuracy: 0.9977\n",
      "Epoch 27/240\n",
      "67/67 [==============================] - 88s 1s/step - loss: 0.0079 - accuracy: 0.9973 - val_loss: 0.0072 - val_accuracy: 0.9973\n",
      "Epoch 28/240\n",
      "67/67 [==============================] - 69s 1s/step - loss: 0.0072 - accuracy: 0.9972 - val_loss: 0.0098 - val_accuracy: 0.9977\n",
      "Epoch 29/240\n",
      "67/67 [==============================] - 67s 1s/step - loss: 0.0092 - accuracy: 0.9974 - val_loss: 0.0097 - val_accuracy: 0.9977\n",
      "Epoch 30/240\n",
      "67/67 [==============================] - 68s 1s/step - loss: 0.0059 - accuracy: 0.9980 - val_loss: 0.0074 - val_accuracy: 0.9982\n",
      "Epoch 31/240\n",
      "67/67 [==============================] - 67s 1s/step - loss: 0.0096 - accuracy: 0.9959 - val_loss: 0.0048 - val_accuracy: 0.9986\n",
      "Epoch 32/240\n",
      "67/67 [==============================] - 67s 1s/step - loss: 0.0073 - accuracy: 0.9974 - val_loss: 0.0028 - val_accuracy: 0.9991\n",
      "Epoch 33/240\n",
      "67/67 [==============================] - 67s 998ms/step - loss: 0.0053 - accuracy: 0.9981 - val_loss: 0.0064 - val_accuracy: 0.9986\n",
      "Epoch 34/240\n",
      "67/67 [==============================] - 79s 1s/step - loss: 0.0070 - accuracy: 0.9973 - val_loss: 0.0085 - val_accuracy: 0.9977\n",
      "Epoch 35/240\n",
      "67/67 [==============================] - 70s 1s/step - loss: 0.0064 - accuracy: 0.9976 - val_loss: 0.0029 - val_accuracy: 0.9991\n",
      "Epoch 36/240\n",
      "67/67 [==============================] - 70s 1s/step - loss: 0.0059 - accuracy: 0.9980 - val_loss: 0.0041 - val_accuracy: 0.9977\n",
      "Epoch 37/240\n",
      "67/67 [==============================] - 89s 1s/step - loss: 0.0081 - accuracy: 0.9966 - val_loss: 0.0043 - val_accuracy: 0.9982\n",
      "Epoch 38/240\n",
      "67/67 [==============================] - 79s 1s/step - loss: 0.0085 - accuracy: 0.9975 - val_loss: 0.0077 - val_accuracy: 0.9977\n",
      "Epoch 39/240\n",
      "67/67 [==============================] - 93s 1s/step - loss: 0.0086 - accuracy: 0.9965 - val_loss: 0.0053 - val_accuracy: 0.9977\n",
      "Epoch 40/240\n",
      "67/67 [==============================] - 90s 1s/step - loss: 0.0070 - accuracy: 0.9972 - val_loss: 0.0092 - val_accuracy: 0.9982\n",
      "Epoch 41/240\n",
      "67/67 [==============================] - 100s 1s/step - loss: 0.0047 - accuracy: 0.9984 - val_loss: 0.0049 - val_accuracy: 0.9982\n",
      "Epoch 42/240\n",
      "67/67 [==============================] - 96s 1s/step - loss: 0.0041 - accuracy: 0.9982 - val_loss: 0.0044 - val_accuracy: 0.9982\n",
      "Epoch 43/240\n",
      "67/67 [==============================] - 92s 1s/step - loss: 0.0075 - accuracy: 0.9978 - val_loss: 0.0044 - val_accuracy: 0.9986\n",
      "Epoch 44/240\n",
      "67/67 [==============================] - 91s 1s/step - loss: 0.0053 - accuracy: 0.9982 - val_loss: 0.0046 - val_accuracy: 0.9982\n",
      "Epoch 45/240\n",
      "67/67 [==============================] - 80s 1s/step - loss: 0.0059 - accuracy: 0.9976 - val_loss: 0.0072 - val_accuracy: 0.9986\n",
      "Epoch 46/240\n",
      "67/67 [==============================] - 73s 1s/step - loss: 0.0073 - accuracy: 0.9974 - val_loss: 0.0043 - val_accuracy: 0.9977\n",
      "Epoch 47/240\n",
      "67/67 [==============================] - 71s 1s/step - loss: 0.0056 - accuracy: 0.9981 - val_loss: 0.0036 - val_accuracy: 0.9982\n",
      "Epoch 48/240\n",
      "67/67 [==============================] - 82s 1s/step - loss: 0.0055 - accuracy: 0.9981 - val_loss: 0.0046 - val_accuracy: 0.9986\n",
      "Epoch 49/240\n",
      "67/67 [==============================] - 92s 1s/step - loss: 0.0068 - accuracy: 0.9978 - val_loss: 0.0048 - val_accuracy: 0.9982\n",
      "Epoch 50/240\n",
      "67/67 [==============================] - 88s 1s/step - loss: 0.0065 - accuracy: 0.9975 - val_loss: 0.0046 - val_accuracy: 0.9977\n",
      "Epoch 51/240\n",
      "67/67 [==============================] - 96s 1s/step - loss: 0.0060 - accuracy: 0.9976 - val_loss: 0.0153 - val_accuracy: 0.9973\n",
      "Epoch 52/240\n",
      "67/67 [==============================] - 107s 2s/step - loss: 0.0082 - accuracy: 0.9981 - val_loss: 0.0043 - val_accuracy: 0.9982\n",
      "Epoch 53/240\n",
      "67/67 [==============================] - 110s 2s/step - loss: 0.0056 - accuracy: 0.9984 - val_loss: 0.0052 - val_accuracy: 0.9982\n",
      "Epoch 54/240\n",
      "67/67 [==============================] - 122s 2s/step - loss: 0.0046 - accuracy: 0.9981 - val_loss: 0.0076 - val_accuracy: 0.9986\n",
      "Epoch 55/240\n",
      "67/67 [==============================] - 114s 2s/step - loss: 0.0057 - accuracy: 0.9986 - val_loss: 0.0310 - val_accuracy: 0.9959\n",
      "Epoch 56/240\n",
      "67/67 [==============================] - 90s 1s/step - loss: 0.0042 - accuracy: 0.9984 - val_loss: 0.0049 - val_accuracy: 0.9982\n",
      "Epoch 57/240\n",
      "67/67 [==============================] - 94s 1s/step - loss: 0.0047 - accuracy: 0.9978 - val_loss: 0.0112 - val_accuracy: 0.9986\n",
      "Epoch 58/240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 91s 1s/step - loss: 0.0069 - accuracy: 0.9975 - val_loss: 0.0085 - val_accuracy: 0.9982\n",
      "Epoch 59/240\n",
      "67/67 [==============================] - 98s 1s/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.0049 - val_accuracy: 0.9986\n",
      "Epoch 60/240\n",
      "67/67 [==============================] - 99s 1s/step - loss: 0.0044 - accuracy: 0.9982 - val_loss: 0.0091 - val_accuracy: 0.9977\n",
      "Epoch 61/240\n",
      "67/67 [==============================] - 72s 1s/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 0.0040 - val_accuracy: 0.9982\n",
      "Epoch 62/240\n",
      "67/67 [==============================] - 71s 1s/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.0076 - val_accuracy: 0.9977\n",
      "Epoch 63/240\n",
      "67/67 [==============================] - 69s 1s/step - loss: 0.0064 - accuracy: 0.9981 - val_loss: 0.0085 - val_accuracy: 0.9982\n",
      "Epoch 64/240\n",
      "67/67 [==============================] - 70s 1s/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.0037 - val_accuracy: 0.9982\n",
      "Epoch 65/240\n",
      "67/67 [==============================] - 70s 1s/step - loss: 0.0036 - accuracy: 0.9988 - val_loss: 0.0054 - val_accuracy: 0.9982\n",
      "Epoch 66/240\n",
      "67/67 [==============================] - 72s 1s/step - loss: 0.0038 - accuracy: 0.9985 - val_loss: 0.0089 - val_accuracy: 0.9986\n",
      "Epoch 67/240\n",
      "67/67 [==============================] - 72s 1s/step - loss: 0.0040 - accuracy: 0.9986 - val_loss: 0.0095 - val_accuracy: 0.9982\n",
      "Epoch 68/240\n",
      "67/67 [==============================] - 63s 944ms/step - loss: 0.0040 - accuracy: 0.9984 - val_loss: 0.0091 - val_accuracy: 0.9982\n",
      "Epoch 69/240\n",
      "67/67 [==============================] - 61s 909ms/step - loss: 0.0046 - accuracy: 0.9986 - val_loss: 0.0055 - val_accuracy: 0.9973\n",
      "Epoch 70/240\n",
      "67/67 [==============================] - 61s 908ms/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.0160 - val_accuracy: 0.9986\n",
      "Epoch 71/240\n",
      "67/67 [==============================] - 61s 907ms/step - loss: 0.0072 - accuracy: 0.9980 - val_loss: 0.0092 - val_accuracy: 0.9986\n",
      "Epoch 72/240\n",
      "67/67 [==============================] - 61s 910ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.0057 - val_accuracy: 0.9986\n",
      "Epoch 73/240\n",
      "67/67 [==============================] - 61s 909ms/step - loss: 0.0041 - accuracy: 0.9986 - val_loss: 0.0054 - val_accuracy: 0.9977\n",
      "Epoch 74/240\n",
      "67/67 [==============================] - 61s 906ms/step - loss: 0.0048 - accuracy: 0.9981 - val_loss: 0.0034 - val_accuracy: 0.9986\n",
      "Epoch 75/240\n",
      "67/67 [==============================] - 60s 891ms/step - loss: 0.0053 - accuracy: 0.9984 - val_loss: 0.0095 - val_accuracy: 0.9982\n",
      "Epoch 76/240\n",
      "67/67 [==============================] - 60s 903ms/step - loss: 0.0038 - accuracy: 0.9986 - val_loss: 0.0073 - val_accuracy: 0.9986\n",
      "Epoch 77/240\n",
      "67/67 [==============================] - 60s 895ms/step - loss: 0.0032 - accuracy: 0.9987 - val_loss: 0.0039 - val_accuracy: 0.9986\n",
      "Epoch 78/240\n",
      "67/67 [==============================] - 61s 914ms/step - loss: 0.0058 - accuracy: 0.9988 - val_loss: 0.0071 - val_accuracy: 0.9986\n",
      "Epoch 79/240\n",
      "67/67 [==============================] - 60s 895ms/step - loss: 0.0043 - accuracy: 0.9981 - val_loss: 0.0049 - val_accuracy: 0.9986\n",
      "Epoch 80/240\n",
      "67/67 [==============================] - 60s 897ms/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 0.0133 - val_accuracy: 0.9982\n",
      "Epoch 81/240\n",
      "67/67 [==============================] - 61s 906ms/step - loss: 0.0046 - accuracy: 0.9982 - val_loss: 0.0063 - val_accuracy: 0.9986\n",
      "Epoch 82/240\n",
      "67/67 [==============================] - 64s 949ms/step - loss: 0.0048 - accuracy: 0.9988 - val_loss: 0.0061 - val_accuracy: 0.9986\n",
      "Epoch 83/240\n",
      "67/67 [==============================] - 59s 887ms/step - loss: 0.0044 - accuracy: 0.9991 - val_loss: 0.0082 - val_accuracy: 0.9977\n",
      "Epoch 84/240\n",
      "67/67 [==============================] - 60s 900ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.0093 - val_accuracy: 0.9977\n",
      "Epoch 85/240\n",
      "67/67 [==============================] - 61s 904ms/step - loss: 0.0042 - accuracy: 0.9989 - val_loss: 0.0088 - val_accuracy: 0.9986\n",
      "Epoch 86/240\n",
      "67/67 [==============================] - 59s 887ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 0.0046 - val_accuracy: 0.9986\n",
      "Epoch 87/240\n",
      "67/67 [==============================] - 61s 905ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0173 - val_accuracy: 0.9986\n",
      "Epoch 88/240\n",
      "67/67 [==============================] - 63s 934ms/step - loss: 0.0031 - accuracy: 0.9988 - val_loss: 0.0072 - val_accuracy: 0.9986\n",
      "Epoch 89/240\n",
      "67/67 [==============================] - 61s 905ms/step - loss: 0.0049 - accuracy: 0.9982 - val_loss: 0.0055 - val_accuracy: 0.9982\n",
      "Epoch 90/240\n",
      "67/67 [==============================] - 61s 906ms/step - loss: 0.0033 - accuracy: 0.9986 - val_loss: 0.0047 - val_accuracy: 0.9986\n",
      "Epoch 91/240\n",
      "67/67 [==============================] - 60s 899ms/step - loss: 0.0050 - accuracy: 0.9985 - val_loss: 0.0116 - val_accuracy: 0.9986\n",
      "Epoch 92/240\n",
      "67/67 [==============================] - 60s 901ms/step - loss: 0.0034 - accuracy: 0.9987 - val_loss: 0.0085 - val_accuracy: 0.9986\n",
      "Epoch 93/240\n",
      "67/67 [==============================] - 60s 895ms/step - loss: 0.0036 - accuracy: 0.9985 - val_loss: 0.0033 - val_accuracy: 0.9986\n",
      "Epoch 94/240\n",
      "67/67 [==============================] - 60s 895ms/step - loss: 0.0045 - accuracy: 0.9979 - val_loss: 0.0091 - val_accuracy: 0.9986\n",
      "Epoch 95/240\n",
      "67/67 [==============================] - 60s 897ms/step - loss: 0.0034 - accuracy: 0.9989 - val_loss: 0.0102 - val_accuracy: 0.9986\n",
      "Epoch 96/240\n",
      "67/67 [==============================] - 60s 892ms/step - loss: 0.0029 - accuracy: 0.9985 - val_loss: 0.0141 - val_accuracy: 0.9986\n",
      "Epoch 97/240\n",
      "67/67 [==============================] - 63s 943ms/step - loss: 0.0039 - accuracy: 0.9984 - val_loss: 0.0069 - val_accuracy: 0.9982\n",
      "Epoch 98/240\n",
      "67/67 [==============================] - 62s 927ms/step - loss: 0.0030 - accuracy: 0.9989 - val_loss: 0.0078 - val_accuracy: 0.9986\n",
      "Epoch 99/240\n",
      "67/67 [==============================] - 61s 905ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.0091 - val_accuracy: 0.9986\n",
      "Epoch 100/240\n",
      "67/67 [==============================] - 61s 912ms/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 0.0099 - val_accuracy: 0.9973\n",
      "Epoch 101/240\n",
      "67/67 [==============================] - 60s 898ms/step - loss: 0.0039 - accuracy: 0.9981 - val_loss: 0.0116 - val_accuracy: 0.9977\n",
      "Epoch 102/240\n",
      "67/67 [==============================] - 60s 891ms/step - loss: 0.0049 - accuracy: 0.9981 - val_loss: 0.0138 - val_accuracy: 0.9968\n",
      "Epoch 103/240\n",
      "67/67 [==============================] - 60s 892ms/step - loss: 0.0035 - accuracy: 0.9986 - val_loss: 0.0185 - val_accuracy: 0.9973\n",
      "Epoch 104/240\n",
      "67/67 [==============================] - 60s 901ms/step - loss: 0.0029 - accuracy: 0.9989 - val_loss: 0.0100 - val_accuracy: 0.9986\n",
      "Epoch 105/240\n",
      "67/67 [==============================] - 61s 905ms/step - loss: 0.0020 - accuracy: 0.9991 - val_loss: 0.0129 - val_accuracy: 0.9986\n",
      "Epoch 106/240\n",
      "67/67 [==============================] - 60s 902ms/step - loss: 0.0071 - accuracy: 0.9981 - val_loss: 0.0070 - val_accuracy: 0.9986\n",
      "Epoch 107/240\n",
      "67/67 [==============================] - 61s 910ms/step - loss: 0.0023 - accuracy: 0.9992 - val_loss: 0.0081 - val_accuracy: 0.9986\n",
      "Epoch 108/240\n",
      "67/67 [==============================] - 60s 902ms/step - loss: 0.0034 - accuracy: 0.9988 - val_loss: 0.0111 - val_accuracy: 0.9986\n",
      "Epoch 109/240\n",
      "67/67 [==============================] - 60s 891ms/step - loss: 0.0044 - accuracy: 0.9984 - val_loss: 0.0083 - val_accuracy: 0.9982\n",
      "Epoch 110/240\n",
      "67/67 [==============================] - 61s 913ms/step - loss: 0.0025 - accuracy: 0.9989 - val_loss: 0.0074 - val_accuracy: 0.9982\n",
      "Epoch 111/240\n",
      "67/67 [==============================] - 60s 895ms/step - loss: 0.0037 - accuracy: 0.9987 - val_loss: 0.0051 - val_accuracy: 0.9982\n",
      "Epoch 112/240\n",
      "67/67 [==============================] - 60s 900ms/step - loss: 0.0023 - accuracy: 0.9991 - val_loss: 0.0053 - val_accuracy: 0.9982\n",
      "Epoch 113/240\n",
      "67/67 [==============================] - 64s 958ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0180 - val_accuracy: 0.9977\n",
      "Epoch 114/240\n",
      "67/67 [==============================] - 60s 902ms/step - loss: 0.0025 - accuracy: 0.9989 - val_loss: 0.0160 - val_accuracy: 0.9986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/240\n",
      "67/67 [==============================] - 60s 903ms/step - loss: 0.0044 - accuracy: 0.9989 - val_loss: 0.0087 - val_accuracy: 0.9982\n",
      "Epoch 116/240\n",
      "67/67 [==============================] - 61s 916ms/step - loss: 0.0076 - accuracy: 0.9981 - val_loss: 0.0148 - val_accuracy: 0.9982\n",
      "Epoch 117/240\n",
      "67/67 [==============================] - 62s 927ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.0133 - val_accuracy: 0.9986\n",
      "Epoch 118/240\n",
      "67/67 [==============================] - 60s 900ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0259 - val_accuracy: 0.9973\n",
      "Epoch 119/240\n",
      "67/67 [==============================] - 60s 893ms/step - loss: 0.0049 - accuracy: 0.9984 - val_loss: 0.0107 - val_accuracy: 0.9982\n",
      "Epoch 120/240\n",
      "67/67 [==============================] - 61s 907ms/step - loss: 0.0025 - accuracy: 0.9987 - val_loss: 0.0066 - val_accuracy: 0.9982\n",
      "Epoch 121/240\n",
      "67/67 [==============================] - 60s 894ms/step - loss: 0.0041 - accuracy: 0.9985 - val_loss: 0.0331 - val_accuracy: 0.9959\n",
      "Epoch 122/240\n",
      "67/67 [==============================] - 60s 902ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0088 - val_accuracy: 0.9986\n",
      "Epoch 123/240\n",
      "67/67 [==============================] - 60s 897ms/step - loss: 0.0053 - accuracy: 0.9981 - val_loss: 0.0124 - val_accuracy: 0.9982\n",
      "Epoch 124/240\n",
      "67/67 [==============================] - 60s 889ms/step - loss: 0.0021 - accuracy: 0.9993 - val_loss: 0.0082 - val_accuracy: 0.9982\n",
      "Epoch 125/240\n",
      "67/67 [==============================] - 60s 901ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.0113 - val_accuracy: 0.9986\n",
      "Epoch 126/240\n",
      "67/67 [==============================] - 59s 886ms/step - loss: 0.0029 - accuracy: 0.9988 - val_loss: 0.0102 - val_accuracy: 0.9982\n",
      "Epoch 127/240\n",
      "67/67 [==============================] - 61s 918ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.0102 - val_accuracy: 0.9986\n",
      "Epoch 128/240\n",
      "67/67 [==============================] - 61s 909ms/step - loss: 0.0027 - accuracy: 0.9989 - val_loss: 0.0178 - val_accuracy: 0.9982\n",
      "Epoch 129/240\n",
      "67/67 [==============================] - 65s 971ms/step - loss: 0.0022 - accuracy: 0.9991 - val_loss: 0.0054 - val_accuracy: 0.9982\n",
      "Epoch 130/240\n",
      "67/67 [==============================] - 60s 903ms/step - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.0032 - val_accuracy: 0.9986\n",
      "Epoch 131/240\n",
      "67/67 [==============================] - 60s 894ms/step - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.0166 - val_accuracy: 0.9986\n",
      "Epoch 132/240\n",
      "67/67 [==============================] - 60s 891ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.0085 - val_accuracy: 0.9982\n",
      "Epoch 133/240\n",
      "67/67 [==============================] - 60s 902ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.0156 - val_accuracy: 0.9986\n",
      "Epoch 134/240\n",
      "67/67 [==============================] - 60s 896ms/step - loss: 0.0042 - accuracy: 0.9992 - val_loss: 0.0156 - val_accuracy: 0.9986\n",
      "Epoch 135/240\n",
      "67/67 [==============================] - 60s 894ms/step - loss: 0.0029 - accuracy: 0.9989 - val_loss: 0.0173 - val_accuracy: 0.9986\n",
      "Epoch 136/240\n",
      "67/67 [==============================] - 60s 892ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.0133 - val_accuracy: 0.9986\n",
      "Epoch 137/240\n",
      "67/67 [==============================] - 62s 924ms/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 0.0130 - val_accuracy: 0.9982\n",
      "Epoch 138/240\n",
      "67/67 [==============================] - 60s 892ms/step - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.0062 - val_accuracy: 0.9986\n",
      "Epoch 139/240\n",
      "67/67 [==============================] - 60s 889ms/step - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.0169 - val_accuracy: 0.9986\n",
      "Epoch 140/240\n",
      "67/67 [==============================] - 61s 907ms/step - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.0146 - val_accuracy: 0.9986\n",
      "Epoch 141/240\n",
      "67/67 [==============================] - 60s 890ms/step - loss: 0.0029 - accuracy: 0.9987 - val_loss: 0.0097 - val_accuracy: 0.9982\n",
      "Epoch 142/240\n",
      "67/67 [==============================] - 60s 892ms/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 0.0126 - val_accuracy: 0.9986\n",
      "Epoch 143/240\n",
      "67/67 [==============================] - 61s 910ms/step - loss: 0.0026 - accuracy: 0.9989 - val_loss: 0.0160 - val_accuracy: 0.9986\n",
      "Epoch 144/240\n",
      "67/67 [==============================] - 64s 949ms/step - loss: 0.0014 - accuracy: 0.9992 - val_loss: 0.0085 - val_accuracy: 0.9982\n",
      "Epoch 145/240\n",
      "67/67 [==============================] - 60s 897ms/step - loss: 0.0038 - accuracy: 0.9987 - val_loss: 0.0081 - val_accuracy: 0.9982\n",
      "Epoch 146/240\n",
      "67/67 [==============================] - 64s 948ms/step - loss: 0.0021 - accuracy: 0.9992 - val_loss: 0.0116 - val_accuracy: 0.9986\n",
      "Epoch 147/240\n",
      "67/67 [==============================] - 60s 892ms/step - loss: 0.0024 - accuracy: 0.9989 - val_loss: 0.0142 - val_accuracy: 0.9977\n",
      "Epoch 148/240\n",
      "67/67 [==============================] - 61s 904ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.0085 - val_accuracy: 0.9986\n",
      "Epoch 149/240\n",
      "67/67 [==============================] - 65s 972ms/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 0.0067 - val_accuracy: 0.9982\n",
      "Epoch 150/240\n",
      "67/67 [==============================] - 73s 1s/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0118 - val_accuracy: 0.9982\n",
      "Epoch 151/240\n",
      "67/67 [==============================] - 72s 1s/step - loss: 0.0036 - accuracy: 0.9986 - val_loss: 0.0065 - val_accuracy: 0.9986\n",
      "Epoch 152/240\n",
      "67/67 [==============================] - 71s 1s/step - loss: 0.0014 - accuracy: 0.9994 - val_loss: 0.0055 - val_accuracy: 0.9986\n",
      "Epoch 153/240\n",
      "67/67 [==============================] - 68s 1s/step - loss: 0.0026 - accuracy: 0.9986 - val_loss: 0.0089 - val_accuracy: 0.9986\n",
      "Epoch 154/240\n",
      "67/67 [==============================] - 70s 1s/step - loss: 0.0014 - accuracy: 0.9994 - val_loss: 0.0050 - val_accuracy: 0.9991\n",
      "Epoch 155/240\n",
      "67/67 [==============================] - 72s 1s/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0095 - val_accuracy: 0.9986\n",
      "Epoch 156/240\n",
      "67/67 [==============================] - 69s 1s/step - loss: 0.0038 - accuracy: 0.9989 - val_loss: 0.0070 - val_accuracy: 0.9986\n",
      "Epoch 157/240\n",
      "67/67 [==============================] - 69s 1s/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.0079 - val_accuracy: 0.9991\n",
      "Epoch 158/240\n",
      "67/67 [==============================] - 66s 989ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0111 - val_accuracy: 0.9986\n",
      "Epoch 159/240\n",
      "67/67 [==============================] - 61s 915ms/step - loss: 0.0017 - accuracy: 0.9992 - val_loss: 0.0097 - val_accuracy: 0.9986\n",
      "Epoch 160/240\n",
      "67/67 [==============================] - 61s 905ms/step - loss: 0.0034 - accuracy: 0.9991 - val_loss: 0.0063 - val_accuracy: 0.9982\n",
      "Epoch 161/240\n",
      "67/67 [==============================] - 60s 894ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0129 - val_accuracy: 0.9977\n",
      "Epoch 162/240\n",
      "67/67 [==============================] - 62s 930ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0094 - val_accuracy: 0.9982\n",
      "Epoch 163/240\n",
      "67/67 [==============================] - 60s 902ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.0071 - val_accuracy: 0.9986\n",
      "Epoch 164/240\n",
      "67/67 [==============================] - 61s 917ms/step - loss: 0.0036 - accuracy: 0.9986 - val_loss: 0.0079 - val_accuracy: 0.9986\n",
      "Epoch 165/240\n",
      "67/67 [==============================] - 60s 901ms/step - loss: 0.0019 - accuracy: 0.9992 - val_loss: 0.0102 - val_accuracy: 0.9986\n",
      "Epoch 166/240\n",
      "67/67 [==============================] - 60s 899ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.0115 - val_accuracy: 0.9986\n",
      "Epoch 167/240\n",
      "67/67 [==============================] - 59s 887ms/step - loss: 0.0021 - accuracy: 0.9991 - val_loss: 0.0095 - val_accuracy: 0.9986\n",
      "Epoch 168/240\n",
      "67/67 [==============================] - 61s 910ms/step - loss: 0.0044 - accuracy: 0.9988 - val_loss: 0.0122 - val_accuracy: 0.9982\n",
      "Epoch 169/240\n",
      "67/67 [==============================] - 59s 877ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.0113 - val_accuracy: 0.9986\n",
      "Epoch 170/240\n",
      "67/67 [==============================] - 60s 896ms/step - loss: 0.0035 - accuracy: 0.9993 - val_loss: 0.0066 - val_accuracy: 0.9991\n",
      "Epoch 171/240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 60s 893ms/step - loss: 0.0028 - accuracy: 0.9988 - val_loss: 0.0059 - val_accuracy: 0.9986\n",
      "Epoch 172/240\n",
      "67/67 [==============================] - 59s 887ms/step - loss: 0.0025 - accuracy: 0.9988 - val_loss: 0.0098 - val_accuracy: 0.9986\n",
      "Epoch 173/240\n",
      "67/67 [==============================] - 59s 887ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0148 - val_accuracy: 0.9973\n",
      "Epoch 174/240\n",
      "67/67 [==============================] - 67s 996ms/step - loss: 0.0049 - accuracy: 0.9987 - val_loss: 0.0098 - val_accuracy: 0.9986\n",
      "Epoch 175/240\n",
      "67/67 [==============================] - 59s 880ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0079 - val_accuracy: 0.9986\n",
      "Epoch 176/240\n",
      "67/67 [==============================] - 60s 898ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.0096 - val_accuracy: 0.9986\n",
      "Epoch 177/240\n",
      "67/67 [==============================] - 60s 901ms/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 0.0122 - val_accuracy: 0.9986\n",
      "Epoch 178/240\n",
      "67/67 [==============================] - 60s 891ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.0090 - val_accuracy: 0.9991\n",
      "Epoch 179/240\n",
      "67/67 [==============================] - 59s 880ms/step - loss: 0.0028 - accuracy: 0.9988 - val_loss: 0.0076 - val_accuracy: 0.9986\n",
      "Epoch 180/240\n",
      "67/67 [==============================] - 60s 896ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.0074 - val_accuracy: 0.9991\n",
      "Epoch 181/240\n",
      "67/67 [==============================] - 60s 890ms/step - loss: 0.0034 - accuracy: 0.9987 - val_loss: 0.0078 - val_accuracy: 0.9982\n",
      "Epoch 182/240\n",
      "67/67 [==============================] - 60s 899ms/step - loss: 0.0022 - accuracy: 0.9993 - val_loss: 0.0062 - val_accuracy: 0.9986\n",
      "Epoch 183/240\n",
      "67/67 [==============================] - 60s 901ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.0031 - val_accuracy: 0.9986\n",
      "Epoch 184/240\n",
      "67/67 [==============================] - 61s 913ms/step - loss: 0.0020 - accuracy: 0.9992 - val_loss: 0.0090 - val_accuracy: 0.9986\n",
      "Epoch 185/240\n",
      "67/67 [==============================] - 60s 892ms/step - loss: 0.0015 - accuracy: 0.9993 - val_loss: 0.0069 - val_accuracy: 0.9986\n",
      "Epoch 186/240\n",
      "67/67 [==============================] - 60s 898ms/step - loss: 0.0013 - accuracy: 0.9993 - val_loss: 0.0088 - val_accuracy: 0.9986\n",
      "Epoch 187/240\n",
      "67/67 [==============================] - 60s 894ms/step - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.0081 - val_accuracy: 0.9991\n",
      "Epoch 188/240\n",
      "67/67 [==============================] - 60s 898ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.0087 - val_accuracy: 0.9986\n",
      "Epoch 189/240\n",
      "67/67 [==============================] - 60s 890ms/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 0.0087 - val_accuracy: 0.9986\n",
      "Epoch 190/240\n",
      "67/67 [==============================] - 64s 954ms/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.0095 - val_accuracy: 0.9986\n",
      "Epoch 191/240\n",
      "67/67 [==============================] - 59s 885ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.0058 - val_accuracy: 0.9991\n",
      "Epoch 192/240\n",
      "67/67 [==============================] - 60s 891ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.0081 - val_accuracy: 0.9986\n",
      "Epoch 193/240\n",
      "67/67 [==============================] - 60s 894ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0162 - val_accuracy: 0.9986\n",
      "Epoch 194/240\n",
      "67/67 [==============================] - 61s 912ms/step - loss: 0.0021 - accuracy: 0.9991 - val_loss: 0.0062 - val_accuracy: 0.9991\n",
      "Epoch 195/240\n",
      "67/67 [==============================] - 59s 874ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0130 - val_accuracy: 0.9986\n",
      "Epoch 196/240\n",
      "67/67 [==============================] - 60s 893ms/step - loss: 0.0038 - accuracy: 0.9987 - val_loss: 0.0116 - val_accuracy: 0.9986\n",
      "Epoch 197/240\n",
      "67/67 [==============================] - 59s 887ms/step - loss: 0.0021 - accuracy: 0.9993 - val_loss: 0.0074 - val_accuracy: 0.9991\n",
      "Epoch 198/240\n",
      "67/67 [==============================] - 60s 889ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0090 - val_accuracy: 0.9991\n",
      "Epoch 199/240\n",
      "67/67 [==============================] - 60s 901ms/step - loss: 0.0019 - accuracy: 0.9992 - val_loss: 0.0108 - val_accuracy: 0.9982\n",
      "Epoch 200/240\n",
      "67/67 [==============================] - 60s 892ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.0056 - val_accuracy: 0.9986\n",
      "Epoch 201/240\n",
      "67/67 [==============================] - 61s 906ms/step - loss: 0.0019 - accuracy: 0.9989 - val_loss: 0.0109 - val_accuracy: 0.9986\n",
      "Epoch 202/240\n",
      "67/67 [==============================] - 60s 897ms/step - loss: 0.0028 - accuracy: 0.9989 - val_loss: 0.0118 - val_accuracy: 0.9986\n",
      "Epoch 203/240\n",
      "67/67 [==============================] - 60s 892ms/step - loss: 0.0017 - accuracy: 0.9993 - val_loss: 0.0089 - val_accuracy: 0.9986\n",
      "Epoch 204/240\n",
      "67/67 [==============================] - 62s 920ms/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.0144 - val_accuracy: 0.9986\n",
      "Epoch 205/240\n",
      "67/67 [==============================] - 61s 912ms/step - loss: 0.0030 - accuracy: 0.9985 - val_loss: 0.0099 - val_accuracy: 0.9986\n",
      "Epoch 206/240\n",
      "67/67 [==============================] - 62s 929ms/step - loss: 0.0024 - accuracy: 0.9989 - val_loss: 0.0093 - val_accuracy: 0.9986\n",
      "Epoch 207/240\n",
      "67/67 [==============================] - 60s 896ms/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 0.0147 - val_accuracy: 0.9982\n",
      "Epoch 208/240\n",
      "67/67 [==============================] - 60s 891ms/step - loss: 0.0025 - accuracy: 0.9989 - val_loss: 0.0071 - val_accuracy: 0.9986\n",
      "Epoch 209/240\n",
      "67/67 [==============================] - 60s 902ms/step - loss: 0.0013 - accuracy: 0.9994 - val_loss: 0.0170 - val_accuracy: 0.9982\n",
      "Epoch 210/240\n",
      "67/67 [==============================] - 60s 894ms/step - loss: 0.0024 - accuracy: 0.9988 - val_loss: 0.0128 - val_accuracy: 0.9986\n",
      "Epoch 211/240\n",
      "67/67 [==============================] - 60s 894ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0083 - val_accuracy: 0.9986\n",
      "Epoch 212/240\n",
      "67/67 [==============================] - 60s 890ms/step - loss: 0.0047 - accuracy: 0.9986 - val_loss: 0.0121 - val_accuracy: 0.9986\n",
      "Epoch 213/240\n",
      "67/67 [==============================] - 60s 903ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0107 - val_accuracy: 0.9986\n",
      "Epoch 214/240\n",
      "67/67 [==============================] - 63s 936ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.0079 - val_accuracy: 0.9986\n",
      "Epoch 215/240\n",
      "67/67 [==============================] - 60s 888ms/step - loss: 0.0018 - accuracy: 0.9991 - val_loss: 0.0112 - val_accuracy: 0.9986\n",
      "Epoch 216/240\n",
      "67/67 [==============================] - 60s 889ms/step - loss: 0.0020 - accuracy: 0.9992 - val_loss: 0.0125 - val_accuracy: 0.9982\n",
      "Epoch 217/240\n",
      "67/67 [==============================] - 60s 890ms/step - loss: 0.0024 - accuracy: 0.9989 - val_loss: 0.0144 - val_accuracy: 0.9982\n",
      "Epoch 218/240\n",
      "67/67 [==============================] - 60s 888ms/step - loss: 0.0023 - accuracy: 0.9991 - val_loss: 0.0148 - val_accuracy: 0.9986\n",
      "Epoch 219/240\n",
      "67/67 [==============================] - 59s 887ms/step - loss: 0.0022 - accuracy: 0.9992 - val_loss: 0.0045 - val_accuracy: 0.9982\n",
      "Epoch 220/240\n",
      "67/67 [==============================] - 60s 889ms/step - loss: 0.0016 - accuracy: 0.9993 - val_loss: 0.0060 - val_accuracy: 0.9986\n",
      "Epoch 221/240\n",
      "67/67 [==============================] - 64s 953ms/step - loss: 0.0021 - accuracy: 0.9991 - val_loss: 0.0069 - val_accuracy: 0.9986\n",
      "Epoch 222/240\n",
      "67/67 [==============================] - 60s 898ms/step - loss: 0.0015 - accuracy: 0.9993 - val_loss: 0.0098 - val_accuracy: 0.9986\n",
      "Epoch 223/240\n",
      "67/67 [==============================] - 62s 926ms/step - loss: 8.9027e-04 - accuracy: 0.9995 - val_loss: 0.0086 - val_accuracy: 0.9986\n",
      "Epoch 224/240\n",
      "67/67 [==============================] - 60s 899ms/step - loss: 0.0022 - accuracy: 0.9993 - val_loss: 0.0053 - val_accuracy: 0.9986\n",
      "Epoch 225/240\n",
      "67/67 [==============================] - 59s 878ms/step - loss: 0.0032 - accuracy: 0.9989 - val_loss: 0.0063 - val_accuracy: 0.9986\n",
      "Epoch 226/240\n",
      "67/67 [==============================] - 59s 886ms/step - loss: 0.0022 - accuracy: 0.9988 - val_loss: 0.0166 - val_accuracy: 0.9986\n",
      "Epoch 227/240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 60s 893ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0129 - val_accuracy: 0.9986\n",
      "Epoch 228/240\n",
      "67/67 [==============================] - 59s 883ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 0.0082 - val_accuracy: 0.9982\n",
      "Epoch 229/240\n",
      "67/67 [==============================] - 60s 900ms/step - loss: 0.0012 - accuracy: 0.9994 - val_loss: 0.0083 - val_accuracy: 0.9982\n",
      "Epoch 230/240\n",
      "67/67 [==============================] - 60s 896ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0070 - val_accuracy: 0.9982\n",
      "Epoch 231/240\n",
      "67/67 [==============================] - 59s 885ms/step - loss: 0.0012 - accuracy: 0.9993 - val_loss: 0.0093 - val_accuracy: 0.9982\n",
      "Epoch 232/240\n",
      "67/67 [==============================] - 60s 891ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.0178 - val_accuracy: 0.9986\n",
      "Epoch 233/240\n",
      "67/67 [==============================] - 63s 937ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.0097 - val_accuracy: 0.9982\n",
      "Epoch 234/240\n",
      "67/67 [==============================] - 60s 893ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0078 - val_accuracy: 0.9982\n",
      "Epoch 235/240\n",
      "67/67 [==============================] - 59s 879ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.0129 - val_accuracy: 0.9982\n",
      "Epoch 236/240\n",
      "67/67 [==============================] - 63s 944ms/step - loss: 9.6365e-04 - accuracy: 0.9994 - val_loss: 0.0088 - val_accuracy: 0.9986\n",
      "Epoch 237/240\n",
      "67/67 [==============================] - 63s 945ms/step - loss: 0.0028 - accuracy: 0.9992 - val_loss: 0.0090 - val_accuracy: 0.9986\n",
      "Epoch 238/240\n",
      "67/67 [==============================] - 59s 885ms/step - loss: 0.0011 - accuracy: 0.9996 - val_loss: 0.0136 - val_accuracy: 0.9986\n",
      "Epoch 239/240\n",
      "67/67 [==============================] - 61s 904ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0161 - val_accuracy: 0.9986\n",
      "Epoch 240/240\n",
      "67/67 [==============================] - 60s 890ms/step - loss: 0.0011 - accuracy: 0.9994 - val_loss: 0.0082 - val_accuracy: 0.9982\n"
     ]
    }
   ],
   "source": [
    "history2 = model2.fit(training_X, training_y, epochs = 240, batch_size= 128, validation_data=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def resultados(history):\n",
    "    pd.DataFrame(history.history).plot(figsize = (8,5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0,1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "resultados(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Pedro tiene 5 monedas y su abuela le da 3 más, ¿Cuántas monedas tiene en total?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Si Juan tiene 3 cajas y pierde 1, Cuantas le quedan?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = [word for word in question.split(' ') if word not in nomb]\n",
    "question = ' '.join(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En el conjunto de predicción nos podemos encontrar palabras que no estén\n",
    "bag_q = np.zeros([1, tamanoTotal, len(palabras_unicas)+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulario['5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_question = question.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(palabras_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tamanoTotal-len(palabras_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "palabra Si\n",
      "24\n",
      "palabra 3\n",
      "25\n",
      "palabra cajas\n",
      "26\n",
      "palabra pierde\n",
      "27\n",
      "palabra 1,\n",
      "28\n",
      "palabra Cuantas\n",
      "29\n",
      "palabra quedan?\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "for j, palabras in enumerate(palabras_question):\n",
    "    print('palabra', palabras)\n",
    "    print(tamanoTotal - len(palabras_question) +j)\n",
    "    if palabras in vocabulario: \n",
    "        bag_q[0, tamanoTotal - len(palabras_question) + j, vocabulario[palabras]] = 1.0\n",
    "    else:\n",
    "        bag_q[0, tamanoTotal - len(palabras_question) + j, vocabulario['<OOP>']] = 1.0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01899333, 0.56132835, 0.37139735, 0.04828091]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.predict(bag_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(loss='sparse_categorical_crossentropy',optimizer='SGD',metrics=['accuracy'])\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='RMSProp',metrics=['accuracy'])\n",
    "#model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(training_X, training_y, epochs = 140, validation_data=[test_X, test_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(training_X, training_y, epochs = 140, validation_data=[test_X, test_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.SGD(lr=0.00001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#opt = keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "model2.compile(loss='sparse_categorical_crossentropy',optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.26 GiB for an array with shape (8500, 31, 2306) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-4073fed245b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m240\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1061\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m                **kwargs):\n\u001b[1;32m    264\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[1;32m    267\u001b[0m         sample_weights, sample_weight_modes)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 635\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1014\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mscipy_sparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mscipy_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    262\u001b[0m   \"\"\"\n\u001b[1;32m    263\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 264\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    273\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 2.26 GiB for an array with shape (8500, 31, 2306) and data type float32"
     ]
    }
   ],
   "source": [
    "history2 = model2.fit(training_X, training_y, epochs = 240, batch_size= 128, validation_data=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt = keras.optimizers.SGD(lr=0.00001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "opt = keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "model2.compile(loss='sparse_categorical_crossentropy',optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model2.fit(training_X, training_y, epochs = 240, batch_size= 128, validation_data=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados(history2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
